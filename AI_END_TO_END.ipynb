{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI END TO END",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter-1 Agent And Control"
      ],
      "metadata": {
        "id": "yzf2wpz21yH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###a. Representing Agents and Environments"
      ],
      "metadata": {
        "id": "JNmiFygnKz6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Body of the AI Agent\n",
        "import random\n",
        "\n",
        "class Agent(object):\n",
        "  def __init__(self,env):\n",
        "    \"setup the agent \"\n",
        "    self.env=env\n",
        "\n",
        "def go(self,n):\n",
        "  \"acts for n time steps\"\n",
        "  raise NotImplementedError(\"go\") # abstract method\n"
      ],
      "metadata": {
        "id": "tRjQKGMb1yKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment implements a do(action) method where action is a variable value dictionary. This returns a percept, which is also a variable-value dictionary. The use of dictionaries allows for structured actions and percepts."
      ],
      "metadata": {
        "id": "xTKwJ_GLGUM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that Environment is a subclass of Displayable so that it can use the\n",
        "display method described in Section 1.7.1.\n"
      ],
      "metadata": {
        "id": "dxXPwsvrGUPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install display"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFzHerI4JmHI",
        "outputId": "a59774c8-eee7-4709-b22b-3fd8048bb283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: display in /usr/local/lib/python3.7/dist-packages (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from display import Displayable\n",
        "class Environment(Displayable):\n",
        "  def initial_percepts(self):\n",
        "    \"\"\"returns the initial percepts for the agent\"\"\"\n",
        "    raise NotImplementedError(\"initial_percepts\") # abstract method\n",
        "    \n",
        "  def do(self,action):\n",
        "    \"\"\"does the action in the environment\n",
        "    returns the next percept \"\"\"\n",
        "    raise NotImplementedError(\"do\") # abstract method\n"
      ],
      "metadata": {
        "id": "OYXJgXKLGUSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Environments.py\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zEfqmxQtGUUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment state is given in terms of the time and the amount of paper in\n",
        "stock. It also remembers the in-stock history and the price history. The percepts\n",
        "are the price and the amount of paper in stock. The action of the agent is the\n",
        "number to buy.\n"
      ],
      "metadata": {
        "id": "LCaewf6kGUYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we assume that the prices are obtained from the prices list plus a random integer in range [0, max price addon) plus a linear ”inflation”. The agent\n",
        "cannot access the price model; it just observes the prices and the amount in\n",
        "stock.\n"
      ],
      "metadata": {
        "id": "69IWVhrFGUau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TP_env(Environment):\n",
        "\n",
        "  prices = [234, 234, 234, 234, 255, 255, 275, 275, 211, 211, 211,\n",
        "            234, 234, 234, 234, 199, 199, 275, 275, 234, 234, 234, 234, 255,\n",
        "            255, 260, 260, 265, 265, 265, 265, 270, 270, 255, 255, 260, 260,\n",
        "            265, 265, 150, 150, 265, 265, 270, 270, 255, 255, 260, 260, 265,\n",
        "            265, 265, 265, 270, 270, 211, 211, 255, 255, 260, 260, 265, 265,\n",
        "            260, 265, 270, 270, 205, 255, 255, 260, 260, 265, 265, 265, 265,\n",
        "            270, 270]\n",
        "  max_price_addon = 20 # maximum of random value added to get price\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"paper buying agent\"\"\"\n",
        "    self.time=0\n",
        "    self.stock=20\n",
        "    self.stock_history = [] # memory of the stock history\n",
        "    self.price_history = [] # memory of the price history\n",
        "\n",
        "  def initial_percepts(self):\n",
        "      \"\"\"return initial percepts\"\"\"\n",
        "      self.stock_history.append(self.stock)\n",
        "      price = self.prices[0]+random.randrange(self.max_price_addon)\n",
        "      self.price_history.append(price)\n",
        "\n",
        "      return {'price': price,\n",
        "      'instock': self.stock}\n",
        "\n",
        "  def do(self, action):\n",
        "      \"\"\"does action (buy) and returns percepts (price and instock)\"\"\"\n",
        "      used = pick_from_dist({6:0.1, 5:0.1, 4:0.2, 3:0.3, 2:0.2, 1:0.1})\n",
        "      bought = action['buy']\n",
        "      self.stock = self.stock+bought-used\n",
        "      self.stock_history.append(self.stock)\n",
        "      self.time += 1\n",
        "\n",
        "      price = (self.prices[self.time%len(self.prices)] # repeating pattern\n",
        "      +random.randrange(self.max_price_addon) # plus randomness\n",
        "      +self.time//2) # plus inflation\n",
        "      self.price_history.append(price)\n",
        "      return {'price': price,\n",
        "      'instock': self.stock}"
      ],
      "metadata": {
        "id": "-oU8qXpwGUda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **pick_from_dist** method takes in a item : probability dictionary, and returns\n",
        "one of the items in proportion to its probability.\n"
      ],
      "metadata": {
        "id": "D5KBbWpSOso0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_from_dist(item_prob_dist):\n",
        "  \"\"\" returns a value from a distribution.\n",
        "  item_prob_dist is an item:probability dictionary, where the\n",
        "  probabilities sum to 1.\n",
        "  returns an item chosen in proportion to its probability\n",
        "  \"\"\"\n",
        "  ranreal = random.random()\n",
        "  for (it,prob) in item_prob_dist.items():\n",
        "    if ranreal < prob:\n",
        "      return it\n",
        "    else: \n",
        "      ranreal -= prob\n",
        "      raise RuntimeError(str(item_prob_dist)+\" is not a probability distribution\")"
      ],
      "metadata": {
        "id": "-ebErslCGUf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Agent.py"
      ],
      "metadata": {
        "id": "tTTSVokFGUif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TP_agent(Agent):\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "    self.spent = 0\n",
        "    percepts = env.initial_percepts()\n",
        "    self.ave = self.last_price = percepts['price']\n",
        "    self.instock = percepts['instock']\n",
        "\n",
        "  def go(self, n):\n",
        "    \"\"\"go for n time steps\n",
        "    \"\"\"\n",
        "    for i in range(n):\n",
        "      if self.last_price < 0.9*self.ave and self.instock < 60:\n",
        "        tobuy = 48\n",
        "      elif self.instock < 12:\n",
        "        tobuy = 12\n",
        "      else:\n",
        "        tobuy = 0\n",
        "      self.spent += tobuy*self.last_price\n",
        "      percepts = env.do({'buy': tobuy})\n",
        "      self.last_price = percepts['price']\n",
        "      self.ave = self.ave+(self.last_price-self.ave)*0.05\n",
        "      self.instock = percepts['instock']\n",
        "env = TP_env()\n",
        "ag = TP_agent(env)\n",
        "ag.go(90)\n",
        "ag.spent/env.time ## average spent per time period"
      ],
      "metadata": {
        "id": "vzfLCa0KGUlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plotting.py"
      ],
      "metadata": {
        "id": "NFFCdunNGUr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Plot_prices(object):\n",
        "  \"\"\"Set up the plot for history of price and number in stock\"\"\"\n",
        "  def __init__(self, ag,env):\n",
        "    self.ag = ag\n",
        "    self.env = env\n",
        "    plt.ion()\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Number in stock.Price.\")\n",
        "\n",
        "  def plot_run(self):\n",
        "    \"\"\"plot history of price and instock\"\"\"\n",
        "    num = len(env.stock_history)\n",
        "    plt.plot(range(num),env.stock_history,label=\"In stock\")\n",
        "    plt.plot(range(num),env.price_history,label=\"Price\")\n",
        "    #plt.legend(loc=\"upper left\")\n",
        "    plt.draw()\n",
        "\n",
        "#pl = Plot_prices(ag,env)\n",
        "#ag.go(90); pl.plot_run()\n"
      ],
      "metadata": {
        "id": "IHFQM2vuGUvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##b.Hierarchical Controller"
      ],
      "metadata": {
        "id": "voxp3uHdGU1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the hierarchical controller, in folder ”aipython”, load\n",
        "**”agentTop.py”**, using e.g., ipython -i **agentTop.py**, and copy and\n",
        "paste the commands near the bottom of that file. This requires Python\n",
        "3 with matplotlib."
      ],
      "metadata": {
        "id": "xec17_gHGU7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, each layer, including the top layer, implements the environment class, because each layer is seen as an environment from the layer\n",
        "above"
      ],
      "metadata": {
        "id": "NTvvh9WVGU96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We arbitrarily divide the environment and the body, so that the environment just defines the walls, and the body includes everything to do with the\n",
        "agent. Note that the named locations are part of the (top-level of the) agent,\n",
        "not part of the environment, although they could have been"
      ],
      "metadata": {
        "id": "nbpMvY5xGVA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Enviroment.py"
      ],
      "metadata": {
        "id": "rwkSZd-FTL8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment defines the walls.\n"
      ],
      "metadata": {
        "id": "FnftUxO_TKtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from agents import Environment\n",
        "class Rob_env(Environment):\n",
        "  def __init__(self,walls = {}):\n",
        "    \"\"\"walls is a set of line segments\n",
        "    where each line segment is of the form ((x0,y0),(x1,y1))\n",
        "    \"\"\"\n",
        "    self.walls = walls\n",
        "    "
      ],
      "metadata": {
        "id": "WGRv5KqVGVDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Body.py"
      ],
      "metadata": {
        "id": "JU5p_Ez6GVGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The body defines everything about the agent body"
      ],
      "metadata": {
        "id": "Se3O-_ZLGVJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from agents import Environment\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "class Rob_body(Environment):\n",
        " def __init__(self, env, init_pos=(0,0,90)):\n",
        "    \"\"\" env is the current environment\n",
        "    init_pos is a triple of (x-position, y-position, direction)\n",
        "    direction is in degrees; 0 is to right, 90 is straight-up, etc\n",
        "    \"\"\"\n",
        "    self.env = env\n",
        "    self.rob_x, self.rob_y, self.rob_dir = init_pos\n",
        "    self.turning_angle = 18 # degrees that a left makes\n",
        "    self.whisker_length = 6 # length of the whisker\n",
        "    self.whisker_angle = 30 # angle of whisker relative to robot\n",
        "    self.crashed = False\n",
        "    # The following control how it is plotted\n",
        "    self.plotting = True # whether the trace is being plotted\n",
        "    self.sleep_time = 0.05 # time between actions (for real-time plotting)\n",
        "    # The following are data structures maintained:\n",
        "    self.history = [(self.rob_x, self.rob_y)] # history of (x,y) positions\n",
        "    self.wall_history = [] # history of hitting the wall\n",
        "\n",
        " def percepts(self):\n",
        "   return {'rob_x_pos':self.rob_x, 'rob_y_pos':self.rob_y, 'rob_dir':self.rob_dir, 'whisker':self.whisker() , 'rashed':self.crashed}\n",
        " initial_percepts = percepts # use percept function for initial percepts too\n",
        "\n",
        " def do(self,action):\n",
        "  \"\"\" action is {'steer':direction}\n",
        "  direction is 'left', 'right' or 'straight'\n",
        " \"\"\"\n",
        "  if self.crashed:\n",
        "    return self.percepts()\n",
        "  direction = action['steer']\n",
        "  compass_deriv ={'left':1,'straight':0,'right':-1}[direction]*self.turning_angle\n",
        "  self.rob_dir = (self.rob_dir + compass_deriv +360)%360 # make in range [0,360)\n",
        "  rob_x_new = self.rob_x + math.cos(self.rob_dir*math.pi/180)\n",
        "  rob_y_new = self.rob_y + math.sin(self.rob_dir*math.pi/180)\n",
        "  path = ((self.rob_x,self.rob_y),(rob_x_new,rob_y_new))\n",
        "\n",
        "  if any(line_segments_intersect(path,wall) for wall in self.env.walls):\n",
        "    self.crashed = True\n",
        "    if self.plotting:\n",
        "      plt.plot([self.rob_x],[self.rob_y],\"r*\",markersize=20.0)\n",
        "      plt.draw()\n",
        "    self.rob_x, self.rob_y = rob_x_new, rob_y_new\n",
        "    self.history.append((self.rob_x, self.rob_y))\n",
        "\n",
        "    if self.plotting and not self.crashed:\n",
        "      plt.plot([self.rob_x],[self.rob_y],\"go\")\n",
        "      plt.draw()\n",
        "      plt.pause(self.sleep_time)\n",
        "    return self.percepts()\n",
        "\n",
        "def whisker(self):\n",
        " \"\"\"returns true whenever the whisker sensor intersects with a wall\n",
        " \"\"\"\n",
        " whisk_ang_world = (self.rob_dir-self.whisker_angle)*math.pi/180\n",
        " # angle in radians in world coordinates\n",
        " wx = self.rob_x + self.whisker_length * math.cos(whisk_ang_world)\n",
        " wy = self.rob_y + self.whisker_length * math.sin(whisk_ang_world)\n",
        " whisker_line = ((self.rob_x,self.rob_y),(wx,wy))\n",
        " hit = any(line_segments_intersect(whisker_line,wall)for wall in self.env.walls)\n",
        " if hit:\n",
        "\n",
        "   self.wall_history.append((self.rob_x, self.rob_y))\n",
        "   if self.plotting:\n",
        "     plt.plot([self.rob_x],[self.rob_y],\"ro\")\n",
        "     plt.draw()\n",
        "   return hit\n",
        "\n",
        " def line_segments_intersect(linea,lineb):\n",
        "  \"\"\"returns true if the line segments, linea and lineb intersect.\n",
        "  A line segment is represented as a pair of points.\n",
        "  A point is represented as a (x,y) pair.\n",
        "  \"\"\"\n",
        "  ((x0a,y0a),(x1a,y1a)) = linea\n",
        "  ((x0b,y0b),(x1b,y1b)) = lineb\n",
        "  da, db = x1a-x0a, x1b-x0b\n",
        "  ea, eb = y1a-y0a, y1b-y0b\n",
        "  denom = db*ea-eb*da\n",
        "  if denom==0: # line segments are parallel\n",
        "    return False\n",
        "  cb = (da*(y0b-y0a)-ea*(x0b-x0a))/denom # position along line b\n",
        "  if cb<0 or cb>1:\n",
        "    return False\n",
        "  ca = (db*(y0b-y0a)-eb*(x0b-x0a))/denom # position along line a\n",
        "  return 0<=ca<=1\n",
        "\n",
        "  # Test cases:\n",
        "  # assert line_segments_intersect(((0,0),(1,1)),((1,0),(0,1)))\n",
        "  # assert not line_segments_intersect(((0,0),(1,1)),((1,0),(0.6,0.4)))\n",
        "  # assert line_segments_intersect(((0,0),(1,1)),((1,0),(0.4,0.6)))\n"
      ],
      "metadata": {
        "id": "8eKyCsicGVLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This detects if the whisker and the wall intersect. It’s value is returned as a\n",
        "percept."
      ],
      "metadata": {
        "id": "BHoROWMZGVOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Middle Layer.py"
      ],
      "metadata": {
        "id": "_MZnI7H4GVRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The middle layer acts like both a controller (for the environment layer) and an\n",
        "environment for the upper layer. It has to tell the environment how to steer.\n",
        "Thus it calls **env.do(·)**. It also is told the position to go to and the timeout. Thus\n",
        "it also has to implement do(·).\n"
      ],
      "metadata": {
        "id": "4KopCJ_1GVUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###AgentMiddle.py"
      ],
      "metadata": {
        "id": "VRBevCapafZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Environment\n",
        "import math\n",
        "class Rob_middle_layer(Environment):\n",
        "  def __init__(self,env):\n",
        "    self.env=env\n",
        "    self.percepts = env.initial_percepts()\n",
        "    self.straight_angle = 11 # angle that is close enough to straight ahead\n",
        "    self.close_threshold = 2 # distance that is close enough to arrived\n",
        "    self.close_threshold_squared = self.close_threshold**2 # just compute it once\n",
        "  def initial_percepts(self):\n",
        "    return {}\n",
        "  def do(self, action):\n",
        "    \"\"\"action is {'go_to':target_pos,'timeout':timeout}\n",
        "    target_pos is (x,y) pair\n",
        "    timeout is the number of steps to try\n",
        "    returns {'arrived':True} when arrived is true\n",
        "    or {'arrived':False} if it reached the timeout\"\"\"\n",
        "    if 'timeout' in action:\n",
        "      remaining = action['timeout']\n",
        "    else:\n",
        "      remaining = -1 # will never reach 0\n",
        "      target_pos = action['go_to']\n",
        "      arrived = self.close_enough(target_pos)\n",
        "    while not arrived and remaining != 0:\n",
        "      self.percepts = self.env.do({\"steer\":self.steer(target_pos)})\n",
        "      remaining -= 1\n",
        "      arrived = self.close_enough(target_pos)\n",
        "    return {'arrived':arrived}\n"
      ],
      "metadata": {
        "id": "FQ8qzj0MZnuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def steer(self,target_pos):\n",
        " if self.percepts['whisker']:\n",
        "  self.display(3,'whisker on', self.percepts)\n",
        "  return \"left\"\n",
        " else:\n",
        "  gx,gy = target_pos\n",
        "  rx,ry = self.percepts['rob_x_pos'],self.percepts['rob_y_pos']\n",
        "  goal_dir = math.acos((gx-rx)/math.sqrt((gx-rx)*(gx-rx)\n",
        "  +(gy-ry)*(gy-ry)))*180/math.pi\n",
        "\n",
        "  if ry>gy:\n",
        "    goal_dir = -goal_dir\n",
        "  goal_from_rob = (goal_dir -self.percepts['rob_dir']+540)%360-180\n",
        "  assert -180 < goal_from_rob <= 180\n",
        "  if goal_from_rob > self.straight_angle:\n",
        "    return \"left\"\n",
        "  elif goal_from_rob < -self.straight_angle:\n",
        "    return \"right\"\n",
        "  else:\n",
        "    return \"straight\"\n",
        "\n",
        " def close_enough(self,target_pos):\n",
        "    gx,gy = target_pos\n",
        "    rx,ry = self.percepts['rob_x_pos'],self.percepts['rob_y_pos']\n",
        "    return (gx-rx)**2 + (gy-ry)**2 <= self.close_threshold_squared"
      ],
      "metadata": {
        "id": "Eoi_3Ik8ZnxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agentMiddle import Rob_middle_layer\n",
        "from agents import Environment\n",
        "class Rob_top_layer(Environment):\n",
        " def __init__(self, middle, timeout=200, locations = {'mail':(-5,10),\n",
        "    'o103':(50,10), 'o109':(100,10),'storage':(101,51)}):\n",
        "    \"\"\"middle is the middle layer\n",
        "    timeout is the number of steps the middle layer goes before giving up\n",
        "    locations is a loc:pos dictionary\n",
        "    where loc is a named location, and pos is an (x,y) position.\n",
        "    \"\"\"\n",
        "    self.middle = middle\n",
        "    self.timeout = timeout # number of steps before the middle layer should give up\n",
        "    self.locations = locations\n",
        "\n",
        " def do(self,plan):\n",
        "    \"\"\"carry out actions.\n",
        "    actions is of the form {'visit':list_of_locations}\n",
        "    It visits the locations in turn.\n",
        "    \"\"\"\n",
        "    to_do = plan['visit']\n",
        "    for loc in to_do:\n",
        "      position = self.locations[loc]\n",
        "      arrived = self.middle.do({'go_to':position, 'timeout':self.timeout})\n",
        "      self.display(1,\"Arrived at\",loc,arrived)\n"
      ],
      "metadata": {
        "id": "Iua01RY9Znzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plotting.py"
      ],
      "metadata": {
        "id": "YPPHICs_Zn2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "class Plot_env(object):\n",
        "  def __init__(self, body,top):\n",
        "    \"\"\"sets up the plot\n",
        "    \"\"\"\n",
        "    self.body = body\n",
        "    plt.ion()\n",
        "    plt.clf()\n",
        "    plt.axes().set_aspect('equal')\n",
        "    \n",
        "    for wall in body.env.walls:\n",
        "      ((x0,y0),(x1,y1)) = wall\n",
        "      plt.plot([x0,x1],[y0,y1],\"-k\",linewidth=3)\n",
        "\n",
        "    for loc in top.locations:\n",
        "      (x,y) = top.locations[loc]\n",
        "      plt.plot([x],[y],\"k<\")\n",
        "      plt.text(x+1.0,y+0.5,loc) # print the label above and to theright\n",
        "    plt.plot([body.rob_x],[body.rob_y],\"go\")\n",
        "    plt.draw()\n",
        "\n",
        "def plot_run(self):\n",
        "  \"\"\"plots the history after the agent has finished.\n",
        "  This is typically only used if body.plotting==False\n",
        "  \"\"\"\n",
        "  xs,ys = zip(*self.body.history)\n",
        "  plt.plot(xs,ys,\"go\")\n",
        "  wxs,wys = zip(*self.body.wall_history)\n",
        "  plt.plot(wxs,wys,\"ro\")\n",
        "  #plt.draw()"
      ],
      "metadata": {
        "id": "IsYxBwZKZn4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code plots the agent as it acts in the world:"
      ],
      "metadata": {
        "id": "nQeTJtKqZn7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agentEnv import Rob_body, Rob_env\n",
        "env = Rob_env({((20,0),(30,20)), ((70,-5),(70,25))})\n",
        "body = Rob_body(env)\n",
        "middle = Rob_middle_layer(body)\n",
        "top = Rob_top_layer(middle)\n",
        "# try:\n",
        "# pl=Plot_env(body,top)\n",
        "# top.do({'visit':['o109','storage','o109','o103']})\n",
        "# You can directly control the middle layer:\n",
        "# middle.do({'go_to':(30,-10),\"timeout\":200)}\n",
        "# Robot Trap for which the current controller cannot escape:\n",
        "trap_env = Rob_env({((10,-21),(10,0)), ((10,10),(10,31)),\n",
        "((30,-10),(30,0)),\n",
        "((30,10),(30,20)), ((50,-21),(50,31)),\n",
        "((10,-21),(50,-21)),\n",
        "((10,0),(30,0)), ((10,10),(30,10)), ((10,31),(50,31))})\n",
        "trap_body = Rob_body(trap_env,init_pos=(-1,0,90))\n",
        "trap_middle = Rob_middle_layer(trap_body)\n",
        "trap_top = Rob_top_layer(trap_middle,locations={'goal':(71,0)})\n",
        "# pl=Plot_env(trap_body,trap_top)\n",
        "# trap_top.do({'visit':['goal']})"
      ],
      "metadata": {
        "id": "8_ChcdnKZn-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter-2 Searching for Solutions"
      ],
      "metadata": {
        "id": "YV9NLTZkZoBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a)Representing Search Problems\n"
      ],
      "metadata": {
        "id": "BwtcFBYPZoFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A search problem consists of:\n",
        "* a start node\n",
        "* a neighbors function that given a node, returns an enumeration of the\n",
        "arcs from the node\n",
        "* a specification of a goal in terms of a Boolean function that takes a node\n",
        "and returns true if the node is a goal\n",
        "* a (optional) heuristic function that, given a node, returns a non-negative\n",
        "real number. The heuristic function defaults to zero.\n"
      ],
      "metadata": {
        "id": "7s2piKyHGVXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As far as the searcher is concerned a node can be anything. If multiple-path\n",
        "pruning is used, a node must be hashable. In the simple examples, it is a string,\n",
        "but in more complicated examples (in later chapters) it can be a tuple, a frozen\n",
        "set, or a Python object."
      ],
      "metadata": {
        "id": "jc0PKY4AGVaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code raise NotImplementedError() is a way to specify that\n",
        "this is an abstract method that needs to be overridden to define an actual search\n",
        "problem.\n"
      ],
      "metadata": {
        "id": "7BMLqBR6GVdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Search_problem(object):\n",
        "    \"\"\"A search problem consists of:\n",
        "    * a start node\n",
        "    * a neighbors function that gives the neighbors of a node\n",
        "    * a specification of a goal\n",
        "    * a (optional) heuristic function\"\"\"\n",
        "\n",
        " def start_node(self):\n",
        "    \"\"\"returns start node\"\"\"\n",
        "    raise NotImplementedError(\"start_node\") # abstract method\n",
        "\n",
        " def is_goal(self,node):\n",
        "    \"\"\"is True if node is a goal\"\"\"\n",
        "    raise NotImplementedError(\"is_goal\") # abstract method\n",
        "\n",
        " def neighbors(self,node):\n",
        "    \"\"\"returns a list of the arcs for the neighbors of node\"\"\"\n",
        "    raise NotImplementedError(\"neighbors\") # abstract method\n",
        "\n",
        " def heuristic(self,n):\n",
        "   \n",
        "  \"\"\"Gives the heuristic value of node n.\n",
        "  Returns 0 if not overridden.\"\"\"\n",
        "  return 0"
      ],
      "metadata": {
        "id": "VWmoi2x9GVgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neighbors is a list of arcs. A (directed) arc consists of a from node node\n",
        "and a to node node. The arc is the pair ⟨from node, to node⟩, but can also contain\n",
        "a non-negative cost (which defaults to 1) and can be labeled with an action."
      ],
      "metadata": {
        "id": "WcBLiaruGVjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SearchProblem.py"
      ],
      "metadata": {
        "id": "9VRNm3RXGVmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Arc(object):\n",
        " \"\"\"An arc has a from_node and a to_node node and a (non-negative)cost\"\"\"\n",
        " def __init__(self, from_node, to_node, cost=1, action=None):\n",
        "  assert cost >= 0, (\"Cost cannot be negative for\"+\n",
        "  str(from_node)+\"->\"+str(to_node)+\", cost:\"+str(cost))\n",
        "  self.from_node = from_node\n",
        "  self.to_node = to_node\n",
        "  self.action = action\n",
        "  self.cost=cost\n",
        "\n",
        " def __repr__(self):\n",
        "  \"\"\"string representation of an arc\"\"\"\n",
        "  if self.action:\n",
        "    return str(self.from_node)+\" --\"+str(self.action)+\"-->\"+str(self.to_node)\n",
        "  else:\n",
        "    return str(self.from_node)+\" --> \"+str(self.to_node)"
      ],
      "metadata": {
        "id": "S1Fo0ZOYGVo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Explicit Representation of Search Graph"
      ],
      "metadata": {
        "id": "rWd0phsQGVrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first representation of a search problem is from an explicit graph (as opposed to one that is generated as needed).\n"
      ],
      "metadata": {
        "id": "uCm_eY-zGVuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representing Search Problems \n",
        "* a list or set of nodes\n",
        "* a list or set of arcs\n",
        "* a start node\n",
        "* a list or set of goal nodes\n",
        "* (optionally) a dictionary that maps a node to a heuristic value for that\n",
        "node"
      ],
      "metadata": {
        "id": "ii12KBKtGVxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To define a search problem, we need to define the start node, the goal predicate,\n",
        "the neighbors function and the heuristic function."
      ],
      "metadata": {
        "id": "hyDk8vnVGV0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Search_problem_from_explicit_graph(Search_problem):\n",
        "\n",
        "  \"\"\"A search problem consists of:\n",
        "  * a list or set of nodes\n",
        "  * a list or set of arcs\n",
        "  * a start node\n",
        "  * a list or set of goal nodes\n",
        "  * a dictionary that maps each node into its heuristic value.\n",
        "  * a dictionary that maps each node into its (x,y) position\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, nodes, arcs, start=None, goals=set(), hmap={},positions={}):\n",
        "      \n",
        "    self.neighs = {}\n",
        "    self.nodes = nodes\n",
        "\n",
        "    for node in nodes:\n",
        "      self.neighs[node]=[]\n",
        "    self.arcs = arcs\n",
        "\n",
        "    for arc in arcs:\n",
        "      self.neighs[arc.from_node].append(arc)\n",
        "      self.start = start\n",
        "      self.goals = goals\n",
        "      self.hmap = hmap\n",
        "      self.positions = positions\n",
        "\n",
        "  def start_node(self):\n",
        "    \"\"\"returns start node\"\"\"\n",
        "    return self.start \n",
        "\n",
        "  def is_goal(self,node):\n",
        "    \"\"\"is True if node is a goal\"\"\"\n",
        "    return node in self.goals\n",
        "\n",
        "  def neighbors(self,node):\n",
        "    \"\"\"returns the neighbors of node\"\"\"\n",
        "    return self.neighs[node]\n",
        "\n",
        "  def heuristic(self,node):\n",
        "    \"\"\"Gives the heuristic value of node n.\n",
        "    Returns 0 if not overridden in the hmap.\"\"\"\n",
        "    if node in self.hmap:\n",
        "      return self.hmap[node]\n",
        "    else:\n",
        "      return 0\n",
        "      \n",
        "  def __repr__(self):\n",
        "    \"\"\"returns a string representation of the search problem\"\"\"\n",
        "    res=\"\"\n",
        "    for arc in self.arcs:\n",
        "      res += str(arc)+\". \"\n",
        "    return res\n",
        "\n",
        "  def neighbor_nodes(self,node):\n",
        "    \"\"\"returns an iterator over the neighbors of node\"\"\"\n",
        "    return (path.to_node for path in self.neighs[node])\n",
        "\n"
      ],
      "metadata": {
        "id": "7xYBMKzsGV27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1.2 Paths"
      ],
      "metadata": {
        "id": "G4LC8OAcGV5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A searcher will return a path from the start node to a goal node. A Python list\n",
        "is not a suitable representation for a path, as many search algorithms consider\n",
        "multiple paths at once, and these paths should share initial parts of the path.\n",
        "If we wanted to do this with Python lists, we would need to keep copying the\n",
        "list, which can be expensive if the list is long. An alternative representation is\n",
        "used here in terms of a recursive data structure that can share subparts"
      ],
      "metadata": {
        "id": "zPqOICrNGV8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A path is either:\n",
        "* a node (representing a path of length 0) or\n",
        "*  a path, initial and an arc, where the from node of the arc is the node at the end of initial."
      ],
      "metadata": {
        "id": "H8511DUcGV-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These cases are distinguished in the following code by having arc = None if the\n",
        "path has length 0, in which case initial is the node of the path. Python yield is\n",
        "used for enumerations only"
      ],
      "metadata": {
        "id": "TOvFlKDJGWBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PathsSearch.py"
      ],
      "metadata": {
        "id": "eIwYzCJ4GeMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Path(object):\n",
        "  \"\"\"A path is either a node or a path followed by an arc\"\"\"\n",
        "  def __init__(self,initial,arc=None):\n",
        "    \"\"\"initial is either a node (in which case arc is None) or\n",
        "    a path (in which case arc is an object of type Arc)\"\"\"\n",
        "    self.initial = initial\n",
        "\n",
        "    self.arc=arc\n",
        "    if arc is None:\n",
        "      self.cost=0\n",
        "    else:\n",
        "      self.cost = initial.cost+arc.cost\n",
        "  def end(self):\n",
        "    \"\"\"returns the node at the end of the path\"\"\"\n",
        "    if self.arc is None:\n",
        "      return self.initial\n",
        "    else:\n",
        "      return self.arc.to_node\n",
        "  def nodes(self):\n",
        "    \"\"\"enumerates the nodes for the path.\n",
        "    This starts at the end and enumerates nodes in the path backwards.\"\"\"\n",
        "    current = self\n",
        "    while current.arc is not None:\n",
        "      yield current.arc.to_node\n",
        "      current = current.initial\n",
        "    yield current.initial\n",
        "\n",
        "  def initial_nodes(self):\n",
        "      \"\"\"enumerates the nodes for the path before the end node.\n",
        "      This starts at the end and enumerates nodes in the path backwards.\"\"\"\n",
        "      if self.arc is not None:\n",
        "        yield from self.initial.nodes()\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"returns a string representation of a path\"\"\"\n",
        "    if self.arc is None:\n",
        "      return str(self.initial)\n",
        "    elif self.arc.action:\n",
        "      return (str(self.initial)+\"\\n --\"+str(self.arc.action)\n",
        "    +\"--> \"+str(self.arc.to_node))\n",
        "    else:\n",
        "      return str(self.initial)+\" --> \"+str(self.arc.to_node)"
      ],
      "metadata": {
        "id": "r7phvHPeGWEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem1 = Search_problem_from_explicit_graph(\n",
        "{'a','b','c','d','g'},\n",
        "[Arc('a','c',1), Arc('a','b',3), Arc('c','d',3), Arc('c','b',1), Arc('b','d',1), Arc('b','g',3), Arc('d','g',1)],\n",
        "start = 'a',\n",
        "goals = {'g'},\n",
        "positions={'a': (0, 0), 'b': (1, 1), 'c': (0,1), 'd': (1,2), 'g': (2,2)})\n",
        "\n",
        "problem2 = Search_problem_from_explicit_graph(\n",
        " {'a','b','c','d','e','g','h','j'},\n",
        " [Arc('a','b',1), Arc('b','c',3), Arc('b','d',1), Arc('d','e',3),\n",
        " Arc('d','g',1), Arc('a','h',3), Arc('h','j',1)],\n",
        " start = 'a',\n",
        " goals = {'g'},\n",
        " positions={'a': (0, 0), 'b': (0, 1), 'c': (0,4), 'd': (1,1), 'e': (1,4),\n",
        " 'g': (2,1), 'h': (3,0), 'j': (3,1)})\n",
        "\n",
        "problem3 = Search_problem_from_explicit_graph(\n",
        "{'a','b','c','d','e','g','h','j'},[],start = 'g',goals = {'k','g'})\n",
        "\n",
        "acyclic_delivery_problem = Search_problem_from_explicit_graph(\n",
        "{'mail','ts','o103','o109','o111','b1','b2','b3','b4','c1','c2','c3',\n",
        "'o125','o123','o119','r123','storage'},\n",
        "[Arc('ts','mail',6),\n",
        "Arc('o103','ts',8),\n",
        "Arc('o103','b3',4),\n",
        "Arc('o103','o109',12),\n",
        "Arc('o109','o119',16),\n",
        "Arc('o109','o111',4),\n",
        "Arc('b1','c2',3),\n",
        "Arc('b1','b2',6),\n",
        "Arc('b2','b4',3),\n",
        "Arc('b3','b1',4),\n",
        "Arc('b3','b4',7),\n",
        "Arc('b4','o109',7),\n",
        "Arc('c1','c3',8),\n",
        "Arc('c2','c3',6),\n",
        "Arc('c2','c1',4),\n",
        "Arc('o123','o125',4),\n",
        "Arc('o123','r123',4),\n",
        "Arc('o119','o123',9),\n",
        "Arc('o119','storage',7)],\n",
        "start = 'o103',\n",
        "goals = {'r123'},\n",
        "hmap = {\n",
        "'mail' : 26,\n",
        "'ts' : 23,\n",
        "'o103' : 21,\n",
        "'o109' : 24,\n",
        "'o111' : 27,\n",
        "'o119' : 11,\n",
        "'o123' : 4,\n",
        "'o125' : 6,\n",
        "'r123' : 0,\n",
        "'b1' : 13,\n",
        "'b2' : 15,\n",
        "'b3' : 17,\n",
        "'b4' : 18,\n",
        "'c1' : 6,\n",
        "'c2' : 10,\n",
        "'c3' : 12,\n",
        "'storage' : 12})\n",
        "\n",
        "cyclic_delivery_problem = Search_problem_from_explicit_graph(\n",
        "{'mail','ts','o103','o109','o111','b1','b2','b3','b4','c1','c2','c3',\n",
        "'o125','o123','o119','r123','storage'},\n",
        "[ Arc('ts','mail',6), Arc('mail','ts',6),\n",
        "Arc('o103','ts',8), Arc('ts','o103',8),\n",
        "Arc('o103','b3',4),\n",
        "Arc('o103','o109',12), Arc('o109','o103',12),\n",
        "Arc('o109','o119',16), Arc('o119','o109',16),\n",
        "Arc('o109','o111',4), Arc('o111','o109',4),\n",
        "Arc('b1','c2',3),\n",
        "Arc('b1','b2',6), Arc('b2','b1',6),\n",
        "Arc('b2','b4',3), Arc('b4','b2',3),\n",
        "Arc('b3','b1',4), Arc('b1','b3',4),\n",
        "Arc('b3','b4',7), Arc('b4','b3',7),\n",
        "Arc('b4','o109',7),\n",
        "Arc('c1','c3',8), Arc('c3','c1',8),\n",
        "Arc('c2','c3',6), Arc('c3','c2',6),\n",
        "Arc('c2','c1',4), Arc('c1','c2',4),\n",
        "Arc('o123','o125',4), Arc('o125','o123',4),\n",
        "Arc('o123','r123',4), Arc('r123','o123',4),\n",
        "Arc('o119','o123',9), Arc('o123','o119',9),\n",
        "Arc('o119','storage',7), Arc('storage','o119',7)],\n",
        "start = 'o103',\n",
        "goals = {'r123'},\n",
        "hmap = {\n",
        "'mail' : 26,\n",
        "'ts' : 23,\n",
        "'o103' : 21,\n",
        "'o109' : 24,\n",
        "'o111' : 27,\n",
        "'o119' : 11,\n",
        "'o123' : 4,\n",
        "'o125' : 6,\n",
        "'r123' : 0,\n",
        "'b1' : 13,\n",
        "'b2' : 15,\n",
        "'b3' : 17,\n",
        "'b4' : 18,\n",
        "'c1' : 6,\n",
        "'c2' : 10,\n",
        "'c3' : 12,\n",
        "'storage' : 12})"
      ],
      "metadata": {
        "id": "EXA14z9BFJRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Generic Searcher and Variants"
      ],
      "metadata": {
        "id": "D3sc43NgFJUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the search demos, in folder **“aipython”**, load\n",
        "**“searchGeneric.py”** , using e.g., ipython -i searchGeneric.py,\n",
        "and copy and paste the example queries at the bottom of that file. This\n",
        "requires Python 3."
      ],
      "metadata": {
        "id": "9NkKInArFJYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2.1 Searcher"
      ],
      "metadata": {
        "id": "JVmMd0yPFJbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Searcher for a problem can be asked repeatedly for the next path. To solve a\n",
        "problem, we can construct a Searcher object for the problem and then repeatedly\n",
        "ask for the next path using search. If there are no more paths, None is returned."
      ],
      "metadata": {
        "id": "45DQgE76GWHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###searchGeneric.py"
      ],
      "metadata": {
        "id": "0f8GSeKBGWKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from display import Displayable, visualize\n",
        "class Searcher(Displayable):\n",
        "  \"\"\"returns a searcher for a problem.\n",
        "  Paths can be found by repeatedly calling search().\n",
        "  This does depth-first search unless overridden\n",
        "  \"\"\"\n",
        "  def __init__(self, problem):\n",
        "    \"\"\"creates a searcher from a problem\n",
        "    \"\"\"\n",
        "    self.problem = problem\n",
        "    self.initialize_frontier()\n",
        "    self.num_expanded = 0\n",
        "    self.add_to_frontier(Path(problem.start_node()))\n",
        "    super().__init__()\n",
        "  def initialize_frontier(self):\n",
        "    self.frontier = []\n",
        "  def empty_frontier(self):\n",
        "    return self.frontier == []\n",
        "  def add_to_frontier(self,path):\n",
        "    self.frontier.append(path)\n",
        "  @visualize\n",
        "  def search(self):\n",
        "    \"\"\"returns (next) path from the problem's start node\n",
        "    to a goal node.\n",
        "\n",
        "    3. Searching for Solutions\n",
        "    Returns None if no path exists.\n",
        "    \"\"\"\n",
        "    while not self.empty_frontier():\n",
        "\n",
        "      path = self.frontier.pop()\n",
        "      self.display(2, \"Expanding:\",path,\"(cost:\",path.cost,\")\")\n",
        "      self.num_expanded += 1\n",
        "      if self.problem.is_goal(path.end()): # solution found\n",
        "        self.display(1, self.num_expanded, \"paths have been expanded and\",\n",
        "        len(self.frontier), \"paths remain in the frontier\")\n",
        "        self.solution = path # store the solution found\n",
        "        return path\n",
        "      else:\n",
        "        neighs = self.problem.neighbors(path.end())\n",
        "        self.display(3,\"Neighbors are\", neighs)\n",
        "        for arc in reversed(list(neighs)):\n",
        "          self.add_to_frontier(Path(path,arc))\n",
        "          self.display(3,\"Frontier:\",self.frontier)\n",
        "          self.display(1,\"No (more) solutions. Total of\",\n",
        "          self.num_expanded,\"paths expanded.\")\n"
      ],
      "metadata": {
        "id": "RmS0Fu85GWN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this reverses the neigbours so that it implements depth-first search in\n",
        "an intutive manner (expanding the first neighbor first), and list is needed if the\n",
        "neighboure are generated. Reversing the neighbours might not be required for\n",
        "other methods. The calls to reversed and list can be removed, and the algothihm\n",
        "still implements depth-fist search."
      ],
      "metadata": {
        "id": "5-WDxPyKGWRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3.1** When it returns a path, the algorithm can be used to find another\n",
        "path by calling search() again. However, it does not find other paths that go\n",
        "through one goal node to another. Explain why, and change the code so that it\n",
        "can find such paths when search() is called again.\n"
      ],
      "metadata": {
        "id": "SJk_gNvHGWUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2.2 Frontier as a Priority Queue\n"
      ],
      "metadata": {
        "id": "S70wbTfhGWWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many of the search algorithms, such as * A∗ * and other best-first searchers, the\n",
        "frontier is implemented as a priority queue. Here we use the Python’s built-in\n",
        "priority queue implementations, heapq."
      ],
      "metadata": {
        "id": "LERXjEE8GWZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq # part of the Python standard library\n",
        "from searchProblem import Path\n",
        "\n",
        "class FrontierPQ(object):\n",
        "  \"\"\"A frontier consists of a priority queue (heap), frontierpq, of\n",
        "  (value, index, path) triples, where\n",
        "  * value is the value we want to minimize (e.g., path cost + h).\n",
        "  * index is a unique index for each element\n",
        "  * path is the path on the queue\n",
        "  Note that the priority queue always returns the smallest element.\n",
        " \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"constructs the frontier, initially an empty priority queue\n",
        "    \"\"\"\n",
        "    self.frontier_index = 0 # the number of items ever added to the\n",
        "    ontier\n",
        "    self.frontierpq = [] # the frontier priority queue\n",
        "\n",
        "  def empty(self):\n",
        "    \"\"\"is True if the priority queue is empty\"\"\"\n",
        "    return self.frontierpq == []\n",
        "\n",
        "  def add(self, path, value):\n",
        "    \"\"\"add a path to the priority queue\n",
        "    value is the value to be minimized\"\"\"\n",
        "    self.frontier_index += 1 # get a new unique index\n",
        "    heapq.heappush(self.frontierpq,(value, -self.frontier_index, path))\n",
        "\n",
        "  def pop(self):\n",
        "    \"\"\"returns and removes the path of the frontier with minimum value.\n",
        "    \"\"\"\n",
        "    (_,_,path) = heapq.heappop(self.frontierpq)\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "FsR6kKPeGWcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following methods are used for finding and printing information about\n",
        "the frontier."
      ],
      "metadata": {
        "id": "0wdZ5zsKGWfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###searchGeneric.py"
      ],
      "metadata": {
        "id": "gOoxOrYdGWiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count(self,val):\n",
        "  \"\"\"returns the number of elements of the frontier with value=val\"\"\"\n",
        "  return sum(1 for e in self.frontierpq if e[0]==val)\n",
        "\n",
        "\n",
        "def __repr__(self):\n",
        "  \"\"\"string representation of the frontier\"\"\"\n",
        "  return str([(n,c,str(p)) for (n,c,p) in self.frontierpq])\n",
        "\n",
        "def __len__(self):\n",
        "  \"\"\"length of the frontier\"\"\"\n",
        "  return len(self.frontierpq)\n",
        "  \n",
        "def __iter__(self):\n",
        "  \"\"\"iterate through the paths in the frontier\"\"\"\n",
        "  for (_,_,path) in self.frontierpq:\n",
        "    yield path\n"
      ],
      "metadata": {
        "id": "QFNYsLY5GWll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2.3 A∗ Search"
      ],
      "metadata": {
        "id": "iNo0uDKPGWor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For an **A∗ Search** the frontier is implemented using the FrontierPQ class."
      ],
      "metadata": {
        "id": "fRRHx-FwGWr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AStarSearcher(Searcher):\n",
        "  \"\"\"returns a searcher for a problem.\n",
        "  Paths can be found by repeatedly calling search().\n",
        "  \"\"\"\n",
        "  def __init__(self, problem):\n",
        "    super().__init__(problem)\n",
        "\n",
        "  def initialize_frontier(self):\n",
        "    self.frontier = FrontierPQ()\n",
        "\n",
        "  def empty_frontier(self):\n",
        "    return self.frontier.empty()\n",
        "\n",
        "  def add_to_frontier(self,path):\n",
        "    \"\"\"add path to the frontier with the appropriate cost\"\"\"\n",
        "    value = path.cost+self.problem.heuristic(path.end())\n",
        "    self.frontier.add(path, value)\n",
        "    # Code should always be tested. The following provides a simple unit test,\n",
        "    # using problem1 as the the default problem.\n",
        "   \n",
        "import searchProblem as searchProblem\n",
        "\n",
        "def test(SearchClass, problem=searchProblem.problem1,\n",
        "  solutions=[['g','d','b','c','a']] ):\n",
        "  \"\"\"Unit test for aipython searching algorithms.\n",
        "  SearchClass is a class that takes a problemm and implements search()\n",
        "  problem is a search problem\n",
        "  solutions is a list of optimal solutions\n",
        "  \"\"\"\n",
        "  print(\"Testing problem 1:\")\n",
        "  schr1 = SearchClass(problem)\n",
        "  path1 = schr1.search()\n",
        "\n",
        "\n",
        "  print(\"Path found:\",path1)\n",
        "  assert path1 is not None, \"No path is found in problem1\"\n",
        "  assert list(path1.nodes()) in solutions, \"Shortest path not found in problem1\"\n",
        "  print(\"Passed unit test\")\n",
        "if __name__ == \"__main__\":\n",
        "  #test(Searcher)\n",
        "  test(AStarSearcher)\n",
        "  # example queries:\n",
        "  # searcher1 = Searcher(searchProblem.acyclic_delivery_problem) # DFS\n",
        "  # searcher1.search() # find first path\n",
        "  # searcher1.search() # find next path\n",
        "  # searcher2 = AStarSearcher(searchProblem.acyclic_delivery_problem) # A*\n",
        "  # searcher2.search() # find first path\n",
        "  # searcher2.search() # find next path\n",
        "  # searcher3 = Searcher(searchProblem.cyclic_delivery_problem) # DFS\n",
        "  # searcher3.search() # find first path with DFS. What do you expect to happen?\n",
        "  # searcher4 = AStarSearcher(searchProblem.cyclic_delivery_problem) # A*\n",
        "  # searcher4.search() # find first path"
      ],
      "metadata": {
        "id": "ok8jIfHVGWvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2.4 Multiple Path Pruning\n"
      ],
      "metadata": {
        "id": "utLhu9_uGWyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the multiple-path pruning demo, in folder **aipython**, load\n",
        "**searchMPP.py** , using e.g., ipython -i searchMPP.py, and copy and\n",
        "paste the example queries at the bottom of that file"
      ],
      "metadata": {
        "id": "o_rbz8wZGW1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following implements A∗ with multiple-path pruning. It overrides search() in Searcher.\n"
      ],
      "metadata": {
        "id": "FT8IIkcrGW4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###searchMPP.py"
      ],
      "metadata": {
        "id": "ZEkwDZJMGW7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SearcherMPP(AStarSearcher):\n",
        "  \"\"\"returns a searcher for a problem.\n",
        "  Paths can be found by repeatedly calling search().\n",
        "  \"\"\"\n",
        "  def __init__(self, problem):\n",
        "    super().__init__(problem)\n",
        "    self.explored = set()\n",
        "\n",
        "  @visualize\n",
        "  def search(self):\n",
        "    \"\"\"returns next path from an element of problem's start nodes\n",
        "    to a goal node.\n",
        "    Returns None if no path exists.\n",
        "    \"\"\"\n",
        "    while not self.empty_frontier():\n",
        "      path = self.frontier.pop()\n",
        "      if path.end() not in self.explored:\n",
        "        self.display(2, \"Expanding:\",path,\"(cost:\",path.cost,\")\")\n",
        "        self.explored.add(path.end())\n",
        "        self.num_expanded += 1\n",
        "      if self.problem.is_goal(path.end()):\n",
        "        self.display(1, self.num_expanded, \"paths have been expanded and\",\n",
        "        len(self.frontier), \"paths remain in the frontier\")\n",
        "        self.solution = path # store the solution found\n",
        "        return path\n",
        "      else:\n",
        "        neighs = self.problem.neighbors(path.end())\n",
        "        self.display(3,\"Neighbors are\", neighs)\n",
        "        for arc in neighs:\n",
        "          self.add_to_frontier(Path(path,arc))\n",
        "          self.display(3,\"Frontier:\",self.frontier)\n",
        "          self.display(1,\"No (more) solutions. Total of\",\n",
        "          self.num_expanded,\"paths expanded.\")\n",
        "\n",
        "from searchGeneric import test\n",
        "if __name__ == \"__main__\":\n",
        " test(SearcherMPP)\n",
        "\n",
        "import searchProblem\n",
        " # searcherMPPcdp = SearcherMPP(searchProblem.cyclic_delivery_problem)\n",
        " # print(searcherMPPcdp.search()) # find first path"
      ],
      "metadata": {
        "id": "qsKCJUitGW9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Branch-and-bound Search\n"
      ],
      "metadata": {
        "id": "pkdPuqajkXAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the demo, in folder “aipython”, load\n",
        "**searchBranchAndBound.py**, and copy and paste the example queries\n",
        "at the bottom of that file.\n"
      ],
      "metadata": {
        "id": "pCdaCwK_kXFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth-first search methods do not need an a priority queue, but can use\n",
        "a list as a stack. In this implementation of branch-and-bound search, we call\n",
        "search to find an optimal solution with cost less than bound. This uses depthfirst search to find a path to a goal that extends path with cost less than the\n",
        "bound. Once a path to a goal has been found, that path is remembered as the\n",
        "best path, the bound is reduced, and the search continues"
      ],
      "metadata": {
        "id": "t2FLUb5UkXIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###searchBranchAndBound.py"
      ],
      "metadata": {
        "id": "tdJdOju1kXLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from searchProblem import Path\n",
        "from searchGeneric import Searcher\n",
        "from display import Displayable, visualize\n",
        "class DF_branch_and_bound(Searcher):\n",
        "    \"\"\"returns a branch and bound searcher for a problem.\n",
        "    17 An optimal path with cost less than bound can be found by calling\n",
        "    search()\n",
        "    \"\"\"\n",
        "    def __init__(self, problem, bound=float(\"inf\")):\n",
        "      \"\"\"creates a searcher than can be used with search() to find an optimal path.\n",
        "      bound gives the initial bound. By default this is infinite - meaning there\n",
        "      is no initial pruning due to depth bound\n",
        "      \"\"\"\n",
        "      super().__init__(problem)\n",
        "      self.best_path = None\n",
        "      self.bound = bound\n",
        "      @visualize\n",
        "      def search(self):\n",
        "        \"\"\"returns an optimal solution to a problem with cost less than bound.\n",
        "        returns None if there is no solution with cost less than bound.\"\"\"\n",
        "        self.frontier = [Path(self.problem.start_node())]\n",
        "        self.num_expanded = 0\n",
        "        while self.frontier:\n",
        "          path = self.frontier.pop()\n",
        "          if path.cost+self.problem.heuristic(path.end()) < self.bound:\n",
        "      # if path.end() not in path.initial_nodes(): # for cycle\n",
        "\n",
        "            self.display(3,\"Expanding:\",path,\"cost:\",path.cost)\n",
        "            self.num_expanded += 1\n",
        "            if self.problem.is_goal(path.end()):\n",
        "              self.best_path = path\n",
        "              self.bound = path.cost\n",
        "              self.display(2,\"New best path:\",path,\" cost:\",path.cost)\n",
        "            else:\n",
        "              neighs = self.problem.neighbors(path.end())\n",
        "              self.display(3,\"Neighbors are\", neighs)\n",
        "              for arc in reversed(list(neighs)):\n",
        "                self.add_to_frontier(Path(path, arc))\n",
        "                self.display(1,\"Number of paths expanded:\",self.num_expanded,\n",
        "                \"(optimal\" if self.best_path else \"(no\", \"solution found)\")\n",
        "                self.solution = self.best_path\n",
        "          \n",
        "          return self.best_path\n"
      ],
      "metadata": {
        "id": "l1CJlq9ukXOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this code used reversed in order to expand the neighbors of a node\n",
        "in the left-to-right order one might expect. It does this because pop() removes\n",
        "the rightmost element of the list. The call to list is there because reversed only\n",
        "works on lists and tuples, but the neighbours can be generated."
      ],
      "metadata": {
        "id": "XNSUTp6SkXQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from searchGeneric import test\n",
        "if __name__ == \"__main__\":\n",
        "  test(DF_branch_and_bound)\n",
        "  \n",
        "# Example queries:\n",
        "import searchProblem\n",
        "# searcherb1 = DF_branch_and_bound(searchProblem.acyclic_delivery_problem)\n",
        "# print(searcherb1.search()) # find optimal path\n",
        "# searcherb2 = DF_branch_and_bound(searchProblem.cyclic_delivery_problem, bound=100)\n",
        "# print(searcherb2.search()) # find optimal path"
      ],
      "metadata": {
        "id": "TwSZGUzdkXTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###searchTest.py "
      ],
      "metadata": {
        "id": "a__GXu5okXVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from searchGeneric import Searcher, AStarSearcher\n",
        "from searchBranchAndBound import DF_branch_and_bound\n",
        "from searchMPP import SearcherMPP\n",
        "DF_branch_and_bound.max_display_level = 1\n",
        "Searcher.max_display_level = 1\n",
        "def run(problem,name):\n",
        "\n",
        "    print(\"\\n\\n*******\",name)\n",
        "    print(\"\\nA*:\")\n",
        "    asearcher = AStarSearcher(problem)\n",
        "    print(\"Path found:\",asearcher.search(),\" cost=\",asearcher.solution.cost)\n",
        "    print(\"there are\",asearcher.frontier.count(asearcher.solution.cost),\"elements remaining on the queue with f-value=\",asearcher.solution.cost)\n",
        "    print(\"\\nA* with MPP:\"),\n",
        "    msearcher = SearcherMPP(problem)\n",
        "    print(\"Path found:\",msearcher.search(),\" cost=\",msearcher.solution.cost)\n",
        "    print(\"there are\",msearcher.frontier.count(msearcher.solution.cost),\"elements remaining on the queue with f-value=\",msearcher.solution.cost)\n",
        "    bound = asearcher.solution.cost+0.01\n",
        "    print(\"\\nBranch and bound (with too-good initial bound of\", bound,\")\")\n",
        "    tbb = DF_branch_and_bound(problem,bound) # cheating!!!!\n",
        "    print(\"Path found:\",tbb.search(),\" cost=\",tbb.solution.cost)\n",
        "    print(\"Rerunning B&B\")\n",
        "    print(\"Path found:\",tbb.search())\n",
        "    bbound = asearcher.solution.cost*2+10\n",
        "    print(\"\\nBranch and bound (with not-very-good initial bound of\", bbound, \")\")\n",
        "    tbb2 = DF_branch_and_bound(problem,bbound) # cheating!!!!\n",
        "    print(\"Path found:\",tbb2.search(),\" cost=\",tbb2.solution.cost)\n",
        "    print(\"Rerunning B&B\")\n",
        "    print(\"Path found:\",tbb2.search())\n",
        "    print(\"\\nDepth-first search: (Use ˆC if it goes on forever)\")\n",
        "    tsearcher = Searcher(problem)\n",
        "    print(\"Path found:\",tsearcher.search(),\" cost=\",tsearcher.solution.cost)\n",
        "import searchProblem\n",
        "from searchTest import run\n",
        "if __name__ == \"__main__\":\n",
        "  run(searchProblem.problem1,\"Problem 1\")\n",
        "  # run(searchProblem.acyclic_delivery_problem,\"Acyclic Delivery\")\n",
        "  # run(searchProblem.cyclic_delivery_problem,\"Cyclic Delivery\")\n",
        "\n",
        "  # also test some graphs with cycles, and some with multiple least-cost paths"
      ],
      "metadata": {
        "id": "Ck4xcjssnITS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wJnjP0O3nIZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter-4 Supervised Machine Learning"
      ],
      "metadata": {
        "id": "BNp_yVj8nIeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.1 Representations of Data and Predictions"
      ],
      "metadata": {
        "id": "csA6jp-3GXb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###learnProblem.py"
      ],
      "metadata": {
        "id": "Db24bmQcGXe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random, statistics\n",
        "import csv\n",
        "from display import Displayable\n",
        "from utilities import argmax\n",
        "\n",
        "boolean = [False, True]\n"
      ],
      "metadata": {
        "id": "E-cvvGNcGXhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When creating a data set, we partition the data into a training set (train) and\n",
        "a test set (test). The target feature is the feature that we are making a prediction\n",
        "of. A dataset ds has the following attributes"
      ],
      "metadata": {
        "id": "bXMiKKuIGXkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ds.train a list of the training examples\n",
        "* ds.test a list of the test examples\n",
        "* ds.target_index the index of the target\n",
        "* ds.target the feature corresponding to the target (a function as described\n",
        "above)\n",
        "* ds.input_features a list of the input features"
      ],
      "metadata": {
        "id": "8zGxYaoIAenk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_set(Displayable):\n",
        " \"\"\" A data set consists of a list of training data and a list of test data.\n",
        " \"\"\"\n",
        "\n",
        " def __init__(self, train, test=None, prob_test=0.20, target_index=0,\n",
        "    header=None, target_type= None, seed=None): #12345):\n",
        "    \"\"\"A dataset for learning.\n",
        "    train is a list of tuples representing the training examples\n",
        "    test is the list of tuples representing the test examples\n",
        "    if test is None, a test set is created by selecting each\n",
        "    example with probability prob_test\n",
        "    target_index is the index of the target.\n",
        "    If negative, it counts from right.\n",
        "    If target_index is larger than the number of properties,\n",
        "    there is no target (for unsupervised learning)\n",
        "    header is a list of names for the features\n",
        "    target_type is either None for automatic detection of target type\n",
        "    or one of \"numerical\", \"boolean\", \"cartegorical\"\n",
        "    seed is for random number; None gives a different test set each time\n",
        "    \"\"\"\n",
        "    if seed: # given seed makes partition consistent from run-to-run\n",
        "      random.seed(seed)\n",
        "    if test is None:\n",
        "      train,test = partition_data(train, prob_test)\n",
        "    self.train = train\n",
        "    self.test = test\n",
        "\n",
        "    self.display(1,\"Training set has\",len(train),\"examples. Number of columns: \",{len(e) for e in train})\n",
        "    self.display(1,\"Test set has\",len(test),\"examples. Number of columns: \",{len(e) for e in test})\n",
        "    self.prob_test = prob_test\n",
        "    self.num_properties = len(self.train[0])\n",
        "    if target_index < 0: #allows for -1, -2, etc.\n",
        "      self.target_index = self.num_properties + target_index\n",
        "    else:\n",
        "      self.target_index = target_index\n",
        "      self.header = header\n",
        "      self.domains = [set() for i in range(self.num_properties)]\n",
        "    for example in self.train:\n",
        "      for ind,val in enumerate(example):\n",
        "        self.domains[ind].add(val)\n",
        "    self.conditions_cache = {} # cache for computed conditions\n",
        "    self.create_features()\n",
        "    if target_type:\n",
        "      self.target.ftype = target_type\n",
        "      self.display(1,\"There are\",len(self.input_features),\"input features\")\n",
        "\n",
        " def __str__(self):\n",
        "  if self.train and len(self.train)>0:\n",
        "    return (\"Data: \"+str(len(self.train))+\" training examples, \" +str(len(self.test))+\" test examples, \"+str(len(self.train[0]))+\" features.\")\n",
        "  else:\n",
        "    return (\"Data: \"+str(len(self.train))+\" training examples, \"+str(len(self.test))+\" test examples.\")\n",
        "\n",
        "def create_features(self):\n",
        "    \"\"\"create the set of features\n",
        "    \"\"\"\n",
        "    self.target = None\n",
        "    self.input_features = []\n",
        "    for i in range(self.num_properties):\n",
        "      def feat(e,index=i):\n",
        "        return e[index]\n",
        "        if self.header:\n",
        "          feat.__doc__ = self.header[i]\n",
        "        else:\n",
        "          feat.__doc__ = \"e[\"+str(i)+\"]\"\n",
        "          feat.frange = list(self.domains[i])\n",
        "          feat.ftype = self.infer_type(feat.frange)\n",
        "        if i == self.target_index:\n",
        "            self.target = feat\n",
        "        else:\n",
        "            self.input_features.append(feat)\n",
        "\n",
        "def infer_type(self,domain):\n",
        "  \"\"\"Infers the type of a feature with domain\n",
        "  \"\"\"\n",
        "  if all(v in {True,False} for v in domain):\n",
        "    return \"boolean\"\n",
        "  if all(isinstance(v,(float,int)) for v in domain):\n",
        "      return \"numeric\"\n",
        "  else:\n",
        "      return \"categoric\"\n",
        "      "
      ],
      "metadata": {
        "id": "k8E9djdUUnc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.1.1 Creating Boolean Conditions from Features"
      ],
      "metadata": {
        "id": "-pZvKqVoUniM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the algorithms require Boolean input features or features with range\n",
        "{0, 1}. In order to be able to use these algorithms on datasets that allow for\n",
        "arbitrary domains of input variables, we construct Boolean conditions from\n",
        "the attributes."
      ],
      "metadata": {
        "id": "HftQ5AIcUnll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 cases:\n",
        "*  When the range only has two values, we designate one to be the “true”\n",
        "value.\n",
        "* When the values are all numeric, we assume they are ordered (as opposed\n",
        "to just being some classes that happen to be labelled with numbers) and\n",
        "construct Boolean features for splits of the data. That is, the feature is\n",
        "e[ind] < cut for some value cut. We choose a number of cut values, up to\n",
        "a maximum number of cuts, given by max num cuts.\n",
        "* When the values are not all numeric, we create an indicator function for\n",
        "each value. An indicator function for a value returns true when that value\n",
        "is given and false otherwise. Note that we can’t create an indicator function for values that appear in the test set but not in the training set because we haven’t seen the test set. For the examples in the test set with a\n",
        "value that doesn’t appear in the training set for that feature, the indicator\n",
        "functions all return false.\n"
      ],
      "metadata": {
        "id": "2y40a8-rUnn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conditions(self, max_num_cuts=8, categorical_only = False):\n",
        "  \"\"\"returns a set of boolean conditions from the input features\n",
        "  max_num_cuts is the maximum number of cute for numerical features\n",
        "  categorical_only is true if only categorical features are made\n",
        "  binary\n",
        "  \"\"\"\n",
        "  if (max_num_cuts, categorical_only) in self.conditions_cache:\n",
        "    return self.conditions_cache[(max_num_cuts, categorical_only)]\n",
        "    conds = []\n",
        "    for ind,frange in enumerate(self.domains):\n",
        "      if ind != self.target_index and len(frange)>1:\n",
        "        if len(frange) == 2:\n",
        "          # two values, the feature is equality to one of them.\n",
        "          true_val = list(frange)[1] # choose one as true\n",
        "          def feat(e, i=ind, tv=true_val):\n",
        "            return e[i]==tv\n",
        "\n",
        "          if self.header:\n",
        "            feat.__doc__ = f\"{self.header[ind]}=={true_val}\"\n",
        "          else:\n",
        "            feat.__doc__ = f\"e[{ind}]=={true_val}\"\n",
        "            feat.frange = boolean\n",
        "            feat.ftype = \"boolean\"\n",
        "            conds.append(feat)\n",
        "        elif all(isinstance(val,(int,float)) for val in frange):\n",
        "            if categorical_only: # numerical, don't make cuts\n",
        "              def feat(e, i=ind):\n",
        "                return e[i]\n",
        "              feat.__doc__ = f\"e[{ind}]\"\n",
        "              \n",
        "              conds.append(feat)\n",
        "      else:\n",
        "        # all numeric, create cuts of the data\n",
        "        sorted_frange = sorted(frange)\n",
        "        num_cuts = min(max_num_cuts,len(frange))\n",
        "        cut_positions = [len(frange)*i//num_cuts for i in\n",
        "        range(1,num_cuts)]\n",
        "        for cut in cut_positions:\n",
        "          cutat = sorted_frange[cut]\n",
        "          def feat(e, ind_=ind, cutat=cutat):\n",
        "            return e[ind_] < cutat\n",
        "\n",
        "          if self.header:\n",
        "            feat.__doc__ = self.header[ind]+\"<\"+str(cutat)\n",
        "          else:\n",
        "            feat.__doc__ = \"e[\"+str(ind)+\"]<\"+str(cutat)\n",
        "            feat.frange = boolean\n",
        "            feat.ftype = \"boolean\"\n",
        "            conds.append(feat)\n",
        "    else:\n",
        "    # create an indicator function for every value\n",
        "      for val in frange:\n",
        "        def feat(e, ind_=ind, val_=val):\n",
        "          return e[ind_] == val_\n",
        "        if self.header:\n",
        "          feat.__doc__ = self.header[ind]+\"==\"+str(val)\n",
        "        else:\n",
        "          feat.__doc__= \"e[\"+str(ind)+\"]==\"+str(val)\n",
        "          feat.frange = boolean\n",
        "          feat.ftype = \"boolean\"\n",
        "          conds.append(feat)\n",
        "    self.conditions_cache[(max_num_cuts, categorical_only)] = conds\n",
        "    return conds"
      ],
      "metadata": {
        "id": "1iJKf0LSUnqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.1.2 Evaluating Predictions"
      ],
      "metadata": {
        "id": "rczPXgAMUnsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_dataset(self, data, predictor, error_measure):\n",
        " \"\"\"Evaluates predictor on data according to the error_measure\n",
        " predictor is a function that takes an example and returns a\n",
        " prediction for the target features.\n",
        " error_measure(prediction,actual) -> non-negative real\n",
        " \"\"\"\n",
        " if data:\n",
        "  try:\n",
        "    value = statistics.mean(error_measure(predictor(e),self.target(e))\n",
        "    for e in data)\n",
        "  except ValueError: # if error_measure gives an error\n",
        "    return float(\"inf\") # infinity\n",
        "  return value\n",
        " else:\n",
        "  return math.nan # not a number\n"
      ],
      "metadata": {
        "id": "8xSYaw0oUnus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluate(object):\n",
        " \"\"\"A container for the evaluation measures\"\"\"\n",
        "\n",
        " def squared_loss(prediction, actual):\n",
        "  \"squared loss \"\n",
        "  if isinstance(prediction, (list,dict)):\n",
        "    return (1-prediction[actual])**2 # the correct value is 1\n",
        "  else:\n",
        "    return (prediction-actual)**2\n",
        "\n",
        " def absolute_loss(prediction, actual):\n",
        "  \"absolute loss \"\n",
        "  if isinstance(prediction, (list,dict)):\n",
        "    return abs(1-prediction[actual]) # the correct value is 1\n",
        "  else:\n",
        "    return abs(prediction-actual)\n",
        "\n",
        " def log_loss(prediction, actual):\n",
        "  \"log loss (bits)\"\n",
        "  try:\n",
        "    if isinstance(prediction, (list,dict)):\n",
        "      return -math.log2(prediction[actual])\n",
        "    else:\n",
        "      return -math.log2(prediction) if actual==1 else-math.log2(1-prediction)\n",
        "  except ValueError:\n",
        "    return float(\"inf\") # infinity\n",
        "\n",
        " def accuracy(prediction, actual):\n",
        "  \"accuracy \"\n",
        "  if isinstance(prediction, dict):\n",
        "    prev_val = prediction[actual]\n",
        "  return 1 if all(prev_val >= v for v in prediction.values()) else 0\n",
        "  if isinstance(prediction, list):\n",
        "    prev_val = prediction[actual]\n",
        "    return 1 if all(prev_val >= v for v in prediction) else 0\n",
        "  else:\n",
        "    return 1 if abs(actual-prediction) <= 0.5 else 0\n",
        "\n",
        " all_criteria = [accuracy, absolute_loss, squared_loss, log_loss]"
      ],
      "metadata": {
        "id": "zHE6-UmvUnw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.1.3 Creating Test and Training Sets"
      ],
      "metadata": {
        "id": "SkWrh_9lUn0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following method partitions the data into a training set and a test set. Note\n",
        "that this does not guarantee that the test set will contain exactly a proportion of the data equal to prob test.\n",
        "[An alternative is to use random.sample() which can guarantee that the test\n",
        "set will contain exactly a particular proportion of the data. However this would\n",
        "require knowing how many elements are in the data set, which we may not\n",
        "know, as data may just be a generator of the data (e.g., when reading the data\n",
        "from a file).]\n"
      ],
      "metadata": {
        "id": "55y7mplfUn3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_data(data, prob_test=0.30):\n",
        "  \"\"\"partitions the data into a training set and a test set, where\n",
        "  prob_test is the probability of each example being in the test set.\n",
        "  \"\"\"\n",
        "  train = []\n",
        "  test = []\n",
        "  for example in data:\n",
        "    if random.random() < prob_test:\n",
        "      test.append(example)\n",
        "    else:\n",
        "      train.append(example)\n",
        "  return train, test\n"
      ],
      "metadata": {
        "id": "nhHVKBhmUn7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.1.4 Importing Data From File"
      ],
      "metadata": {
        "id": "RhzcuAseUn-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_from_file(Data_set):\n",
        " def __init__(self, file_name, separator=',', num_train=None,prob_test=0.3,has_header=False, target_index=0, boolean_features=True,\n",
        "              categorical=[], target_type= None, include_only=None,seed=None): #seed=12345):\n",
        "              \"\"\"create a dataset from a file\n",
        "                  separator is the character that separates the attributes\n",
        "                  num_train is a number specifying the first num_train tuples are training, or None\n",
        "                  prob_test is the probability an example should in the test set (if num_train is None)\n",
        "                  has_header is True if the first line of file is a header\n",
        "                  target_index specifies which feature is the target\n",
        "                  boolean_features specifies whether we want to create Boolean features\n",
        "                  (if False, it uses the original features).\n",
        "                  categorical is a set (or list) of features that should be treated as categorical\n",
        "                  target_type is either None for automatic detection of target type\n",
        "                  or one of \"numerical\", \"boolean\", \"cartegorical\"\n",
        "                  include_only is a list or set of indexes of columns to include\n",
        "                  \"\"\"\n",
        "              self.boolean_features = boolean_features\n",
        "              with open(file_name,'r',newline='') as csvfile:\n",
        "                self.display(1,\"Loading\",file_name)\n",
        "                # data_all = csv.reader(csvfile,delimiter=separator) # for more complicated CSV files\n",
        "                data_all = (line.strip().split(separator) for line in csvfile)\n",
        "                if include_only is not None:\n",
        "                  data_all = ([v for (i,v) in enumerate(line) if i in include_only]\n",
        "                for line in data_all)\n",
        "                  if has_header:\n",
        "                    header = next(data_all)\n",
        "                  else:\n",
        "                    header = None\n",
        "                    data_tuples = (interpret_elements(d) for d in data_all if len(d)>1)\n",
        "                    if num_train is not None:\n",
        "                    # training set is divided into training then text examples\n",
        "                    # the file is only read once, and the data is placed in appropriate list\n",
        "                      train = []\n",
        "                      for i in range(num_train): # will give an error if insufficient examples\n",
        "                          train.append(next(data_tuples))\n",
        "                          test = list(data_tuples)\n",
        "                          Data_set.__init__(self,train, test=test,target_index=target_index,header=header)\n",
        "                    else: # randomly assign training and test examples\n",
        "                      Data_set.__init__(self,data_tuples, test=None,prob_test=prob_test,\n",
        "                                            target_index=target_index, header=header,seed=seed, target_type=target_type)\n"
      ],
      "metadata": {
        "id": "bu5clVI-UoAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class is used for datasets where the training and test are in different files"
      ],
      "metadata": {
        "id": "hPj7lsQpUoCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_from_files(Data_set):\n",
        " def __init__(self, train_file_name, test_file_name, separator=',',\n",
        " has_header=False, target_index=0, boolean_features=True,\n",
        " categorical=[], target_type= None, include_only=None):\n",
        "    \"\"\"create a dataset from separate training and file\n",
        "    separator is the character that separates the attributes\n",
        "    num_train is a number specifying the first num_train tuples are training, or None\n",
        "    prob_test is the probability an example should in the test set (if num_train is None)\n",
        "    has_header is True if the first line of file is a header\n",
        "    target_index specifies which feature is the target\n",
        "    boolean_features specifies whether we want to create Boolean features\n",
        "    (if False, it uses the original features).\n",
        "    categorical is a set (or list) of features that should be treated as categorical\n",
        "    target_type is either None for automatic detection of target type\n",
        "    or one of \"numerical\", \"boolean\", \"cartegorical\"\n",
        "    include_only is a list or set of indexes of columns to include\n",
        "    \"\"\"\n",
        "    self.boolean_features = boolean_features\n",
        "    with open(train_file_name,'r',newline='') as train_file:\n",
        "      with open(test_file_name,'r',newline='') as test_file:\n",
        "      # data_all = csv.reader(csvfile,delimiter=separator) # for more complicated CSV files\n",
        "        train_data = (line.strip().split(separator) for line in train_file)\n",
        "        test_data = (line.strip().split(separator) for line in test_file)\n",
        "        if include_only is not None:\n",
        "          train_data = ([v for (i,v) in enumerate(line) if i in include_only] for line in train_data)\n",
        "          test_data = ([v for (i,v) in enumerate(line) if i in include_only] for line in test_data)\n",
        "        if has_header: # this assumes the training file has a header and the test file doesn't\n",
        "          header = next(train_data)\n",
        "        else:\n",
        "          header = None\n",
        "          train_tuples = [interpret_elements(d) for d in train_data if len(d)>1]\n",
        "          test_tuples = [interpret_elements(d) for d in test_data if len(d)>1]\n",
        "          Data_set.__init__(self,train_tuples, test_tuples,target_index=target_index, header=header)\n",
        "          "
      ],
      "metadata": {
        "id": "JaHl0-AJUoE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When reading from a file all of the values are strings. This next method\n",
        "tries to convert each values into a number (an int or a float) or Boolean, if it is\n",
        "possible"
      ],
      "metadata": {
        "id": "733CoU7ZUoHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_elements(str_list):\n",
        "  \"\"\"make the elements of string list str_list numerical if possible.\n",
        "  Otherwise remove initial and trailing spaces.\n",
        "  \"\"\"\n",
        "  res = []\n",
        "  for e in str_list:\n",
        "    try:\n",
        "      res.append(int(e))\n",
        "    except ValueError:\n",
        "      try:\n",
        "        res.append(float(e))\n",
        "      except ValueError:\n",
        "        se = e.strip()\n",
        "        if se in [\"True\",\"true\",\"TRUE\"]:\n",
        "          res.append[True]\n",
        "        if se in [\"False\",\"false\",\"FALSE\"]:\n",
        "          res.append[False]\n",
        "        else:\n",
        "          res.append(e.strip())\n",
        "  return res"
      ],
      "metadata": {
        "id": "8iiviqVhUoJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.1.5 Augmented Features\n"
      ],
      "metadata": {
        "id": "vdgJWWlecdNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes we want to augment the features with new features computed from\n",
        "the old features (eg. the product of features). Here we allow the creation of\n",
        "a new dataset from an old dataset but with new features. Note that special\n",
        "cases of these are **kernels** mapping the original feature space into a new space,\n",
        "which allow a neat way to do learning in the augmented space (the “kernel\n",
        "trick”). This is beyond the scope of AIPython; those interested should read\n",
        "about support vector machines."
      ],
      "metadata": {
        "id": "g7t10QtOsefz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A feature is a function of examples. A unary feature constructor takes a feature and returns a new feature. A binary feature combiner takes two features\n",
        "and returns a new feature"
      ],
      "metadata": {
        "id": "2_kWr7NEseiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_set_augmented(Data_set):\n",
        "  def __init__(self, dataset, unary_functions=[], binary_functions=[],include_orig=True):\n",
        "      \"\"\"creates a dataset like dataset but with new features\n",
        "      unary_function is a list of unary feature constructors\n",
        "      binary_functions is a list of binary feature combiners.\n",
        "      include_orig specifies whether the original features should be\n",
        "      included\n",
        "      \"\"\"\n",
        "      self.orig_dataset = dataset\n",
        "      self.unary_functions = unary_functions\n",
        "      self.binary_functions = binary_functions\n",
        "      self.include_orig = include_orig\n",
        "      self.target = dataset.target\n",
        "      Data_set.__init__(self,dataset.train, test=dataset.test,\n",
        "      target_index = dataset.target_index)\n",
        "\n",
        "  def create_features(self):\n",
        "    if self.include_orig:\n",
        "      self.input_features = self.orig_dataset.input_features.copy()\n",
        "    else:\n",
        "      self.input_features = []\n",
        "    for u in self.unary_functions:\n",
        "      for f in self.orig_dataset.input_features:\n",
        "        self.input_features.append(u(f))\n",
        "        for b in self.binary_functions:\n",
        "          for f1 in self.orig_dataset.input_features:\n",
        "            for f2 in self.orig_dataset.input_features:\n",
        "              if f1 != f2:\n",
        "                self.input_features.append(b(f1,f2))\n",
        "\n",
        "def square(f):\n",
        "  \"\"\"a unary feature constructor to construct the square of a feature\n",
        "  \"\"\"\n",
        "  def sq(e):\n",
        "    return f(e)**2\n",
        "  sq.__doc__ = f.__doc__+\"**2\"\n",
        "  return sq\n",
        "\n",
        "def power_feat(n):\n",
        "  \"\"\"given n returns a unary feature constructor to construct the nth power of a feature.\n",
        "  e.g., power_feat(2) is the same as square, defined above\n",
        "  \"\"\"\n",
        "  def fn(f,n=n):\n",
        "    def pow(e,n=n):\n",
        "      return f(e)**n\n",
        "    pow.__doc__ = f.__doc__+\"**\"+str(n)\n",
        "    return pow\n",
        "  return fn\n",
        "\n",
        "def prod_feat(f1,f2):\n",
        "  \"\"\"a new feature that is the product of features f1 and f2\n",
        "  \"\"\"\n",
        "  def feat(e):\n",
        "\n",
        "    return f1(e)*f2(e)\n",
        "  feat.__doc__ = f1.__doc__+\"*\"+f2.__doc__\n",
        "  return feat\n",
        "def eq_feat(f1,f2):\n",
        "  \"\"\"a new feature that is 1 if f1 and f2 give same value\n",
        "  \"\"\"\n",
        "  def feat(e):\n",
        "    return 1 if f1(e)==f2(e) else 0\n",
        "  feat.__doc__ = f1.__doc__+\"==\"+f2.__doc__\n",
        "  return feat\n",
        "  \n",
        "def neq_feat(f1,f2):\n",
        "  \"\"\"a new feature that is 1 if f1 and f2 give different values\n",
        "  \"\"\"\n",
        "  def feat(e):\n",
        "    return 1 if f1(e)!=f2(e) else 0\n",
        "  feat.__doc__ = f1.__doc__+\"!=\"+f2.__doc__\n",
        "  return feat\n"
      ],
      "metadata": {
        "id": "2Er2l_qDsxAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # from learnProblem import Data_set_augmented,prod_feat\n",
        "# data = Data_from_file('data/holiday.csv', num_train=19, target_index=-1)\n",
        " # data = Data_from_file('data/iris.data', prob_test=1/3, target_index=-1)\n",
        " ## Data = Data_from_file('data/SPECT.csv', prob_test=0.5, target_index=0)\n",
        " # dataplus = Data_set_augmented(data,[],[prod_feat])\n",
        " # dataplus = Data_set_augmented(data,[],[prod_feat,neq_feat])\n"
      ],
      "metadata": {
        "id": "rMQ90dQ7sxC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.2 Generic Learner Interface\n"
      ],
      "metadata": {
        "id": "490RgGdSsxGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A learnertakes a dataset (and possibly other arguments specific to the method).\n",
        "To get it to learn, we call the learn() method. This implements Displayable so\n",
        "that we can display traces at multiple levels of detail (and perhaps with a GUI)."
      ],
      "metadata": {
        "id": "jy01FeUksxI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from display import Displayable\n",
        "class Learner(Displayable):\n",
        "  def __init__(self, dataset):\n",
        "    raise NotImplementedError(\"Learner.__init__\") # abstract method\n",
        "\n",
        "\n",
        "\n",
        "  def learn(self):\n",
        "   \"\"\"returns a predictor, a function from a tuple to a value for the\n",
        "  target feature\n",
        "   \"\"\"\n",
        "  raise NotImplementedError(\"learn\") # abstract method\n"
      ],
      "metadata": {
        "id": "8zfrxEtTsxMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.3 Learning With No Input Features"
      ],
      "metadata": {
        "id": "8nMsq_fxselg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we make the same prediction for each example, what prediction should we\n",
        "make? This can be used as a naive baseline; if a more sophisticated method\n",
        "does not do better than this, it is not useful. This also provides the base case\n",
        "for some methods, such as decision-tree learning."
      ],
      "metadata": {
        "id": "Ws4E75HTsepS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###learnNoInputs.py"
      ],
      "metadata": {
        "id": "En1NWCHFcdXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Evaluate\n",
        "import math, random, collections, statistics\n",
        "import utilities # argmax for (element,value) pairs\n",
        "\n",
        "class Predict(object):\n",
        " \"\"\"The class of prediction methods for a list of values.\n",
        " Please make the doc strings the same length, because they are used in tables.\n",
        " Note that we don't need self argument, as we are creating Predict objects,\n",
        " To use call Predict.laplace(data) etc.\"\"\"\n",
        "\n",
        " ### The following return a distribution over values (for classification)\n",
        " def empirical(data, domain=[0,1], icount=0):\n",
        "    \"empirical dist \"\n",
        "    # returns a distribution over values\n",
        "    counts = {v:icount for v in domain}\n",
        "    for e in data:\n",
        "     counts[e] += 1\n",
        "    s = sum(counts.values())\n",
        "    return {k:v/s for (k,v) in counts.items()}\n",
        "\n",
        " def bounded_empirical(data, domain=[0,1], bound=0.01):\n",
        "   \"bounded empirical\"\n",
        "   return {k:min(max(v,bound),1-bound) for (k,v) in Predict.empirical(data, domain).items()}\n",
        "\n",
        " def laplace(data, domain=[0,1]):\n",
        "   \"Laplace \" # for categorical data\n",
        "   return Predict.empirical(data, domain, icount=1)\n",
        "\n",
        " def cmode(data, domain=[0,1]):\n",
        "    \"mode \" # for categorical data\n",
        "    md = statistics.mode(data)\n",
        "    return {v: 1 if v==md else 0 for v in domain}\n",
        "\n",
        " def cmedian(data, domain=[0,1]):\n",
        "   \"median \" # for categorical data\n",
        "   md = statistics.median_low(data) # always return one of the values\n",
        "   return {v: 1 if v==md else 0 for v in domain}\n",
        " ### The following return a single prediction (for regression). domain is ignored.\n",
        "\n",
        " def mean(data, domain=[0,1]):\n",
        "  \"mean \"\n",
        "  # returns a real number\n",
        "  return statistics.mean(data)\n",
        "\n",
        "  def rmean(data, domain=[0,1], mean0=0, pseudo_count=1):\n",
        "    \"regularized mean\"\n",
        "    # returns a real number.\n",
        "    # mean0 is the mean to be used for 0 data points\n",
        "    # With mean0=0.5, pseudo_count=2, same as laplace for [0,1] data\n",
        "    # this works for enumerations as well as lists\n",
        "    sum = mean0 * pseudo_count\n",
        "    count = pseudo_count\n",
        "    for e in data:\n",
        "      sum += e\n",
        "      count += 1\n",
        "  return sum/count\n",
        "\n",
        "  def mode(data, domain=[0,1]):\n",
        "    \"mode \"\n",
        "    return statistics.mode(data)\n",
        "\n",
        "  def median(data, domain=[0,1]):\n",
        "    \"median \"\n",
        "    return statistics.median(data)\n",
        "\n",
        "  all = [empirical, mean, rmean, bounded_empirical, laplace, cmode, mode, median,cmedian]\n",
        "\n",
        "  # The following suggests appropriate predictions as a function of the target type\n",
        "  select = {\n",
        "  \"boolean\": [empirical, bounded_empirical, laplace, cmode, cmedian],\n",
        "  \"categorical\": [empirical, bounded_empirical, laplace, cmode, cmedian],\n",
        "  \"numeric\": [mean, rmean, mode, median]}\n",
        "  "
      ],
      "metadata": {
        "id": "ie2o2Skfx3XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.3.1 Evaluation"
      ],
      "metadata": {
        "id": "N47EnSYzx3aN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate a point prediction, we first generate some data from a simple (Bernoulli)\n",
        "distribution, where there are two possible values, 0 and 1 for the target feature.\n",
        "Given prob, a number in the range [0, 1], this generate some training and test\n",
        "data where prob is the probability of each example being 1. To generate a 1 with\n",
        "probability prob, we generate a random number in range [0,1] and return 1 if\n",
        "that number is less than prob. A prediction is computed by applying the predictor to the training data, which is evaluated on the test set. This is repeated\n",
        "num_samples times"
      ],
      "metadata": {
        "id": "eWJATDsNx3eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_no_inputs(error_measures = Evaluate.all_criteria,num_samples=10000, test_size=10 ):\n",
        " for train_size in [1,2,3,4,5,10,20,100,1000]:\n",
        "   results = {predictor: {error_measure: 0 for error_measure in error_measures} for predictor in Predict.all}\n",
        " for sample in range(num_samples):\n",
        "    prob = random.random()\n",
        "    training = [1 if random.random()<prob else 0 for i in range(train_size)]\n",
        "    test = [1 if random.random()<prob else 0 for i in range(test_size)]\n",
        "    for predictor in Predict.all:\n",
        "      prediction = predictor(training)\n",
        "      for error_measure in error_measures:\n",
        "        results[predictor][error_measure] += sum(error_measure(prediction,actual) for actual in test)/test_size\n",
        " print(f\"For training size {train_size}:\")\n",
        " print(\" Predictor\\t\",\"\\t\".join(error_measure.__doc__ for\n",
        " error_measure in error_measures),sep=\"\\t\")\n",
        " for predictor in Predict.all:\n",
        "    print(f\" {predictor.__doc__}\",\"\\t\".join(\"{:.7f}\".format(results[predictor][error_measure]/num_samples) for error_measure in error_measures),sep=\"\\t\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  test_no_input()"
      ],
      "metadata": {
        "id": "i5RHaehZx3hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.4 Decision Tree Learning"
      ],
      "metadata": {
        "id": "M3VUhjZvx3j1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision tree algorithm does binary splits, and assumes that all input\n",
        "features are binary functions of the examples. It stops splitting if there are\n",
        "no input features, the number of examples is less than a specified number of\n",
        "examples or all of the examples agree on the target feature."
      ],
      "metadata": {
        "id": "JjMdLOwh2ytt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###learnDT.py"
      ],
      "metadata": {
        "id": "OG9wtLOw2ywe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Learner, Evaluate\n",
        "from learnNoInputs import Predict\n",
        "import math\n",
        "\n",
        "class DT_learner(Learner):\n",
        "\n",
        "  def __init__(self,\n",
        "                dataset,\n",
        "                split_to_optimize=Evaluate.log_loss, # to minimize for at each split\n",
        "                leaf_prediction=Predict.empirical, # what to use for value at leaves\n",
        "                train=None, # used for cross validation\n",
        "                max_num_cuts=8, # maximum number of conditions to split a numerical feature into\n",
        "                gamma=1e-7 , # minimum improvement needed to expand a node\n",
        "                min_child_weight=10):\n",
        "    \n",
        "      self.dataset = dataset\n",
        "      self.target = dataset.target\n",
        "      self.split_to_optimize = split_to_optimize\n",
        "      self.leaf_prediction = leaf_prediction\n",
        "      self.max_num_cuts = max_num_cuts\n",
        "      self.gamma = gamma\n",
        "      self.min_child_weight = min_child_weight\n",
        "      if train is None:\n",
        "        self.train = self.dataset.train\n",
        "      else:\n",
        "        self.train = train\n",
        "\n",
        "  def learn(self, max_num_cuts=8):\n",
        "      \"\"\"learn a decision tree\"\"\"\n",
        "      return self.learn_tree(self.dataset.conditions(self.max_num_cuts),self.train)\n"
      ],
      "metadata": {
        "id": "0lzu2giN2yzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main recursive algorithm, takes in a set of input features and a set of\n",
        "training data. It first decides whether to split. If it doesn’t split, it makes a point\n",
        "prediction, ignoring the input features.\n"
      ],
      "metadata": {
        "id": "2KxScdj62y0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##It splits unless:"
      ],
      "metadata": {
        "id": "JCSi_XBZ2y4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are no more input features\n",
        "* there are fewer examples than min number examples,\n",
        "* all the examples agree on the value of the target, or\n",
        "* the best split makes all examples in the same partition."
      ],
      "metadata": {
        "id": "FBBLpj4c2y6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_tree(self, conditions, data_subset):\n",
        "\n",
        "  \"\"\"returns a decision tree\n",
        "  conditions is a set of possible conditions\n",
        "  data_subset is a subset of the data used to build this (sub)tree\n",
        "\n",
        "  where a decision tree is a function that takes an example and\n",
        "  makes a prediction on the target feature\n",
        "  \"\"\"\n",
        "  self.display(2,f\"learn_tree with {len(conditions)} features and {len(data_subset)} examples\")\n",
        "  split, partn = self.select_split(conditions, data_subset)\n",
        "  if split is None: # no split; return a point prediction\n",
        "     prediction = self.leaf_value(data_subset, self.target.frange)\n",
        "     self.display(2,f\"leaf prediction for {len(data_subset)} examples is {prediction}\")\n",
        "\n",
        "     def leaf_fun(e):\n",
        "      \n",
        "        return prediction\n",
        "     leaf_fun.__doc__ = str(prediction)\n",
        "     leaf_fun.num_leaves = 1\n",
        "     return leaf_fun\n",
        "\n",
        "  else: # a split succeeded\n",
        "    false_examples, true_examples = partn\n",
        "    rem_features = [fe for fe in conditions if fe != split]\n",
        "    self.display(2,\"Splitting on\",split.__doc__,\"with examples split\",\n",
        "    len(true_examples),\":\",len(false_examples))\n",
        "    true_tree = self.learn_tree(rem_features,true_examples)\n",
        "    false_tree = self.learn_tree(rem_features,false_examples)\n",
        "\n",
        "  def fun(e):\n",
        "    if split(e):\n",
        "      return true_tree(e)\n",
        "    else:\n",
        "      return false_tree(e)\n",
        "  #fun = lambda e: true_tree(e) if split(e) else false_tree(e)\n",
        "  fun.__doc__ = (\"if \"+split.__doc__+\" then (\"+true_tree.__doc__+\") else (\"+false_tree.__doc__+\")\")\n",
        "  fun.num_leaves = true_tree.num_leaves + false_tree.num_leaves\n",
        "  return fun\n"
      ],
      "metadata": {
        "id": "3nwEy3_V2y9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leaf_value(self, egs, domain):\n",
        "  return self.leaf_prediction((self.target(e) for e in egs), domain)\n",
        "\n",
        "  def select_split(self, conditions, data_subset):\n",
        "    \"\"\"finds best feature to split on.\n",
        "\n",
        "    conditions is a non-empty list of features.\n",
        "    returns feature, partition\n",
        "    where feature is an input feature with the smallest error as\n",
        "    judged by split_to_optimize or\n",
        "    feature==None if there are no splits that improve the error\n",
        "    partition is a pair (false_examples, true_examples) if feature is not None\n",
        "    \"\"\"\n",
        "    best_feat = None # best feature\n",
        "    # best_error = float(\"inf\") # infinity - more than any error\n",
        "    best_error = self.sum_losses(data_subset) - self.gamma\n",
        "    self.display(3,\" no split has error=\",best_error,\"with\",len(conditions),\"conditions\")\n",
        "    best_partition = None\n",
        "    for feat in conditions:\n",
        "      false_examples, true_examples = partition(data_subset,feat)\n",
        "\n",
        "      if min(len(false_examples),len(true_examples))>=self.min_child_weight:\n",
        "        err = (self.sum_losses(false_examples) + self.sum_losses(true_examples))\n",
        "        self.display(3,\" split on\",feat.__doc__,\"has error=\",err, \"splits into\",len(true_examples),\":\",len(false_examples),\"gamma=\",self.gamma)\n",
        "      if err < best_error:\n",
        "        best_feat = feat\n",
        "        best_error=err\n",
        "        best_partition = false_examples, true_examples\n",
        "    self.display(2,\"best split is on\",best_feat.__doc__, \"with err=\",best_error)\n",
        "\n",
        "    return best_feat, best_partition\n",
        "\n",
        "  def sum_losses(self, data_subset):\n",
        "      \"\"\"returns sum of losses for dataset (with no more splits)\n",
        "      There a single prediction for all leaves using leaf_prediction\n",
        "      It is evaluated using split_to_optimize\n",
        "      \"\"\"\n",
        "      prediction = self.leaf_value(data_subset, self.target.frange)\n",
        "      error = sum(self.split_to_optimize(prediction, self.target(e)) for e in data_subset)\n",
        "      return error\n",
        "\n",
        "def partition(data_subset,feature):\n",
        "  \"\"\"partitions the data_subset by the feature\"\"\"\n",
        "  true_examples = []\n",
        "  false_examples = []\n",
        "  for example in data_subset:\n",
        "    if feature(example):\n",
        "      true_examples.append(example)\n",
        "    else:\n",
        "      false_examples.append(example)\n",
        "  return false_examples, true_examples\n"
      ],
      "metadata": {
        "id": "lCp-j0js2zA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###results.py"
      ],
      "metadata": {
        "id": "H1hkVGkN2zC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Data_set, Data_from_file\n",
        "\n",
        "def testDT(data, print_tree=True, selections = None, **tree_args):\n",
        "  \"\"\"Prints errors and the trees for various evaluation criteria and ways to select leaves.\n",
        "  \"\"\"\n",
        "  if selections == None: # use selections suitable for target type\n",
        "    selections = Predict.select[data.target.ftype]\n",
        "  evaluation_criteria = Evaluate.all_criteria\n",
        "  print(\"Split Choice\",\"Leaf Choice\\t\",\"#leaves\",'\\t'.join(ecrit.__doc__for ecrit in evaluation_criteria),sep=\"\\t\")\n",
        "  for crit in evaluation_criteria:\n",
        "    for leaf in selections:\n",
        "      tree = DT_learner(data, split_to_optimize=crit, leaf_prediction=leaf, **tree_args).learn()\n",
        "      print(crit.__doc__, leaf.__doc__, tree.num_leaves, \"\\t\".join(\"{:.7f}\".format(data.evaluate_dataset(data.test, tree, ecrit)) for ecrit in evaluation_criteria),sep=\"\\t\")\n",
        "      if print_tree:\n",
        "        print(tree.__doc__)\n",
        "\n",
        " #DT_learner.max_display_level = 4\n",
        "if __name__ == \"__main__\":\n",
        " # Choose one of the data files\n",
        " #data=Data_from_file('data/SPECT.csv', target_index=0); print(\"SPECT.csv\")\n",
        " #data=Data_from_file('data/iris.data', target_index=-1); print(\"iris.data\")\n",
        " data = Data_from_file('data/carbool.csv', target_index=-1, seed=123)\n",
        " #data = Data_from_file('data/mail_reading.csv', target_index=-1); print(\"mail_reading.csv\")\n",
        " #data = Data_from_file('data/holiday.csv', num_train=19, target_index=-1); print(\"holiday.csv\")"
      ],
      "metadata": {
        "id": "BWuN-Qaq2zEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that different runs may provide different values as they split the training and test sets differently. So if you have a hypothesis about what works\n",
        "better, make sure it is true for different runs.\n"
      ],
      "metadata": {
        "id": "n9reJBNtx3mT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cross Validation and Parameter Tuning\n"
      ],
      "metadata": {
        "id": "aJDIveJi9HmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "he prediction is overfitting is by cross validation. The code below implements\n",
        "k-fold cross validation, which can be used to choose the value of parameters to best fit the training data. If we want to use parameter tuning to improve\n",
        "predictions on a particular data set, we can only use the training data (and not\n",
        "the test data) to tune the parameter.\n"
      ],
      "metadata": {
        "id": "wkvHNnJO9MYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In k-fold cross validation, we partition the training set into k approximately\n",
        "equal-sized folds (each fold is an enumeration of examples). For each fold, we\n",
        "train on the other examples, and determine the error of the prediction on that\n",
        "fold. For example, if there are 10 folds, we train on 90% of the data, and then\n",
        "test on remaining 10% of the data. We do this 10 times, so that each example\n",
        "gets used as a test set once, and in the training set 9 times.\n"
      ],
      "metadata": {
        "id": "HT26EoA89Mam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below creates one copy of the data, and multiple views of the data.\n",
        "For each fold, fold enumerates the examples in the fold, and fold complement\n",
        "enumerates the examples not in the fold."
      ],
      "metadata": {
        "id": "ZIIWF5xg9MdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##learnCrossValidation.py"
      ],
      "metadata": {
        "id": "1b4CWJyT9MfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Data_set, Data_from_file, Evaluate\n",
        "from learnNoInputs import Predict\n",
        "from learnDT import DT_learner\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "class K_fold_dataset(object):\n",
        "  def __init__(self, training_set, num_folds):\n",
        "    self.data = training_set.train.copy()\n",
        "    self.target = training_set.target\n",
        "    self.input_features = training_set.input_features\n",
        "    self.num_folds = num_folds\n",
        "    self.conditions = training_set.conditions\n",
        "    random.shuffle(self.data)\n",
        "    self.fold_boundaries = [(len(self.data)*i)//num_folds for i in range(0,num_folds+1)]\n",
        "  def fold(self, fold_num):\n",
        "    for i in range(self.fold_boundaries[fold_num], self.fold_boundaries[fold_num+1]):\n",
        "      yield self.data[i]\n",
        "  def fold_complement(self, fold_num):\n",
        "    for i in range(0,self.fold_boundaries[fold_num]):\n",
        "      yield self.data[i]\n",
        "    for i in range(self.fold_boundaries[fold_num+1],len(self.data)):\n",
        "      yield self.data[i]\n"
      ],
      "metadata": {
        "id": "x_7K-BsC9d8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation error is the average error for each example, where we test on\n",
        "each fold, and learn on the other folds"
      ],
      "metadata": {
        "id": "J-JPcA6b9d_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_error(self, learner, error_measure, **other_params):\n",
        " error = 0\n",
        " try:\n",
        "\n",
        "\n",
        "  for i in range(self.num_folds):\n",
        "    predictor = learner(self, train=list(self.fold_complement(i)),**other_params).learn()\n",
        "    error += sum( error_measure(predictor(e), self.target(e)) for e in self.fold(i))\n",
        " except ValueError:\n",
        "   return float(\"inf\") #infinity\n",
        " return error/len(self.data)\n",
        "\n",
        "def plot_error(data, criterion=Evaluate.squared_loss,leaf_prediction=Predict.empirical,num_folds=5, maxx=None, xscale='linear'):\n",
        "  \"\"\"Plots the error on the validation set and the test set\n",
        "  with respect to settings of the minimum number of examples.\n",
        "  xscale should be 'log' or 'linear'\n",
        "  \"\"\"\n",
        "  plt.ion()\n",
        "  plt.xscale(xscale) # change between log and linear scale\n",
        "  plt.xlabel(\"min_child_weight\")\n",
        "  plt.ylabel(\"average \"+criterion.__doc__)\n",
        "  folded_data = K_fold_dataset(data, num_folds)\n",
        "  if maxx == None:\n",
        "    maxx = len(data.train)//2+1\n",
        "  verrors = [] # validation errors\n",
        "  terrors = [] # test set errors\n",
        "  for mcw in range(maxx):\n",
        "    verrors.append(folded_data.validation_error(DT_learner,criterion,leaf_prediction=leaf_predi69, min_child_weight=mcw))\n",
        "    tree = DT_learner(data, criterion, leaf_prediction=leaf_prediction, min_child_weight=mcw).learn()\n",
        "    terrors.append(data.evaluate_dataset(data.test,tree,criterion))\n",
        "  plt.plot(range(maxx), verrors, ls='-',color='k',\n",
        "  label=\"validation for \"+criterion.__doc__)\n",
        "  plt.plot(range(maxx), terrors, ls='--',color='k',\n",
        "  label=\"test set for \"+criterion.__doc__)\n",
        "  plt.legend()\n",
        "  plt.draw()\n",
        "# The following produces Figure 7.15 of Poole and Mackworth [2017]\n",
        "# data = Data_from_file('data/SPECT.csv',target_index=0, seed=123)\n",
        "# plot_error(data) # warning, may take a long time depending on the dataset\n",
        "\n",
        "#also try:\n",
        "# data = Data_from_file('data/mail_reading.csv', target_index=-1)\n",
        "# data = Data_from_file('data/carbool.csv', target_index=-1, seed=123)\n",
        "# plot_error(data, criterion=Evaluate.log_loss,leaf_prediction=Predict.laplace) # warning, may take a long time depending on the dataset\n"
      ],
      "metadata": {
        "id": "lnDsGAYk9eC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.6 Linear Regression and Classification\n"
      ],
      "metadata": {
        "id": "gig0m4iK9eE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Learner\n",
        "import random, math\n",
        "\n",
        "class Linear_learner(Learner):\n",
        " def __init__(self, dataset, train=None,\n",
        " learning_rate=0.1, max_init = 0.2,\n",
        " squashed=True):\n",
        "  \"\"\"Creates a gradient descent searcher for a linear classifier.\n",
        "  The main learning is carried out by learn()\n",
        "\n",
        "  dataset provides the target and the input features\n",
        "  train provides a subset of the training data to use\n",
        "  number_iterations is the default number of steps of gradient descent\n",
        "  learning_rate is the gradient descent step size\n",
        "  max_init is the maximum absolute value of the initial weights\n",
        "  squashed specifies whether the output is a squashed linear function\n",
        "  \"\"\"\n",
        " self.dataset = dataset\n",
        " self.target = dataset.target\n",
        " if train==None:\n",
        "  self.train = self.dataset.train\n",
        " else:\n",
        "  self.train = train\n",
        "  self.learning_rate = learning_rate\n",
        "  self.squashed = squashed\n",
        "  self.input_features = [one]+dataset.input_features # one is defined below\n",
        "  self.weights = {feat:random.uniform(-max_init,max_init)\n",
        "  for feat in self.input_features}\n",
        "\n",
        " def predictor(self,e):\n",
        "  \"\"\"returns the prediction of the learner on example e\"\"\"\n",
        " linpred = sum(w*f(e) for f,w in self.weights.items())\n",
        " if self.squashed:\n",
        "   return sigmoid(linpred)\n",
        " else:\n",
        "   return linpred\n",
        "\n",
        " def predictor_string(self, sig_dig=3):\n",
        "  \"\"\"returns the doc string for the current prediction function\n",
        "  sig_dig is the number of significant digits in the numbers\"\"\"\n",
        "  doc = \"+\".join(str(round(val,sig_dig))+\"*\"+feat.__doc__\n",
        "  for feat,val in self.weights.items())\n",
        "  if self.squashed:\n",
        "    return \"sigmoid(\"+ doc+\")\"\n",
        "  else:\n",
        "    return doc\n",
        "\n",
        "def learn(self,num_iter=100):\n",
        " for it in range(num_iter):\n",
        "  self.display(2,\"prediction=\",self.predictor_string())\n",
        "  for e in self.train:\n",
        "    predicted = self.predictor(e)\n",
        "    error = self.target(e) - predicted\n",
        "    update = self.learning_rate*error\n",
        "    for feat in self.weights:\n",
        "      self.weights[feat] += update*feat(e)\n",
        "  return self.predictor\n"
      ],
      "metadata": {
        "id": "1Tr2kmSe9eIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(xs,domain=None):\n",
        "  \"\"\"xs is a list of values, and\n",
        "  domain is the domain (a list) or None if the list should be returned\n",
        "  returns a distribution over the domain (a dict)\n",
        "  \"\"\"\n",
        "  m = max(xs) # use of m prevents overflow (and all values underflowing)\n",
        "  exps = [math.exp(x-m) for x in xs]\n",
        "  s = sum(exps)\n",
        "  if domain:\n",
        "    return {d:v/s for (d,v) in zip(domain,exps)}\n",
        "  else:\n",
        "    return [v/s for v in exps]\n",
        "\n",
        "def indicator(v, domain):\n",
        "  return [1 if v==dv else 0 for dv in domain]\n"
      ],
      "metadata": {
        "id": "xmtdZhuZ8za0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following tests the learner on a data sets. Uncomment the other data\n",
        "sets for different examples.\n"
      ],
      "metadata": {
        "id": "rWaFMglr9eKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Data_set, Data_from_file, Evaluate\n",
        "from learnProblem import Evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "def test(**args):\n",
        " data = Data_from_file('data/SPECT.csv', target_index=0)\n",
        " # data = Data_from_file('data/mail_reading.csv', target_index=-1)\n",
        " # data = Data_from_file('data/carbool.csv', target_index=-1)\n",
        " learner = Linear_learner(data,**args)\n",
        " learner.learn()\n",
        " print(\"function learned is\", learner.predictor_string())\n",
        " for ecrit in Evaluate.all_criteria:\n",
        "  test_error = data.evaluate_dataset(data.test, learner.predictor,\n",
        "  ecrit)\n",
        "  print(\" Average\", ecrit.__doc__, \"is\", test_error)\n"
      ],
      "metadata": {
        "id": "iJIPRYMz9eOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following plots the errors on the training and test sets as a function of\n",
        "the number of steps of gradient descent."
      ],
      "metadata": {
        "id": "BA58jNSz9MjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_steps(learner=None,\n",
        " data = None,\n",
        " criterion=Evaluate.squared_loss,\n",
        " step=1,\n",
        " num_steps=1000,\n",
        " log_scale=True,\n",
        " legend_label=\"\"):\n",
        " \"\"\"\n",
        " plots the training and test error for a learner.\n",
        " data is the\n",
        " learner_class is the class of the learning algorithm\n",
        " criterion gives the evaluation criterion plotted on the y-axis\n",
        " step specifies how many steps are run for each point on the plot\n",
        " num_steps is the number of points to plot\n",
        "\n",
        " \"\"\"\n",
        " if legend_label != \"\": legend_label+=\" \"\n",
        " plt.ion()\n",
        " plt.xlabel(\"step\")\n",
        " plt.ylabel(\"Average \"+criterion.__doc__)\n",
        " if log_scale:\n",
        "    plt.xscale('log') #plt.semilogx() #Makes a log scale\n",
        " else:\n",
        "    plt.xscale('linear')\n",
        " if data is None:\n",
        "  data = Data_from_file('data/holiday.csv', num_train=19,target_index=-1)\n",
        " #data = Data_from_file('data/SPECT.csv', target_index=0)\n",
        " # data = Data_from_file('data/mail_reading.csv', target_index=-1)\n",
        " # data = Data_from_file('data/carbool.csv', target_index=-1)\n",
        " #random.seed(None) # reset seed\n",
        " if  learner is None:\n",
        "    learner = Linear_learner(data)\n",
        " train_errors = []\n",
        " test_errors = []\n",
        " for i in range(1,num_steps+1,step):\n",
        "    test_errors.append(data.evaluate_dataset(data.test,learner.predictor, criterion))\n",
        "    train_errors.append(data.evaluate_dataset(data.train,\n",
        "    learner.predictor, criterion))\n",
        "    learner.display(2, \"Train error:\",train_errors[-1],\"Test error:\",test_errors[-1])\n",
        "    learner.learn(num_iter=step)\n",
        " plt.plot(range(1,num_steps+1,step),train_errors,ls='-',label=legend_label+\"training\")\n",
        " plt.plot(range(1,num_steps+1,step),test_errors,ls='--',label=legend_label+\"test\")\n",
        " plt.legend()\n",
        " plt.draw()\n",
        "\n",
        "learner.display(1, \"Train error:\",train_errors[-1],\"Test error:\",test_errors[-1])\n",
        "if __name__ == \"__main__\":\n",
        "  test()\n",
        "# This generates the figure\n",
        "# from learnProblem import Data_set_augmented,prod_feat\n",
        "# data = Data_from_file('data/SPECT.csv', prob_test=0.5, target_index=0)\n",
        "# dataplus = Data_set_augmented(data,[],[prod_feat])\n",
        "# plot_steps(data=data,num_steps=1000)\n",
        "# plot_steps(data=dataplus,num_steps=1000) # warning very slow\n"
      ],
      "metadata": {
        "id": "LMIQwyL48_f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7JFvRFhrNXHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following plots the prediction as a function of the function of the number of steps of gradient descent. We first define a version of range that allows\n",
        "for real numbers (integers and floats)."
      ],
      "metadata": {
        "id": "jOIdOCdN8_i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def arange(start,stop,step):\n",
        " \"\"\"returns enumeration of values in the range [start,stop) separated by step.\n",
        " like the built-in range(start,stop,step) but allows for integers and floats.\n",
        " Note that rounding errors are expected with real numbers. (or use numpy.arange)\n",
        " \"\"\"\n",
        " while start<stop:\n",
        "  yield start\n",
        "  start += step\n",
        "\n",
        "def plot_prediction(data,\n",
        " learner = None,\n",
        " minx = 0,\n",
        " maxx = 5,\n",
        " step_size = 0.01, # for plotting\n",
        " label = \"function\"):\n",
        " plt.ion()\n",
        " plt.xlabel(\"x\")\n",
        " plt.ylabel(\"y\")\n",
        " if learner is None:\n",
        "    learner = Linear_learner(data, squashed=False)\n",
        " learner.learning_rate=0.001\n",
        " learner.learn(100)\n",
        " learner.learning_rate=0.0001\n",
        " learner.learn(1000)\n",
        " learner.learning_rate=0.00001\n",
        " learner.learn(10000)\n",
        " learner.display(1,\"function learned is\", learner.predictor_string(),\"error=\",data.evaluate_dataset(data.train, learner.predictor,Evaluate.squared_loss))\n",
        " plt.plot([e[0] for e in data.train],[e[-1] for e in data.train],\"bo\",label=\"data\")\n",
        " plt.plot(list(arange(minx,maxx,step_size)),[learner.predictor([x])\n",
        " for x in arange(minx,maxx,step_size)],\n",
        " label=label)\n",
        " plt.legend()\n"
      ],
      "metadata": {
        "id": "buBM2e_K8_mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###learnLinear.py"
      ],
      "metadata": {
        "id": "5ZcJ-dk08_oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Data_set_augmented, power_feat\n",
        "def plot_polynomials(data, learner_class = Linear_learner,\n",
        "                                          max_degree = 5,\n",
        "                                          minx = 0,\n",
        "                                          maxx = 5,\n",
        "                                          num_iter = 100000,\n",
        "                                          learning_rate = 0.0001,\n",
        "                                          step_size = 0.01, # for plotting \n",
        "                     ):\n",
        " plt.ion()\n",
        " plt.xlabel(\"x\")\n",
        " plt.ylabel(\"y\")\n",
        " plt.plot([e[0] for e in data.train],[e[-1] for e in data.train],\"ko\",label=\"data\")\n",
        " x_values = list(arange(minx,maxx,step_size))\n",
        " line_styles = ['-','--','-.',':']\n",
        " colors = ['0.5','k','k','k','k']\n",
        " for degree in range(max_degree): \n",
        "   data_aug = Data_set_augmented(data,[power_feat(n) for n in range(1,degree+1)],\n",
        " include_orig=False)\n",
        " learner = learner_class(data_aug,squashed=False)\n",
        " learner.learning_rate = learning_rate\n",
        " learner.learn(num_iter)\n",
        "\n",
        " learner.display(1,\"For degree\",degree, \"function learned is\", learner.predictor_string(), \"error=\",data.evaluate_dataset(data.train, learner.predictor, Evaluate.squared_loss))\n",
        " ls = line_styles[degree % len(line_styles)]\n",
        " col = colors[degree % len(colors)]\n",
        "\n",
        " plt.plot(x_values,[learner.predictor([x]) for x in x_values],linestyle=ls, color=col,\n",
        " label=\"degree=\"+str(degree))\n",
        " plt.legend(loc='upper left')\n",
        " plt.draw()\n",
        "\n",
        "# Try:\n",
        "# data0 = Data_from_file('data/simp_regr.csv', prob_test=0, boolean_features=False, target_index=-1)\n",
        "# plot_prediction(data0)\n",
        "# plot_polynomials(data0)\n",
        "#datam = Data_from_file('data/mail_reading.csv', target_index=-1)\n",
        "#plot_prediction(datam)\n"
      ],
      "metadata": {
        "id": "aoKtuPvs8_ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.6.1 Batched Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "8awNOB3T8_uI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implements batched stochastic gradient descent. If the batch size is 1, it\n",
        "can be simplified by not storing the differences in d, but applying them directly;\n",
        "this would the be equivalent to the original code!\n"
      ],
      "metadata": {
        "id": "2jRsHCe_9gtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This overrides the learner_Linear learner. Note that the comparison with\n",
        "regular gradient descent is unfair as the number of updates per step is not the\n",
        "same. (How could it me made more fair?)\n"
      ],
      "metadata": {
        "id": "ISJFRML19gwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###learnLinearBSGD.py "
      ],
      "metadata": {
        "id": "e63eBasE9gzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnLinear import Linear_learner\n",
        "import random, math\n",
        "\n",
        "class Linear_learner_bsgd(Linear_learner):\n",
        " def __init__(self, *args, batch_size=10, **kargs):\n",
        "    Linear_learner.__init__(self, *args, **kargs)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        " def learn(self,num_iter=None):\n",
        "    if num_iter is None:\n",
        "      num_iter = self.number_iterations\n",
        "    batch_size = min(self.batch_size, len(self.train))\n",
        "    d = {feat:0 for feat in self.weights}\n",
        "    for it in range(num_iter):\n",
        "        self.display(2,\"prediction=\",self.predictor_string())\n",
        "        for e in random.sample(self.train, batch_size):\n",
        "            predicted = self.predictor(e)\n",
        "            error = self.target(e) - predicted\n",
        "            update = self.learning_rate*error\n",
        "            for feat in self.weights:\n",
        "              d[feat] += update*feat(e)\n",
        "        for feat in self.weights:\n",
        "          self.weights[feat] += d[feat]\n",
        "          d[feat]=0\n",
        "    return self.predictor\n",
        "\n",
        " # from learnLinear import plot_steps\n",
        " # from learnProblem import Data_from_file\n",
        " # data = Data_from_file('data/holiday.csv', target_index=-1)\n",
        " # learner = Linear_learner_bsgd(data) # plot_steps(learner = learner, data=data)\n",
        "\n",
        " # to plot polynomials with batching (compare to SGD)\n",
        " # from learnLinear import plot_polynomials\n",
        " # plot_polynomials(learner_class = Linear_learner_bsgd)"
      ],
      "metadata": {
        "id": "TWRZLBUd9g1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.7 Boosting"
      ],
      "metadata": {
        "id": "87hTEn7o9_Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Boosted dataset is created from a base dataset by subtracting the prediction of the offset function from each example. This does not save the new\n",
        "dataset, but generates it as needed. The amount of space used is constant, independent on the size of the data set.\n"
      ],
      "metadata": {
        "id": "zz1KAzMS9_R8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##learnBoosting.py"
      ],
      "metadata": {
        "id": "d0Qk6pKx9_Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Data_set, Learner, Evaluate\n",
        "from learnNoInputs import Predict\n",
        "from learnLinear import sigmoid\n",
        "import statistics\n",
        "import random\n",
        "\n",
        "class Boosted_dataset(Data_set):\n",
        " def __init__(self, base_dataset, offset_fun, subsample=1.0):\n",
        "    \"\"\"new dataset which is like base_dataset,\n",
        "    but offset_fun(e) is subtracted from the target of each example e\n",
        "    \"\"\"\n",
        "    self.base_dataset = base_dataset\n",
        "    self.offset_fun = offset_fun\n",
        "    self.train = random.sample(base_dataset.train,int(subsample*len(base_dataset.train)))\n",
        "    self.test = base_dataset.test\n",
        "    #Data_set.__init__(self, base_dataset.train, base_dataset.test,\n",
        "    # base_dataset.prob_test, base_dataset.target_index)\n",
        "\n",
        "#  def create_features(self):\n",
        "    \"\"\"creates new features - called at end of Data_set.init()\n",
        "    defines a new target\n",
        "    \"\"\"\n",
        "    self.input_features = self.base_dataset.input_features\n",
        "    def newout(e):\n",
        "      return self.base_dataset.target(e) - self.offset_fun(e)\n",
        "    newout.frange = self.base_dataset.target.frange\n",
        "    newout.ftype = self.infer_type(newout.frange)\n",
        "    self.target = newout\n",
        "\n",
        " def conditions(self, *args, colsample_bytree=0.5, **nargs):\n",
        "    conds = self.base_dataset.conditions(*args, **nargs)\n",
        "    return random.sample(conds, int(colsample_bytree*len(conds)))\n",
        "\n",
        "class Boosting_learner(Learner):\n",
        "  def __init__(self, dataset, base_learner_class, subsample=0.8):\n",
        "    self.dataset = dataset\n",
        "    self.base_learner_class = base_learner_class\n",
        "    self.subsample = subsample\n",
        "    mean = sum(self.dataset.target(e) for e in self.dataset.train)/len(self.dataset.train)\n",
        "    self.predictor = lambda e:mean # function that returns mean for each example\n",
        "    self.predictor.__doc__ = \"lambda e:\"+str(mean)\n",
        "    self.offsets = [self.predictor] # list of base learners\n",
        "    self.predictors = [self.predictor] # list of predictors\n",
        "    self.errors = [data.evaluate_dataset(data.test, self.predictor,Evaluate.squared_loss)]\n",
        "    self.display(1,\"Predict mean test set mean squared loss=\",\n",
        "    self.errors[0] )\n",
        "\n",
        "\n",
        "  def learn(self, num_ensembles=10):\n",
        "    \"\"\"adds num_ensemble learners to the ensemble.\n",
        "    returns a new predictor.\n",
        "    \"\"\"\n",
        "    for i in range(num_ensembles):\n",
        "        train_subset = Boosted_dataset(self.dataset, self.predictor,subsample=self.subsample)\n",
        "        learner = self.base_learner_class(train_subset)\n",
        "        new_offset = learner.learn()\n",
        "        self.offsets.append(new_offset)\n",
        "        def new_pred(e, old_pred=self.predictor, off=new_offset):\n",
        "          return old_pred(e)+off(e)\n",
        "        self.predictor = new_pred\n",
        "        self.predictors.append(new_pred)\n",
        "        self.errors.append(data.evaluate_dataset(data.test,\n",
        "        self.predictor, Evaluate.squared_loss))\n",
        "        self.display(1,f\"Iteration {len(self.offsets)-1},treesize = {new_offset.num_leaves}. mean squared loss={self.errors[-1]}\")\n",
        "    return self.predictor\n"
      ],
      "metadata": {
        "id": "5kaDPew09_Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing"
      ],
      "metadata": {
        "id": "O9hVqPXw-kdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnDT import DT_learner\n",
        "from learnProblem import Data_set, Data_from_file\n",
        "\n",
        "def sp_DT_learner(split_to_optimize=Evaluate.squared_loss,leaf_prediction=Predict.mean,**nargs):\n",
        "    \"\"\"Creates a learner with different default arguments replaced by **nargs\n",
        "    \"\"\"\n",
        "    def new_learner(dataset):\n",
        "      return DT_learner(dataset,split_to_optimize=split_to_optimize,\n",
        "      leaf_prediction=leaf_prediction, **nargs)\n",
        "    return new_learner\n",
        "\n",
        "#data = Data_from_file('data/car.csv', target_index=-1) regression\n",
        "data = Data_from_file('data/student/student-mat-nq.csv', separator=';',has_header=True,target_index=-1,seed=13,include_only=list(range(30))+[32]) #2.0537973790924946\n",
        "#data = Data_from_file('data/SPECT.csv', target_index=0, seed=62) #123)\n",
        "#data = Data_from_file('data/mail_reading.csv', target_index=-1)\n",
        "#data = Data_from_file('data/holiday.csv', num_train=19, target_index=-1)\n",
        "#learner10 = Boosting_learner(data,\n",
        "sp_DT_learner(split_to_optimize=Evaluate.squared_loss,\n",
        "leaf_prediction=Predict.mean, min_child_weight=10))\n",
        "#learner7 = Boosting_learner(data, sp_DT_learner(0.7))\n",
        "#learner5 = Boosting_learner(data, sp_DT_learner(0.5))\n",
        "#predictor9 =learner9.learn(10)\n",
        "#for i in learner9.offsets: print(i.__doc__)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_boosting_trees(data, steps=10, mcws=[30,20,20,10], gammas=[100,200,300,500]):\n",
        " # to reduce clutter uncomment one of following two lines\n",
        " #mcws=[10]\n",
        " #gammas=[200]\n",
        " learners = [(mcw, gamma, Boosting_learner(data,\n",
        " sp_DT_learner(min_child_weight=mcw, gamma=gamma)))\n",
        " for gamma in gammas for mcw in mcws]\n",
        " plt.ion()\n",
        " plt.xscale('linear') # change between log and linear scale\n",
        " plt.xlabel(\"number of trees\")\n",
        " plt.ylabel(\"mean squared loss\")\n",
        " markers = (m+c for c in ['k','g','r','b','m','c','y'] for m in ['-','--','-.',':'])\n",
        " for (mcw,gamma,learner) in learners:\n",
        "      data.display(1,f\"min_child_weight={mcw}, gamma={gamma}\")\n",
        " learner.learn(steps)\n",
        " plt.plot(range(steps+1), learner.errors, next(markers),\n",
        " label=f\"min_child_weight={mcw}, gamma={gamma}\")\n",
        " plt.legend()\n",
        " plt.draw()"
      ],
      "metadata": {
        "id": "MMto_7g_-kgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.7.1 Gradient Tree Boosting\n"
      ],
      "metadata": {
        "id": "1cZbXRt--kjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following implements gradient Boosted trees for classification. If you want\n",
        "to use this gradient tree boosting for a real problem, we recommend using\n",
        "XGBoost [Chen and Guestrin, 2016]."
      ],
      "metadata": {
        "id": "wttVCs8_-klN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GTB_learner subclasses DT-learner. The method learn_tree is used unchanged. DT-learner assumes that the value at the leaf is the prediction of the\n",
        "leaf, thus leaf_value needs to be overridden. It also assumes that all nodes\n",
        "at a leaf have the same prediction, but in GBT the elements of a leaf can have\n",
        "different values, depending on the previous trees. Thus sum_losses also needs\n",
        "to be overridden."
      ],
      "metadata": {
        "id": "0uqla_cc-kyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###learnBoosting.py"
      ],
      "metadata": {
        "id": "qmD07prn9_ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GTB_learner(DT_learner):\n",
        "  def __init__(self, dataset, number_trees, lambda_reg=1, gamma=0, **dtargs):\n",
        "    DT_learner.__init__(self, dataset, split_to_optimize=Evaluate.log_loss, **dtargs)\n",
        "    self.number_trees = number_trees\n",
        "    self.lambda_reg = lambda_reg\n",
        "    self.gamma = gamma\n",
        "    self.trees = []\n",
        "\n",
        "  def learn(self):\n",
        "    for i in range(self.number_trees):\n",
        "      tree = self.learn_tree(self.dataset.conditions(self.max_num_cuts), self.train)\n",
        "      self.trees.append(tree)\n",
        "      self.display(1,f\"\"\"Iteration {i} treesize = {tree.num_leaves} train logloss={\n",
        "      self.dataset.evaluate_dataset(self.dataset.train, self.gtb_predictor, Evaluate.log_loss)\n",
        "    } test logloss={\n",
        "    self.dataset.evaluate_dataset(self.dataset.test,self.gtb_predictor, Evaluate.log_loss)}\"\"\")\n",
        "    return self.gtb_predictor\n",
        "\n",
        "  def gtb_predictor(self, example, extra=0):\n",
        "    \"\"\"prediction for example,\n",
        "    extras is an extra contribution for this example being considered\n",
        "    \"\"\"\n",
        "    return sigmoid(sum(t(example) for t in self.trees)+extra)\n",
        "\n",
        "  def leaf_value(self, egs, domain=[0,1]):\n",
        "    \"\"\"value at the leaves for examples egs domain argument is ignored\"\"\"\n",
        "    pred_acts = [(self.gtb_predictor(e),self.target(e)) for e in egs]\n",
        "    return sum(a-p for (p,a) in pred_acts) /(sum(p*(1-p) for (p,a) in pred_acts)+self.lambda_reg)\n",
        "\n",
        "\n",
        "  def sum_losses(self, data_subset):\n",
        "    \"\"\"returns sum of losses for dataset (assuming a leaf is formed with no more splits)\n",
        "    \"\"\"\n",
        "    leaf_val = self.leaf_value(data_subset)\n",
        "    error = sum(Evaluate.log_loss(self.gtb_predictor(e,leaf_val), self.target(e))for e in data_subset) + self.gamma\n",
        "    return error\n",
        "    # data = Data_from_file('data/carbool.csv', target_index=-1, seed=123)\n",
        "    # gtb_learner = GTB_learner(data, 10)\n",
        "    # gtb_learner.learn()"
      ],
      "metadata": {
        "id": "hvBTWaai9_dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VV1I7x3D9_f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter-4 Neural Networks and Deep Learning\n"
      ],
      "metadata": {
        "id": "2KBmIwBW9_jA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.1 Layers"
      ],
      "metadata": {
        "id": "Szh4iIXK9_mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnProblem import Learner, Data_set, Data_from_file,Data_from_files, Evaluate\n",
        "from learnLinear import sigmoid, one, softmax, indicator\n",
        "import random, math, time\n",
        "class Layer(object):\n",
        "    def __init__(self, nn, num_outputs=None):\n",
        "\n",
        "      \"\"\"Given a list of inputs, outputs will produce a list of length\n",
        "      num_outputs.\n",
        "      nn is the neural network this layer is part of\n",
        "      num outputs is the number of outputs for this layer.\n",
        "      \"\"\"\n",
        "      self.nn = nn\n",
        "      self.num_inputs = nn.num_outputs # output of nn is the input to this layer\n",
        "      if num_outputs:\n",
        "        self.num_outputs = num_outputs\n",
        "      else:\n",
        "        self.num_outputs = nn.num_outputs # same as the inputs\n",
        "    def output_values(self,input_values, training=False):\n",
        "        \"\"\"Return the outputs for this layer for the given input values.\n",
        "        input_values is a list of the inputs to this layer (of length\n",
        "        num_inputs)\n",
        "        returns a list of length self.num_outputs.\n",
        "        It can act differently when training and when predicting.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"output_values\") # abstract method\n",
        "    def backprop(self,errors):\n",
        "      \"\"\"Backpropagate the errors on the outputs\n",
        "      errors is a list of errors for the outputs (of length\n",
        "      self.num_outputs).\n",
        "      Returns the errors for the inputs to this layer (of length\n",
        "      self.num_inputs).\n",
        "\n",
        "      You can assume that this is only called after corresponding\n",
        "      output_values,\n",
        "      which can remember information information required for the\n",
        "      back-propagation.\n",
        "      \"\"\"\n",
        "    raise NotImplementedError(\"backprop\") # abstract method\n",
        "    def update(self):\n",
        "      \"\"\"updates parameters after a batch.\n",
        "      overridden by layers that have parameters\n",
        "      \"\"\"\n",
        "      pass"
      ],
      "metadata": {
        "id": "zHpleYFT9_pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A linear layer maintains an array of weights. self.weights[o][i] is the weight\n",
        "between input i and output o. A 1 is added to the end of the inputs. The default\n",
        "initialization is the Glorot uniform initializer [Glorot and Bengio, 2010], which\n",
        "is the default in Keras. An alternative is to provide a limit, in which case the\n",
        "values are selected uniformly in the range [−limit, limit]. Keras treats the bias\n",
        "separately, and defaults to zero.\n"
      ],
      "metadata": {
        "id": "CYqS8PCS9_sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##learnNN.py"
      ],
      "metadata": {
        "id": "Ef4ma_SN9_yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_complete_layer(Layer):\n",
        "  \"\"\"a completely connected layer\"\"\"\n",
        "  def __init__(self, nn, num_outputs, limit=None):\n",
        "    \"\"\"A completely connected linear layer.\n",
        "    nn is a neural network that the inputs come from\n",
        "    num_outputs is the number of outputs\n",
        "    the random initialization of parameters is in range [-limit,limit]\n",
        "    \"\"\"\n",
        "    Layer.__init__(self, nn, num_outputs)\n",
        "    if limit is None:\n",
        "      limit =math.sqrt(6/(self.num_inputs+self.num_outputs))\n",
        "    # self.weights[o][i] is the weight between input i and output o\n",
        "    self.weights = [[random.uniform(-limit, limit) if inf < self.num_inputs else 0 for inf in range(self.num_inputs+1)] for outf in range(self.num_outputs)] self.delta = [[0 for inf in range(self.num_inputs+1)]\n",
        "    for outf in range(self.num_outputs)]\n",
        "\n",
        "  def output_values(self,input_values, training=False):\n",
        "    \"\"\"Returns the outputs for the input values.\n",
        "    It remembers the values for the backprop.\n",
        "\n",
        "    Note in self.weights there is a weight list for every output,\n",
        "    so wts in self.weights loops over the outputs.\n",
        "    The bias is the *last* value of each list in self.weights.\n",
        "    \"\"\"\n",
        "    self.inputs = input_values + [1]\n",
        "    return [sum(w*val for (w,val) in zip(wts,self.inputs))\n",
        "    for wts in self.weights]\n",
        "\n",
        "  def backprop(self,errors):\n",
        "    \"\"\"Backpropagate the errors, updating the weights and returning the error in its inputs.\n",
        "    \"\"\"\n",
        "    input_errors = [0]*(self.num_inputs+1)\n",
        "    for out in range(self.num_outputs):\n",
        "      for inp in range(self.num_inputs+1):\n",
        "        input_errors[inp] += self.weights[out][inp] * errors[out]\n",
        "        self.delta[out][inp] += self.inputs[inp] * errors[out]\n",
        "        return input_errors[:-1] # remove the error for the \"1\"\n",
        "  def update(self):\n",
        "    \"\"\"updates parameters after a batch\"\"\"\n",
        "    batch_step_size = self.nn.learning_rate / self.nn.batch_size\n",
        "  for out in range(self.num_outputs):\n",
        "    for inp in range(self.num_inputs+1):\n",
        "      self.weights[out][inp] -= batch_step_size *self.delta[out][inp]\n",
        "      self.delta[out][inp] = 0\n"
      ],
      "metadata": {
        "id": "FRciSVS99_0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##learnNN.py"
      ],
      "metadata": {
        "id": "oZJ9OFc8TgSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU_layer(Layer):\n",
        "  \"\"\"Rectified linear unit (ReLU) f(z) = max(0, z).\n",
        "  The number of outputs is equal to the number of inputs.\n",
        "  \"\"\"\n",
        "  def __init__(self, nn):\n",
        "    Layer.__init__(self, nn)\n",
        "\n",
        "  def output_values(self, input_values, training=False):\n",
        "    \"\"\"Returns the outputs for the input values.\n",
        "    It remembers the input values for the backprop.\n",
        "    \"\"\"\n",
        "    self.input_values = input_values\n",
        "    self.outputs= [max(0,inp) for inp in input_values]\n",
        "    return self.outputs\n",
        "  def backprop(self,errors):\n",
        "    \"\"\"Returns the derivative of the errors\"\"\"\n",
        "    return [e if inp>0 else 0 for e,inp in zip(errors, self.input_values)]"
      ],
      "metadata": {
        "id": "_R_U-6TH9_3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the old standards for the activation function for hidden layers is the\n",
        "sigmoid. It is included here to experiment with."
      ],
      "metadata": {
        "id": "vOckz-Nz9_6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##learnNN.py"
      ],
      "metadata": {
        "id": "hqZu8dhq9_9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid_layer(Layer):\n",
        "  \"\"\"sigmoids of the inputs.\n",
        "  The number of outputs is equal to the number of inputs.\n",
        "  Each output is the sigmoid of its corresponding input.\n",
        "  \"\"\"\n",
        "  def __init__(self, nn):\n",
        "    Layer.__init__(self, nn)\n",
        "  def output_values(self, input_values, training=False):\n",
        "    \"\"\"Returns the outputs for the input values.\n",
        "    It remembers the output values for the backprop.\n",
        "    \"\"\"\n",
        "    self.outputs= [sigmoid(inp) for inp in input_values]\n",
        "    return self.outputs\n",
        "  def backprop(self,errors):\n",
        "    \"\"\"Returns the derivative of the errors\"\"\"\n",
        "    return [e*out*(1-out) for e,out in zip(errors, self.outputs)]\n"
      ],
      "metadata": {
        "id": "zFzQfG1m-ABS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.2 Feedforward Networks\n"
      ],
      "metadata": {
        "id": "pM7uk46L-ADQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LearnNN.py"
      ],
      "metadata": {
        "id": "kSo59-xY-AGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(Learner):\n",
        "  def __init__(self, dataset, validation_proportion = 0.1,learning_rate=0.001):\n",
        "    \"\"\"Creates a neural network for a dataset,\n",
        "      layers is the list of layers\n",
        "      \"\"\"\n",
        "    self.dataset = dataset\n",
        "    self.output_type = dataset.target.ftype\n",
        "    self.learning_rate = learning_rate\n",
        "    self.input_features = dataset.input_features\n",
        "    self.num_outputs = len(self.input_features)\n",
        "    validation_num = int(len(self.dataset.train)*validation_proportion)\n",
        "    if validation_num > 0:\n",
        "      random.shuffle(self.dataset.train)\n",
        "      self.validation_set = self.dataset.train[-validation_num:]\n",
        "      self.training_set = self.dataset.train[:-validation_num]\n",
        "    else:\n",
        "      self.validation_set = []\n",
        "      self.training_set = self.dataset.train\n",
        "      self.layers = []\n",
        "      self.bn = 0 # number of batches run\n",
        "\n",
        "    def add_layer(self,layer):\n",
        "      \"\"\"add a layer to the network.\n",
        "      Each layer gets number of inputs from the previous layers outputs.\n",
        "      \"\"\"\n",
        "      self.layers.append(layer)\n",
        "      self.num_outputs = layer.num_outputs\n",
        "\n",
        "    def predictor(self,ex):\n",
        "      \"\"\"Predicts the value of the first output for example ex.\n",
        "      \"\"\"\n",
        "      values = [f(ex) for f in self.input_features]\n",
        "      for layer in self.layers:\n",
        "        values = layer.output_values(values)\n",
        "        return sigmoid(values[0]) if self.output_type ==\"boolean\" else softmax(values, self.dataset.target.frange) if self.output_type == \"categorical\" else values[0]\n",
        "\n",
        "    def predictor_string(self):\n",
        "      return \"not implemented\"\n",
        "\n"
      ],
      "metadata": {
        "id": "l4jsNfAeLhBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learn(self, epochs=5, batch_size=32, num_iter = None,report_each=10):\n",
        "  \"\"\"Learns parameters for a neural network using stochastic gradient decent.\n",
        "  epochs is the number of times through the data (on average)\n",
        "  batch_size is the maximum size of each batch num_iter is the number of iterations over the batches\n",
        "  - overrides epochs if provided (allows for fractions of epochs)\n",
        "  report_each means give the errors after each multiple of that iterations\n",
        "  \"\"\"\n",
        "  self.batch_size = min(batch_size, len(self.training_set)) # don't have batches bigger than training size\n",
        "  if num_iter is None:\n",
        "    num_iter = (epochs * len(self.training_set)) // self.batch_size\n",
        "  #self.display(0,\"Batch\\t\",\"\\t\".join(criterion.__doc__ for criterion in Evaluate.all_criteria))\n",
        "  for i in range(num_iter):\n",
        "    batch = random.sample(self.training_set, self.batch_size)\n",
        "    for e in batch:\n",
        "    # compute all outputs\n",
        "      values = [f(e) for f in self.input_features]\n",
        "      for layer in self.layers:\n",
        "        values = layer.output_values(values, training=True)\n",
        "      # backpropagate\n",
        "      predicted = [sigmoid(v) for v in values] if self.output_type == \"boolean\" else softmax(values) if self.output_type ==\"categorical\" else values\n",
        "      actuals = indicator(self.dataset.target(e),self.dataset.target.frange)  if self.output_type == \"categorical\" else [self.dataset.target(e)]\n",
        "      errors = [pred-obsd for (obsd,pred) in zip(actuals,predicted)]\n",
        "      for layer in reversed(self.layers):\n",
        "        errors = layer.backprop(errors)\n",
        "      # Update all parameters in batch\n",
        "    for layer in self.layers:\n",
        "      layer.update()\n",
        "    self.bn+=1\n",
        "    if (i+1)%report_each==0:\n",
        "      self.display(0,self.bn,\"\\t\",\"\\t\\t\".join(\"{:.4f}\".format(self.dataset.evaluate_dataset(self.validation_set,self.predictor, criterion))for criterion in Evaluate.all_criteria),sep=\"\")\n"
      ],
      "metadata": {
        "id": "qR6wT1yWLhD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.3 Improved Optimization"
      ],
      "metadata": {
        "id": "x1XhdAOeLhHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8.3.1 Momentum"
      ],
      "metadata": {
        "id": "E4plRZOSLhN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###learnNN.py"
      ],
      "metadata": {
        "id": "eis5cARWgEf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_complete_layer_momentum(Linear_complete_layer):\n",
        "  \"\"\"a completely connected layer\"\"\"\n",
        "  def __init__(self, nn, num_outputs, limit=None, alpha=0.9, epsilon =1e-07, vel0=0):\n",
        "    \"\"\"A completely connected linear layer.\n",
        "    nn is a neural network that the inputs come from\n",
        "    num_outputs is the number of outputs\n",
        "    max_init is the maximum value for random initialization of\n",
        "    meters\n",
        "    vel0 is the initial velocity for each parameter\n",
        "    \"\"\"\n",
        "    Linear_complete_layer.__init__(self, nn, num_outputs, limit=limit)\n",
        "   # self.weights[o][i] is the weight between input i and output o\n",
        "    self.velocity = [[vel0 for inf in range(self.num_inputs+1)] for outf in range(self.num_outputs)]\n",
        "    self.alpha = alpha\n",
        "    self.epsilon = epsilon\n",
        "  def update(self):\n",
        "   \"\"\"updates parameters after a batch\"\"\"\n",
        "   batch_step_size = self.nn.learning_rate / self.nn.batch_size\n",
        "   for out in range(self.num_outputs):\n",
        "    for inp in range(self.num_inputs+1):\n",
        "      self.velocity[out][inp] = self.alpha*self.velocity[out][inp]- batch_step_size * self.delta[out][inp]\n",
        "      self.weights[out][inp] += self.velocity[out][inp]\n",
        "      self.delta[out][inp] = 0\n"
      ],
      "metadata": {
        "id": "72X5Cp4sLhP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.3.2 RMS-Prop\n"
      ],
      "metadata": {
        "id": "1XKSwKtbLhR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_complete_layer_RMS_Prop(Linear_complete_layer):\n",
        "  \"\"\"a completely connected layer\"\"\"\n",
        "  def __init__(self, nn, num_outputs, limit=None, rho=0.9, epsilon = 1e-07):\n",
        "    \"\"\"A completely connected linear layer.\n",
        "     nn is a neural network that the inputs come from\n",
        "     num_outputs is the number of outputs\n",
        "     max_init is the maximum value for random initialization of parameters\n",
        "     \"\"\"\n",
        "    Linear_complete_layer.__init__(self, nn, num_outputs, limit=limit)\n",
        "    # self.weights[o][i] is the weight between input i and output o\n",
        "    self.ms = [[0 for inf in range(self.num_inputs+1)]\n",
        "    for outf in range(self.num_outputs)]\n",
        "    self.rho = rho\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def update(self):\n",
        "    \"\"\"updates parameters after a batch\"\"\"\n",
        "    for out in range(self.num_outputs):\n",
        "      for inp in range(self.num_inputs+1):\n",
        "        gradient = self.delta[out][inp] / self.nn.batch_size\n",
        "        self.ms[out][inp] = self.rho*self.ms[out][inp]+ (1-self.rho) * gradient**2\n",
        "        self.weights[out][inp] -=self.nn.learning_rate/(self.ms[out][inp]+self.epsilon)**0.5* gradient\n",
        "        self.delta[out][inp] = 0\n"
      ],
      "metadata": {
        "id": "nRhyELdKLhTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.4 Dropout\n"
      ],
      "metadata": {
        "id": "8jJ9z04RksJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utilities import flip\n",
        "class Dropout_layer(Layer):\n",
        " \"\"\"Dropout layer\n",
        " \"\"\"\n",
        " def __init__(self, nn, rate=0):\n",
        "  \"\"\"\n",
        "  rate is fraction of the input units to drop. 0 =< rate < 1\n",
        "  \"\"\"\n",
        "  self.rate = rate\n",
        "  Layer.__init__(self, nn)\n",
        "\n",
        " def output_values(self, input_values, training=False):\n",
        "    \"\"\"Returns the outputs for the input values.\n",
        "    It remembers the input values for the backprop.\n",
        "    \"\"\"\n",
        "    if training:\n",
        "      scaling = 1/(1-self.rate)\n",
        "      self.mask = [0 if flip(self.rate) else 1 for _ in input_values]\n",
        "      self.outputs= [x*y*scaling for (x,y) in zip(input_values, self.mask)]\n",
        "      return self.outputs\n",
        "    else:\n",
        "      return input_values\n",
        "\n",
        " def backprop(self,errors):\n",
        "  \"\"\"Returns the derivative of the errors\"\"\"\n",
        "  return [x*y for (x,y) in zip(errors, self.mask)]\n",
        "\n",
        "class Dropout_layer_0(Layer):\n",
        "  \"\"\"Dropout layer\n",
        "  \"\"\"\n",
        "  def __init__(self, nn, rate=0): \n",
        "   \"\"\"rate is fraction of the input units to drop. 0 =< rate < 1\"\"\"\n",
        "   self.rate = rate\n",
        "   Layer.__init__(self, nn)\n",
        "\n",
        "  def output_values(self, input_values, training=False):\n",
        "     \"\"\"Returns the outputs for the input values.\n",
        " It remembers the input values for the backprop.\n",
        "     \"\"\"\n",
        "     if training:\n",
        "      scaling = 1/(1-self.rate)\n",
        "      self.outputs= [0 if flip(self.rate) else inp*scaling # make 0 with probability rate\n",
        "      for inp in input_values]\n",
        "      return self.outputs\n",
        "     else:\n",
        "      return input_values\n",
        "\n",
        "  def backprop(self,errors):\n",
        "    \"\"\"Returns the derivative of the errors\"\"\"\n",
        "    return errors\n"
      ],
      "metadata": {
        "id": "RjSnwL_8ksMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4.1 Examples"
      ],
      "metadata": {
        "id": "pBAbwZD2ksP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following constructs a neural network with one hidden layer. The hidden\n",
        "layer has width 2 with a ReLU activation function. The output layer used a\n",
        "sigmoid"
      ],
      "metadata": {
        "id": "PWrvVCVim7Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data = Data_from_file('data/mail_reading.csv', target_index=-1)\n",
        "#data = Data_from_file('data/mail_reading_consis.csv', target_index=-1)\n",
        "data = Data_from_file('data/SPECT.csv', prob_test=0.5, target_index=0)\n",
        "#data = Data_from_file('data/iris.data', prob_test=0.2, target_index=-1) #\n",
        "# examples approx 120 test + 30 test\n",
        "#data = Data_from_file('data/if_x_then_y_else_z.csv', num_train=8, target_index=-1) # not linearly sep\n",
        "#data = Data_from_file('data/holiday.csv', target_index=-1) #, num_train=19)\n",
        "#data = Data_from_file('data/processed.cleveland.data', target_index=-1)\n",
        "random.seed(None)\n",
        "nn1 = NN(data)\n",
        "nn1.add_layer(Linear_complete_layer(nn1,3))\n",
        "#nn1.add_layer(Sigmoid_layer(nn1)) # comment this or the next\n",
        "nn1.add_layer(ReLU_layer(nn1))\n",
        "#nn1.add_layer(Linear_complete_layer(nn1,1)) # when using output_type=\"boolean\"\n",
        "nn1.add_layer(Linear_complete_layer(nn1,1)) # when using output_type=\"categorical\"\n",
        "#nn1.learn(epochs = 100)\n",
        "\n",
        "nn1do = NN(data)\n",
        "nn1do.add_layer(Linear_complete_layer(nn1do,3))\n",
        "#nn1.add_layer(Sigmoid_layer(nn1)) # comment this or the next\n",
        "nn1do.add_layer(ReLU_layer(nn1do))\n",
        "nn1do.add_layer(Dropout_layer(nn1do, rate=0.5))\n",
        "#nn1.add_layer(Linear_complete_layer(nn1do,1)) # when using output_type=\"boolean\"\n",
        "nn1do.add_layer(Linear_complete_layer(nn1do,1)) # when using output_type=\"categorical\"\n",
        "#nn1do.learn(epochs = 100)\n",
        "nn_r1 = NN(data)\n",
        "nn_r1.add_layer(Linear_complete_layer_RMS_Prop(nn_r1,3))\n",
        "#nn_r1.add_layer(Sigmoid_layer(nn_r1)) # comment this or the next\n",
        "nn_r1.add_layer(ReLU_layer(nn_r1))\n",
        "#nn_r1.add_layer(Linear_complete_layer(nn_r1,1)) # when using output_type=\"boolean\"\n",
        "nn_r1.add_layer(Linear_complete_layer_RMS_Prop(nn_r1,1)) # when using output_type=\"categorical\"\n",
        "#nn_r1.learn(epochs = 100)\n",
        "nnm1 = NN(data)\n",
        "nnm1.add_layer(Linear_complete_layer_momentum(nnm1,3))\n",
        "#nnm1.add_layer(Sigmoid_layer(nnm1)) # comment this or the next\n",
        "nnm1.add_layer(ReLU_layer(nnm1))\n",
        "#nnm1.add_layer(Linear_complete_layer(nnm1,1)) # when using output_type=\"boolean\"\n",
        "nnm1.add_layer(Linear_complete_layer_momentum(nnm1,1)) # when using output_type=\"categorical\"\n",
        "#nnm1.learn(epochs = 100)\n",
        "nn2 = NN(data) #\"boolean\") #\n",
        "nn2.add_layer(Linear_complete_layer_RMS_Prop(nn2,2))\n",
        "nn2.add_layer(ReLU_layer(nn2))\n",
        "nn2.add_layer(Linear_complete_layer_RMS_Prop(nn2,1)) # when using output_type=\"categorical\"\n",
        "nn3 = NN(data) #\"boolean\") #\n",
        "nn3.add_layer(Linear_complete_layer_RMS_Prop(nn3,5))\n",
        "nn3.add_layer(ReLU_layer(nn3))\n",
        "nn3.add_layer(Linear_complete_layer_RMS_Prop(nn3,1)) # when using output_type=\"categorical\"\n",
        "nn0 = NN(data,learning_rate=0.05)\n",
        "nn0.add_layer(Linear_complete_layer(nn0,1)) # categorical linear regression\n",
        "#nn0.add_layer(Linear_complete_layer_RMS_Prop(nn0,1)) # categorical linear regression"
      ],
      "metadata": {
        "id": "L4UEzCujm7Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##learnNN.py"
      ],
      "metadata": {
        "id": "coQEyK60m7GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from learnLinear import plot_steps\n",
        "383 from learnProblem import Evaluate\n",
        "384\n",
        "385 # To show plots:\n",
        "386 # plot_steps(learner = nn1, data = data, criterion=Evaluate.log_loss,\n",
        "num_steps=10000, log_scale=False, legend_label=\"nn1\")\n",
        "387 # plot_steps(learner = nn2, data = data, criterion=Evaluate.log_loss,\n",
        "num_steps=10000, log_scale=False, legend_label=\"nn2\")\n",
        "388 # plot_steps(learner = nn3, data = data, criterion=Evaluate.log_loss,\n",
        "num_steps=100000, log_scale=False, legend_label=\"nn3\")\n",
        "389 # plot_steps(learner = nn0, data = data, criterion=Evaluate.log_loss,\n",
        "num_steps=10000, log_scale=False, legend_label=\"nn0\")\n",
        "390\n",
        "391 # plot_steps(learner = nn0, data = data, criterion=Evaluate.accuracy,\n",
        "num_steps=10000, log_scale=False, legend_label=\"nn0\")\n",
        "392 # plot_steps(learner = nn1, data = data, criterion=Evaluate.accuracy,\n",
        "num_steps=10000, log_scale=False, legend_label=\"nn1\")\n",
        "393 # plot_steps(learner = nn2, data = data, criterion=Evaluate.accuracy,\n",
        "num_steps=10000, log_scale=False, legend_label=\"nn2\")\n",
        "394 # plot_steps(learner = nn3, data = data, criterion=Evaluate.accuracy,\n",
        "num_steps=10000, log_scale=False, legend_label=\"nn3\")\n",
        "395\n",
        "396\n",
        "397 # Print some training examples\n",
        "398 #for eg in random.sample(data.train,10): print(eg,nn1.predictor(eg))\n",
        "399\n",
        "400 # Print some test examples\n",
        "401 #for eg in random.sample(data.test,10): print(eg,nn1.predictor(eg))\n",
        "402\n",
        "403 # To see the weights learned in linear layers\n",
        "404 # nn1.layers[0].weights\n",
        "405 # nn1.layers[2].weights\n",
        "406\n",
        "407 # Print test:\n",
        "408 # for e in data.train: print(e,nn0.predictor(e))\n",
        "409\n",
        "410 def test(data, hidden_widths = [5], epochs=100,\n",
        "411 optimizers = [Linear_complete_layer,\n",
        "412 Linear_complete_layer_momentum,\n",
        "Linear_complete_layer_RMS_Prop]):\n",
        "413 data.display(0,\"Batch\\t\",\"\\t\".join(criterion.__doc__ for criterion in\n",
        "Evaluate.all_criteria))\n",
        "414 for optimizer in optimizers:\n",
        "415 nn = NN(data)\n",
        "416 for width in hidden_widths:\n",
        "nn.add_layer(optimizer(nn,width))\n",
        "418 nn.add_layer(ReLU_layer(nn))\n",
        "419 if data.target.ftype == \"boolean\":\n",
        "420 nn.add_layer(optimizer(nn,1))\n",
        "421 else:\n",
        "422 error(f\"Not implemented: {data.output_type}\")\n",
        "423 nn.learn(epochs)"
      ],
      "metadata": {
        "id": "OM5sC2tNm7JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following tests on MNIST. The original files are from http://yann.lecun.\n",
        "com/exdb/mnist/. This code assumes you use the csv files from https://pjreddie.\n",
        "com/projects/mnist-in-csv/, and put them in the directory ../MNIST/. Note\n",
        "that this is very inefficient; you would be better to use Keras or Pytorch. There\n",
        "are 28 ∗ 28 = input units and 512 hidden units, which makes 401,408 parameters for the lowest linear layer. So don’t be surprised when it takes many hours."
      ],
      "metadata": {
        "id": "euQ23Kk0m7Lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified version: (6000 training instances)\n",
        "428 # data_mnist = Data_from_file('../MNIST/mnist_train.csv', prob_test=0.9,\n",
        "target_index=0, boolean_features=False, target_type=\"categorical\")\n",
        "429\n",
        "430 # Full version:\n",
        "431 # data_mnist = Data_from_files('../MNIST/mnist_train.csv', '../MNIST/mnist_test.csv', target_index=0, boolean_features=False, target_type=\"categorical\")\n",
        "432\n",
        "433 # nn_mnist = NN(data_mnist, validation_proportion = 0.02,\n",
        "learning_rate=0.001) #validation set = 1200\n",
        "434 # nn_mnist.add_layer(Linear_complete_layer_RMS_Prop(nn_mnist,512));\n",
        "nn_mnist.add_layer(ReLU_layer(nn_mnist));\n",
        "nn_mnist.add_layer(Linear_complete_layer_RMS_Prop(nn_mnist,10))\n",
        "435 # start_time = time.perf_counter();nn_mnist.learn(epochs=1, batch_size=128);end_time = time.perf_counter();print(\"Time:\", end_time - start_time,\"seconds\") #1 epoch\n",
        "436 # determine test error:\n",
        "437 # data_mnist.evaluate_dataset(data_mnist.test, nn_mnist.predictor, Evaluate.accuracy)\n",
        "438 # Print some random predictions:\n",
        "439 # for eg in random.sample(data_mnist.test,10):\n",
        "print(data_mnist.target(eg),nn_mnist.predictor(eg),nn_mnist.predictor(eg)[data_mnist.target(eg)])\n"
      ],
      "metadata": {
        "id": "2bnF3ljzm7Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter-5 Multiagent System"
      ],
      "metadata": {
        "id": "WQIEdqXjm7R7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12.1 Minimax\n"
      ],
      "metadata": {
        "id": "CgcHguHLp7aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from display import Displayable\n",
        "\n",
        "class Node(Displayable):\n",
        " \"\"\"A node in a search tree. It has a\n",
        " name a string\n",
        " isMax is True if it is a maximizing node, otherwise it is minimizing node\n",
        " children is the list of children\n",
        " value is what it evaluates to if it is a leaf.\n",
        " \"\"\"\n",
        " def __init__(self, name, isMax, value, children):\n",
        "  self.name = name\n",
        "  self.isMax = isMax\n",
        "  self.value = value\n",
        "  self.allchildren = children\n",
        "\n",
        " def isLeaf(self):\n",
        "    \"\"\"returns true of this is a leaf node\"\"\"\n",
        "    return self.allchildren is None def children(self):\n",
        "    \"\"\"returns the list of all children.\"\"\"\n",
        "    return self.allchildren\n",
        "\n",
        " def evaluate(self):\n",
        "    \"\"\"returns the evaluation for this node if it is a leaf\"\"\"\n",
        "    return self.value\n"
      ],
      "metadata": {
        "id": "qbRVw89xp7gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##masProblem.py"
      ],
      "metadata": {
        "id": "Wke2eKRWp7if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig10_5 = Node(\"a\",True,None, [\n",
        " Node(\"b\",False,None, [\n",
        " Node(\"d\",True,None, [\n",
        " Node(\"h\",False,None, [\n",
        " Node(\"h1\",True,7,None),\n",
        " Node(\"h2\",True,9,None)]),\n",
        " Node(\"i\",False,None, [\n",
        " Node(\"i1\",True,6,None),\n",
        " Node(\"i2\",True,888,None)])]),\n",
        " Node(\"e\",True,None, [\n",
        " Node(\"j\",False,None, [\n",
        " Node(\"j1\",True,11,None),\n",
        " Node(\"j2\",True,12,None)]),\n",
        " Node(\"k\",False,None, [\n",
        " Node(\"k1\",True,888,None),\n",
        " Node(\"k2\",True,888,None)])])]),\n",
        " Node(\"c\",False,None, [\n",
        " Node(\"f\",True,None, [\n",
        " Node(\"l\",False,None, [\n",
        " Node(\"l1\",True,5,None),\n",
        " Node(\"l2\",True,888,None)]),\n",
        " Node(\"m\",False,None, [\n",
        " Node(\"m1\",True,4,None),\n",
        " Node(\"m2\",True,888,None)])]),\n",
        " Node(\"g\",True,None, [\n",
        " Node(\"n\",False,None, [\n",
        " Node(\"n1\",True,888,None),\n",
        " Node(\"n2\",True,888,None)]),\n",
        " Node(\"o\",False,None, [\n",
        " Node(\"o1\",True,888,None),\n",
        " Node(\"o2\",True,888,None)])])])])\n"
      ],
      "metadata": {
        "id": "xRvDzN-jp7lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a representation of a magic-sum game, where players take\n",
        "turns picking a number in the range [1, 9], and the first player to have 3 numbers that sum to 15 wins. Note that this is a syntactic variant of tic-tac-toe or\n",
        "naughts and crosses. To see this, consider the numbers on a magic square (Figure 12.1); 3 numbers that add to 15 correspond exactly to the winning positions"
      ],
      "metadata": {
        "id": "tp5BdcnKp7nM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## masProblem.py"
      ],
      "metadata": {
        "id": "ShInnBsqq0SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Magic_sum(Node):\n",
        " def __init__(self, xmove=True, last_move=None,\n",
        "    available=[1,2,3,4,5,6,7,8,9], x=[], o=[]):\n",
        "    \"\"\"This is a node in the search for the magic-sum game.\n",
        "    xmove is True if the next move belongs to X.\n",
        "    last_move is the number selected in the last move\n",
        "    available is the list of numbers that are available to be chosen\n",
        "    x is the list of numbers already chosen by x\n",
        "    o is the list of numbers already chosen by o\n",
        "    \"\"\"\n",
        "    self.isMax = self.xmove = xmove\n",
        "    self.last_move = last_move\n",
        "    self.available = available\n",
        "    self.x = x\n",
        "    self.o = o\n",
        "    self.allchildren = None #computed on demand\n",
        "    lm = str(last_move)\n",
        "    self.name = \"start\" if not last_move else \"o=\"+lm if xmove else \"x=\"+lm\n",
        "\n",
        " def children(self):\n",
        "    if self.allchildren is None:\n",
        "      if self.xmove:\n",
        "        self.allchildren = [\n",
        "        Magic_sum(xmove = not self.xmove,\n",
        "        last_move = sel,\n",
        "        available = [e for e in self.available if e is not sel], x = self.x+[sel], o = self.o) for sel in self.available]\n",
        "      else:\n",
        "        self.allchildren = [Magic_sum(xmove = not self.xmove, last_move = sel, available = [e for e in self.available if e is not sel], x = self.x, o = self.o+[sel]) for sel in self.available]\n",
        "        return self.allchildren\n",
        "\n",
        " def isLeaf(self):\n",
        "  \"\"\"A leaf has no numbers available or is a win for one of the\n",
        "  players.\n",
        "  We only need to check for a win for o if it is currently x's turn,\n",
        "  and only check for a win for x if it is o's turn (otherwise it would\n",
        "  have been a win earlier).\n",
        "  \"\"\"\n",
        "  return (self.available == [] or\n",
        "  (sum_to_15(self.last_move,self.o)\n",
        "  if self.xmove\n",
        "  else sum_to_15(self.last_move,self.x)))\n",
        "\n",
        " def evaluate(self):\n",
        "  if self.xmove and sum_to_15(self.last_move,self.o):\n",
        "    return -1\n",
        "  elif not self.xmove and sum_to_15(self.last_move,self.x):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        " def sum_to_15(last,selected):\n",
        "  \"\"\"is true if last, toegether with two other elements of selected sum to 15.\n",
        "  \"\"\"\n",
        "  return any(last+a+b == 15 for a in selected if a != last for b in selected if b != last and b != a)\n"
      ],
      "metadata": {
        "id": "HffQZ2bgq0VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12.1.2 Minimax and α-β Pruning"
      ],
      "metadata": {
        "id": "HtUt-xATq0Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a naive depth-first minimax algorithm:"
      ],
      "metadata": {
        "id": "n6fQtPp_q0bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimax(node,depth):\n",
        "  \"\"\"returns the value of node, and a best path for the agents\n",
        "  \"\"\"\n",
        "  if node.isLeaf():\n",
        "    return node.evaluate(),None\n",
        "  elif node.isMax:\n",
        "    max_score = float(\"-inf\")\n",
        "    max_path = None\n",
        "    for C in node.children():\n",
        "      score,path = minimax(C,depth+1)\n",
        "      if score > max_score:\n",
        "        max_score = score\n",
        "        max_path = C.name,path\n",
        "      return max_score,max_path\n",
        "  else:\n",
        "    min_score = float(\"inf\")\n",
        "    min_path = None\n",
        "    for C in node.children():\n",
        "      score,path = minimax(C,depth+1)\n",
        "      if score < min_score:\n",
        "        min_score = score\n",
        "        min_path = C.name,path\n",
        "        return min_score,min_path \n",
        "def minimax_alpha_beta(node,alpha,beta,depth=0):\n",
        "  \"\"\"node is a Node, alpha and beta are cutoffs, depth is the depth\n",
        "  returns value, path\n",
        "  where path is a sequence of nodes that results in the value\n",
        "\"\"\"\n",
        "  node.display(2,\" \"*depth,\"minimax_alpha_beta(\",node.name,\", \",alpha, \", \", beta,\")\")\n",
        "  best=None # only used if it will be pruned\n",
        "  if node.isLeaf():\n",
        "    node.display(2,\" \"*depth,\"returning leaf value\",node.evaluate())\n",
        "    return node.evaluate(),None\n",
        "  elif node.isMax:\n",
        "    for C in node.children():\n",
        "      score,path = minimax_alpha_beta(C,alpha,beta,depth+1)\n",
        "      if score >= beta: # beta pruning\n",
        "         node.display(2,\" \"*depth,\"pruned due to beta=\",beta,\"C=\",C.name)\n",
        "         return score, None\n",
        "      if score > alpha:\n",
        "        alpha = score\n",
        "        best = C.name, path\n",
        "        node.display(2,\" \"*depth,\"returning max alpha\",alpha,\"best\",best)\n",
        "        return alpha,best\n",
        "  else:\n",
        "    for C in node.children():\n",
        "      score,path = minimax_alpha_beta(C,alpha,beta,depth+1)\n",
        "      if score <= alpha: # alpha pruning\n",
        "        node.display(2,\" \"*depth,\"pruned due to alpha=\",alpha,\"C=\",C.name)\n",
        "        return score, None\n",
        "      if score < beta:\n",
        "        beta=score\n",
        "        best = C.name,path\n",
        "        node.display(2,\" \"*depth,\"returning min beta\",beta,\"best=\",best)\n",
        "        return beta,best"
      ],
      "metadata": {
        "id": "RF0TguVdq0dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from masProblem import fig10_5, Magic_sum, Node\n",
        "\n",
        "# Node.max_display_level=2 # print detailed trace\n",
        " # minimax_alpha_beta(fig10_5, -9999, 9999,0)\n",
        " # minimax_alpha_beta(Magic_sum(), -9999, 9999,0)\n",
        "\n",
        " #To see how much time alpha-beta pruning can save over minimax, uncomment the following:\n",
        "\n",
        "## import timeit\n",
        " ## timeit.Timer(\"minimax(Magic_sum(),0)\",setup=\"from __main__ import minimax, Magic_sum\"\n",
        " ## ).timeit(number=1)\n",
        " ## trace=False\n",
        " ## timeit.Timer(\"minimax_alpha_beta(Magic_sum(), -9999, 9999,0)\",\n",
        " ## setup=\"from __main__ import minimax_alpha_beta, Magic_sum\"\n",
        " ## ).timeit(number=1)"
      ],
      "metadata": {
        "id": "xEYCKLK9q0gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter-6 Reinforcement Learning\n"
      ],
      "metadata": {
        "id": "e0lET3zeOJwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13.1 Representing Agents and Environments\n"
      ],
      "metadata": {
        "id": "_21MHvxsOJzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the learning agent does an action in the environment, it observes a (state,reward)\n",
        "pair from the environment. The state is the world state; this is the fully observable assumption."
      ],
      "metadata": {
        "id": "ovZeXmK3OJ2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An RL environment implements a do(action) method that returns a (state,reward)\n",
        "pair."
      ],
      "metadata": {
        "id": "TbuHO3tyOJ4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##rlProblem.py"
      ],
      "metadata": {
        "id": "_0i8rQzXOJ76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from display import Displayable\n",
        "from utilities import flip\n",
        "class RL_env(Displayable):\n",
        "    def __init__(self,actions,state):\n",
        "      self.actions = actions # set of actions\n",
        "      self.state = state # initial state\n",
        "    def do(self, action):\n",
        "      \"\"\"do action\n",
        "      returns state,reward\n",
        "      \"\"\"\n",
        "      raise NotImplementedError(\"RL_env.do\") # abstract method"
      ],
      "metadata": {
        "id": "9SkUV9NVOJ-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the definition of the simple 2-state, 2-action party/relax decision"
      ],
      "metadata": {
        "id": "rBRoc78fOKBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Healthy_env(RL_env):\n",
        "  def __init__(self):\n",
        "      RL_env.__init__(self,[\"party\",\"relax\"], \"healthy\")\n",
        "  def do(self, action):\n",
        "      \"\"\"updates the state based on the agent doing action.\n",
        "      returns state,reward\n",
        "      \"\"\"\n",
        "      if self.state==\"healthy\":\n",
        "        if action==\"party\":\n",
        "          self.state = \"healthy\" if flip(0.7) else \"sick\"\n",
        "          reward = 10\n",
        "        else: # action==\"relax\"\n",
        "          self.state = \"healthy\" if flip(0.95) else \"sick\"\n",
        "          reward = 7\n",
        "      else: # self.state==\"sick\"\n",
        "          if action==\"party\":\n",
        "            self.state = \"healthy\" if flip(0.1) else \"sick\"\n",
        "            reward = 2\n",
        "          else:\n",
        "            self.state = \"healthy\" if flip(0.5) else \"sick\"\n",
        "            reward = 0\n",
        "      return self.state,reward"
      ],
      "metadata": {
        "id": "MEZj0q_XOKEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13.1.1 Simulating an environment from an MDP\n"
      ],
      "metadata": {
        "id": "Vy6Bz_ZUOKQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the MDP does not contain enough information to simulate a system, because it loses any dependency between the rewards and the resulting\n",
        "state; here we assume the agent always received the average reward for the\n",
        "state and action.\n"
      ],
      "metadata": {
        "id": "Cft5DetUOKWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##rlProblem.py"
      ],
      "metadata": {
        "id": "nExik5deOKZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Env_from_MDP(RL_env):\n",
        "def __init__(self, mdp):\n",
        "    initial_state = mdp.states[0]\n",
        "    RL_env.__init__(self,mdp.actions, initial_state)\n",
        "    self.mdp = mdp\n",
        "    self.action_index = {action:index for (index,action) in enumerate(mdp.actions)}\n",
        "    self.state_index = {state:index for (index,state) in enumerate(mdp.states)}\n",
        "def do(self, action):\n",
        "    \"\"\"updates the state based on the agent doing action.\n",
        "    returns state,reward\n",
        "    \"\"\"\n",
        "    action_ind = self.action_index[action]\n",
        "    state_ind = self.state_index[self.state]\n",
        "    self.state = pick_from_dist(self.mdp.trans[state_ind][action_ind], self.mdp.states)\n",
        "    reward = self.mdp.reward[state_ind][action_ind]return self.state, reward\n",
        "def pick_from_dist(dist,values):\n",
        "    \"\"\"\n",
        "    e.g. pick_from_dist([0.3,0.5,0.2],['a','b','c']) should pick 'a' withrobability 0.3, etc.\n",
        "    \"\"\"\n",
        "    ran = random.random()\n",
        "    i=0\n",
        "    while ran>dist[i]:\n",
        "      ran -= dist[i]\n",
        "      i += 1\n",
        "    return values[i]"
      ],
      "metadata": {
        "id": "ZtfqJ4VZOKcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.1.2 Simple Game\n"
      ],
      "metadata": {
        "id": "Uu_L0-eROKjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from utilities import flip\n",
        "from rlProblem import RL_env\n",
        "class Simple_game_env(RL_env):\n",
        "    xdim = 5\n",
        "    ydim = 5\n",
        "    vwalls = [(0,3), (0,4), (1,4)] # vertical walls right of these locations\n",
        "    hwalls = [] # not implemented\n",
        "    crashed_reward = -1\n",
        "    prize_locs = [(0,0), (0,4), (4,0), (4,4)]\n",
        "    prize_apears_prob = 0.3\n",
        "    prize_reward = 10\n",
        "    monster_locs = [(0,1), (1,1), (2,3), (3,1), (4,2)]\n",
        "    monster_appears_prob = 0.4\n",
        "    monster_reward_when_damaged = -10\n",
        "    repair_stations = [(1,4)]\n",
        "    actions = [\"up\",\"down\",\"left\",\"right\"]\n",
        "def __init__(self):\n",
        "    # State:\n",
        "    self.x = 2\n",
        "    self.y = 2\n",
        "    self.damaged = False\n",
        "    self.prize = None\n",
        "    # Statistics\n",
        "    self.number_steps = 0\n",
        "    self.total_reward = 0\n",
        "    self.min_reward = 0\n",
        "    self.min_step = 0\n",
        "    self.zero_crossing = 0\n",
        "    RL_env.__init__(self, Simple_game_env.actions,\n",
        "    (self.x, self.y, self.damaged, self.prize))\n",
        "    self.display(2,\"\",\"Step\",\"Tot Rew\",\"Ave Rew\",sep=\"\\t\")\n",
        "\n",
        "def do(self,action):\n",
        "    \"\"\"updates the state based on the agent doing action.\n",
        "    returns state,reward\n",
        "    \"\"\"\n",
        "    reward = 0.0\n",
        "    # A prize can appear:\n",
        "    if self.prize is None and flip(self.prize_apears_prob):\n",
        "    self.prize = random.choice(self.prize_locs)\n",
        "    # Actions can be noisy\n",
        "    if flip(0.4):\n",
        "      actual_direction = random.choice(self.actions)\n",
        "    else:\n",
        "      actual_direction = action\n",
        "    # Modeling the actions given the actual direction\n",
        "    if actual_direction == \"right\":\n",
        "      if self.x==self.xdim-1 or (self.x,self.y) in self.vwalls:\n",
        "          reward += self.crashed_reward\n",
        "      else:\n",
        "          self.x += 1\n",
        "    elif actual_direction == \"left\":\n",
        "        if self.x==0 or (self.x-1,self.y) in self.vwalls:\n",
        "            reward += self.crashed_reward\n",
        "\n",
        "        else:\n",
        "          self.x += -1\n",
        "    elif actual_direction == \"up\":\n",
        "        if self.y==self.ydim-1:\n",
        "            reward += self.crashed_reward\n",
        "        else:\n",
        "            self.y += 1\n",
        "    elif actual_direction == \"down\":\n",
        "        if self.y==0:\n",
        "            reward += self.crashed_reward\n",
        "        else:\n",
        "            self.y += -1\n",
        "    else:\n",
        "        raise RuntimeError(\"unknown_direction \"+str(direction))\n",
        "    # Monsters\n",
        "    if (self.x,self.y) in self.monster_locs and flip(self.monster_appears_prob):\n",
        "        if self.damaged:\n",
        "            reward += self.monster_reward_when_damaged\n",
        "        else:\n",
        "            self.damaged = True\n",
        "    if (self.x,self.y) in self.repair_stations:\n",
        "        self.damaged = False\n",
        "     # Prizes\n",
        "    if (self.x,self.y) == self.prize:\n",
        "      reward += self.prize_reward\n",
        "      self.prize = None\n",
        "\n",
        "    # Statistics\n",
        "    self.number_steps += 1\n",
        "    self.total_reward += reward\n",
        "    if self.total_reward < self.min_reward:\n",
        "        self.min_reward = self.total_reward\n",
        "        self.min_step = self.number_steps\n",
        "    if self.total_reward>0 and reward>self.total_reward:\n",
        "        self.zero_crossing = self.number_steps\n",
        "    self.display(2,\"\",self.number_steps,self.total_reward,self.total_reward/self.number_steps,sep=\"\\t\")\n",
        "    return (self.x, self.y, self.damaged, self.prize), reward\n"
      ],
      "metadata": {
        "id": "y8giGrS7OKmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.1.3 Evaluation and Plotting\n"
      ],
      "metadata": {
        "id": "A4w5fzKGOKpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_rl(ag, label=None, yplot='Total', step_size=None,\n",
        "    steps_explore=1000, steps_exploit=1000, xscale='linear'):\n",
        "    \"\"\"\n",
        "    plots the agent ag\n",
        "    label is the label for the plot\n",
        "    yplot is 'Average' or 'Total'\n",
        "    step_size is the number of steps between each point plotted\n",
        "    steps_explore is the number of steps the agent spends exploring\n",
        "    steps_exploit is the number of steps the agent spends exploiting\n",
        "    xscale is 'log' or 'linear'\n",
        "\n",
        "    returns total reward when exploring, total reward when exploiting\n",
        "    \"\"\"\n",
        "    assert yplot in ['Average','Total']\n",
        "    if step_size is None:\n",
        "        step_size = max(1,(steps_explore+steps_exploit)//500)\n",
        "    if label is None:\n",
        "        label = ag.label\n",
        "    ag.max_display_level,old_mdl = 1,ag.max_display_level\n",
        "    plt.ion()\n",
        "    plt.xscale(xscale)\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(yplot+\" reward\")\n",
        "    steps = [] # steps\n",
        "    rewards = [] # return\n",
        "    ag.restart()\n",
        "    step = 0\n",
        "\n",
        "    while step < steps_explore:\n",
        "    ag.do(step_size)\n",
        "    step += step_size\n",
        "    steps.append(step)\n",
        "    if yplot == \"Average\":\n",
        "        rewards.append(ag.acc_rewards/step)\n",
        "    else:\n",
        "        rewards.append(ag.acc_rewards)\n",
        "    acc_rewards_exploring = ag.acc_rewards\n",
        "    ag.explore,explore_save = 0,ag.explore\n",
        "    while step < steps_explore+steps_exploit:\n",
        "        ag.do(step_size)\n",
        "        step += step_size\n",
        "        steps.append(step)\n",
        "        if yplot == \"Average\":\n",
        "            rewards.append(ag.acc_rewards/step)\n",
        "        else:\n",
        "            rewards.append(ag.acc_rewards)\n",
        "    plt.plot(steps,rewards,label=label)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.draw()\n",
        "    ag.max_display_level = old_mdl\n",
        "    ag.explore=explore_save\n",
        "    return acc_rewards_exploring, ag.acc_rewards-acc_rewards_exploring"
      ],
      "metadata": {
        "id": "fJNqhTDHOKtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.2 Q Learning\n"
      ],
      "metadata": {
        "id": "68MRLMlzOKv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from display import Displayable\n",
        "from utilities import argmaxe, flip\n",
        "class RL_agent(Displayable):\n",
        "    \"\"\"An RL_Agent\n",
        "    has percepts (s, r) for some state s and real reward r\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "m0Mn1hd5OKzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##rlQLearner.py"
      ],
      "metadata": {
        "id": "SD2ovjPqOK14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Q_learner(RL_agent):\n",
        " \"\"\"A Q-learning agent has\n",
        " belief-state consisting of\n",
        " state is the previous state\n",
        " q is a {(state,action):value} dict\n",
        " visits is a {(state,action):n} dict. n is how many times action was done in state\n",
        " acc_rewards is the accumulated reward\n",
        "\n",
        " it observes (s, r) for some world-state s and real reward r\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "6xy8qhjEOK5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, env, discount, explore=0.1, fixed_alpha=True, alpha=0.2, alpha_fun=lambda k:1/k, qinit=0, label=\"Q_learner\"):\n",
        "    \"\"\"env is the environment to interact with.\n",
        "    discount is the discount factor\n",
        "    explore is the proportion of time the agent will explore\n",
        "    fixed_alpha specifies whether alpha is fixed or varies with the number of visits\n",
        "    alpha is the weight of new experiences compared to old experiences\n",
        "    alpha_fun is a function that computes alpha from the number of visits\n",
        "    qinit is the initial value of the Q's\n",
        "    label is the label for plotting\n",
        "    \"\"\"\n",
        "    RL_agent.__init__(self)\n",
        "    self.env = env\n",
        "    self.actions = env.actions\n",
        "    self.discount = discount\n",
        "    self.explore = explore\n",
        "    self.fixed_alpha = fixed_alpha\n",
        "    self.alpha = alpha\n",
        "    self.alpha_fun = alpha_fun\n",
        "    self.qinit = qinit\n",
        "    self.label = label\n",
        "    self.restart()\n"
      ],
      "metadata": {
        "id": "rFSwa676fS17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "restart is used to make the learner relearn everything. This is used by the plotter to create new plots."
      ],
      "metadata": {
        "id": "0kEAguIWfS49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def restart(self):\n",
        " \"\"\"make the agent relearn, and reset the accumulated rewards\n",
        " \"\"\"\n",
        " self.acc_rewards = 0\n",
        " self.state = self.env.state\n",
        " self.q = {}\n",
        " self.visits = {}\n"
      ],
      "metadata": {
        "id": "L9JOgD-zfS8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "do takes in the number of steps."
      ],
      "metadata": {
        "id": "yIEmUrATfS-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##rlQLearner.py"
      ],
      "metadata": {
        "id": "d1PzSRp4fTAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do(self,num_steps=100):\n",
        "  \"\"\"do num_steps of interaction with the environment\"\"\"\n",
        "  self.display(2,\"s\\ta\\tr\\ts'\\tQ\")\n",
        "  alpha = self.alpha\n",
        "  for i in range(num_steps):\n",
        "      action = self.select_action(self.state)\n",
        "      next_state,reward = self.env.do(action)\n",
        "      if not self.fixed_alpha:\n",
        "          k = self.visits[(self.state, action)] = self.visits.get((self.state, action),0)+1\n",
        "          alpha = self.alpha_fun(k)\n",
        "          self.q[(self.state, action)] = ((1-alpha) * self.q.get((self.state, action),self.qinit)+ alpha * (reward + self.discount\n",
        "                        * max(self.q.get((next_state, next_act),self.qinit) for next_act in self.actions)))\n",
        "      self.display(2,self.state, action, reward, next_state,\n",
        "      self.q[(self.state, action)], sep='\\t')\n",
        "      self.state = next_state\n",
        "      self.acc_rewards += reward\n"
      ],
      "metadata": {
        "id": "iw0TudENfjiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(self, state):\n",
        "  \"\"\"returns an action to carry out for the current agent given the state, and the q-function\n",
        "  \"\"\"\n",
        "  if flip(self.explore):\n",
        "     return random.choice(self.actions)\n",
        "  else:\n",
        "     return argmaxe((next_act, self.q.get((state, next_act),self.qinit)) for next_act in self.actions)"
      ],
      "metadata": {
        "id": "sX0kNQ-mfjlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.2.1 Testing Q-learning"
      ],
      "metadata": {
        "id": "x2HavIJafjoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rlProblem import Healthy_env\n",
        "from rlQLearner import Q_learner\n",
        "from rlPlot import plot_rl\n",
        "env = Healthy_env()\n",
        "ag = Q_learner(env, 0.7)\n",
        "ag_opt = Q_learner(env, 0.7, qinit=100, label=\"optimistic\" ) # optimistic agent\n",
        "ag_exp_l = Q_learner(env, 0.7, explore=0.01, label=\"less explore\")\n",
        "ag_exp_m = Q_learner(env, 0.7, explore=0.5, label=\"more explore\")\n",
        "ag_disc = Q_learner(env, 0.9, qinit=100, label=\"disc 0.9\")\n",
        "ag_va = Q_learner(env, 0.7, qinit=100,fixed_alpha=False,alpha_fun=lambda k:10/(9+k),label=\"alpha=10/(9+k)\")\n",
        "# ag.max_display_level = 2\n",
        "# ag.do(20)\n",
        "# ag.q # get the learned q-values\n",
        "# ag.max_display_level = 1\n",
        "# ag.do(1000)\n",
        "# ag.q # get the learned q-values\n",
        "# plot_rl(ag,yplot=\"Average\")\n",
        "# plot_rl(ag_opt,yplot=\"Average\")\n",
        "# plot_rl(ag_exp_l,yplot=\"Average\")\n",
        "# plot_rl(ag_exp_m,yplot=\"Average\")\n",
        "# plot_rl(ag_disc,yplot=\"Average\")\n",
        "# plot_rl(ag_va,yplot=\"Average\")\n",
        "from mdpExamples import MDPtiny\n",
        "from rlProblem import Env_from_MDP\n",
        "envt = Env_from_MDP(MDPtiny())\n",
        "agt = Q_learner(envt, 0.8)\n",
        "# agt.do(20)\n",
        "from rlSimpleEnv import Simple_game_env\n",
        "senv = Simple_game_env()\n",
        "sag1 = Q_learner(senv,0.9,explore=0.2,fixed_alpha=True,alpha=0.1)\n",
        "# plot_rl(sag1,steps_explore=100000,steps_exploit=100000,label=\"alpha=\"+str(sag1.alpha))\n",
        "sag2 = Q_learner(senv,0.9,explore=0.2,fixed_alpha=False)\n",
        "# plot_rl(sag2,steps_explore=100000,steps_exploit=100000,label=\"alpha=1/k\")\n",
        "sag3 = Q_learner(senv,0.9,explore=0.2,fixed_alpha=False,alpha_fun=lambda\n",
        "k:10/(9+k))\n",
        "# plot_rl(sag3,steps_explore=100000,steps_exploit=100000,label=\"alpha=10/(9+k)\")\n"
      ],
      "metadata": {
        "id": "a04mi2hafjr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.3 Q-leaning with Experience Replay\n"
      ],
      "metadata": {
        "id": "oxZmq5jVfjv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##rlQExperienceReplay.py"
      ],
      "metadata": {
        "id": "nzAt3NItfjyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rlQLearner import Q_learner\n",
        "from utilities import flip\n",
        "import random\n",
        "class BoundedBuffer(object):\n",
        "    def __init__(self, buffer_size=1000):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.buffer = [0]*buffer_size\n",
        "        self.number_added = 0\n",
        "    def add(self,experience):\n",
        "        if self.number_added < self.buffer_size:\n",
        "          self.buffer[self.number_added] = experience\n",
        "        else:\n",
        "          if flip(self.buffer_size/self.number_added):\n",
        "            position = random.randrange(self.buffer_size)\n",
        "            self.buffer[position] = experience\n",
        "        self.number_added += 1\n",
        "    def get(self):\n",
        "        return self.buffer[random.randrange(min(self.number_added, self.buffer_size))]\n",
        "\n",
        "class Q_AR_learner(Q_learner):\n",
        "  def __init__(self, env, discount, explore=0.1, fixed_alpha=True,alpha=0.2, alpha_fun=lambda k:1/k, qinit=0, label=\"Q_AR_learner\",\n",
        "    max_buffer_size=5000,\n",
        "    num_updates_per_action=5, burn_in=1000 ):\n",
        "    Q_learner.__init__(self, env, discount, explore, fixed_alpha, alpha, alpha_fun, qinit, label)\n",
        "    self.experience_buffer = BoundedBuffer(max_buffer_size)\n",
        "    self.num_updates_per_action = num_updates_per_action\n",
        "    self.burn_in = burn_in\n",
        "\n",
        "\n",
        "  def do(self,num_steps=100):\n",
        "    \"\"\"do num_steps of interaction with the environment\"\"\"\n",
        "    self.display(2,\"s\\ta\\tr\\ts'\\tQ\")\n",
        "    alpha = self.alpha\n",
        "    for i in range(num_steps):\n",
        "      action = self.select_action(self.state)\n",
        "      next_state,reward = self.env.do(action)\n",
        "      self.experience_buffer.add((self.state,action,reward,next_state)) \n",
        "      #remember experience\n",
        "    if not self.fixed_alpha:\n",
        "       k = self.visits[(self.state, action)] = self.visits.get((self.state, action),0)+1\n",
        "       alpha = self.alpha_fun(k)\n",
        "       self.q[(self.state, action)] = ( (1-alpha) * self.q.get((self.state, action),self.qinit) + alpha * (reward + self.discount* max(self.q.get((next_state,next_act),self.qinit) for next_act in self.actions)))\n",
        "       self.display(2,self.state, action, reward, next_state,\n",
        "       self.q[(self.state, action)], sep='\\t')\n",
        "       self.state = next_state\n",
        "       self.acc_rewards += reward\n",
        "    # do some updates from experince buffer\n",
        "    if self.experience_buffer.number_added > self.burn_in:\n",
        "      for i in range(self.num_updates_per_action):\n",
        "        (s,a,r,ns) = self.experience_buffer.get()\n",
        "        if not self.fixed_alpha:\n",
        "          k = self.visits[(s,a)]\n",
        "          alpha = self.alpha_fun(k)\n",
        "          self.q[(s,a)] = (\n",
        "          (1-alpha) * self.q[(s,a)]\n",
        "          + alpha * (reward + self.discount\n",
        "          * max(self.q.get((ns,na),self.qinit)\n",
        "          for na in self.actions)))"
      ],
      "metadata": {
        "id": "QxegcNtIiXWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rlSimpleEnv import Simple_game_env\n",
        "from rlQTest import sag1, sag2, sag3\n",
        "from rlPlot import plot_rl\n",
        "senv = Simple_game_env()\n",
        "sag1ar = Q_AR_learner(senv,0.9,explore=0.2,fixed_alpha=True,alpha=0.1)\n",
        "# plot_rl(sag1ar,steps_explore=100000,steps_exploit=100000,label=\"AR\n",
        "alpha=\"+str(sag1ar.alpha))\n",
        "sag2ar = Q_AR_learner(senv,0.9,explore=0.2,fixed_alpha=False)\n",
        "# plot_rl(sag2ar,steps_explore=100000,steps_exploit=100000,label=\"AR\n",
        "alpha=1/k\")\n",
        "sag3ar =Q_AR_learner(senv,0.9,explore=0.2,fixed_alpha=False,alpha_fun=lambdak:10/(9+k))\n",
        "# plot_rl(sag3ar,step"
      ],
      "metadata": {
        "id": "BLt8pO1ziXaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.4 Model-based Reinforcement Learner"
      ],
      "metadata": {
        "id": "_bOz16G9iXdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A model-based reinforcement learner builds a Markov decision process model\n",
        "of the domain, simultaneously learns the model and plans with that model."
      ],
      "metadata": {
        "id": "Qwv6tO20iXgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###rlModelLearner.py —"
      ],
      "metadata": {
        "id": "0dT9E4wZiXiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from rlQLearner import RL_agent\n",
        "from display import Displayable\n",
        "from utilities import argmaxe, flip\n",
        "class Model_based_reinforcement_learner(RL_agent):\n",
        "    \"\"\"A Model-based reinforcement learner\n",
        "    \"\"\"\n",
        "    def __init__(self, env, discount, explore=0.1, qinit=0,\n",
        "      updates_per_step=10, label=\"MBR_learner\"):\n",
        "      \"\"\"env is the environment to interact with.\n",
        "      discount is the discount factor\n",
        "      explore is the proportion of time the agent will explore\n",
        "      qinit is the initial value of the Q's\n",
        "      updates_per_step is the number of AVI updates per action\n",
        "      label is the label for plotting\n",
        "      \"\"\"\n",
        "      RL_agent.__init__(self)\n",
        "      self.env = env\n",
        "      self.actions = env.actions\n",
        "      self.discount = discount\n",
        "      self.explore = explore\n",
        "      self.qinit = qinit\n",
        "      self.updates_per_step = updates_per_step\n",
        "      self.label = label\n",
        "      self.restart()\n",
        "\n",
        "    def restart(self):\n",
        "      \"\"\"make the agent relearn, and reset the accumulated rewards\n",
        "      \"\"\"\n",
        "      self.acc_rewards = 0\n",
        "      self.state = self.env.state\n",
        "      self.q = {} # {(st,action):q_value} map\n",
        "      self.r = {} # {(st,action):reward} map\n",
        "      self.t = {} # {(st,action,st_next):count} map\n",
        "      self.visits = {} # {(st,action):count} map\n",
        "      self.res_states = {} # {(st,action):set_of_states} map\n",
        "      self.visits_list = [] # list of (st,action)\n",
        "      self.previous_action = None\n",
        "\n",
        "    def do(self,num_steps=100):\n",
        "        \"\"\"do num_steps of interaction with the environment\"\"\"\n",
        "        self.display(2,\"s\\ta\\tr\\ts'\\tQ\")\n",
        "        alpha = self.alpha\n",
        "        for i in range(num_steps):\n",
        "          action = self.select_action(self.state)\n",
        "          next_state,reward = self.env.do(action)\n",
        "          self.experience_buffer.add((self.state,action,reward,next_state))\n",
        "          #remember experience\n",
        "          if not self.fixed_alpha:\n",
        "              k = self.visits[(self.state, action)] = self.visits.get((self.state, action),0)+1\n",
        "              alpha = self.alpha_fun(k)\n",
        "              self.q[(self.state, action)] = ( (1-alpha) * self.q.get((self.state, action),self.qinit)+ alpha * (reward + self.discount* max(self.q.get((next_state, next_act),self.qinit)\n",
        "              for next_act in self.actions)))\n",
        "                self.display(2,self.state, action, reward, next_state,\n",
        "                self.q[(self.state, action)], sep='\\t')\n",
        "                self.state = next_state\n",
        "                self.acc_rewards += reward\n",
        "          # do some updates from experince buffer\n",
        "          if self.experience_buffer.number_added > self.burn_in:\n",
        "            for i in range(self.num_updates_per_action):\n",
        "                (s,a,r,ns) = self.experience_buffer.get()\n",
        "                if not self.fixed_alpha:\n",
        "                  k = self.visits[(s,a)]\n",
        "                  alpha = self.alpha_fun(k)\n",
        "                  self.q[(s,a)] = ( (1-alpha) * self.q[(s,a)]+ alpha * (reward + self.discount* max(self.q.get((ns,na),self.qinit)for na in self.actions)))"
      ],
      "metadata": {
        "id": "L4fzgcRAiXlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rlQTest import senv # simple game environment\n",
        "mbl1 = Model_based_reinforcement_learner(senv,0.9,updates_per_step=10)\n",
        "#plot_rl(mbl1,steps_explore=100000,steps_exploit=100000,label=\"model-based(10)\")\n",
        "mbl2 = Model_based_reinforcement_learner(senv,0.9,updates_per_step=1)\n",
        "#plot_rl(mbl2,steps_explore=100000,steps_exploit=100000,label=\"model-based(1)\")"
      ],
      "metadata": {
        "id": "qxtZ4FvqiXnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_rl(sag1ar,steps_explore=100000,steps_exploit=100000,label=\"AR\n",
        "alpha=\"+str(sag1ar.alpha))\n",
        "sag2ar = Q_AR_learner(senv,0.9,explore=0.2,fixed_alpha=False)\n",
        "# plot_rl(sag2ar,steps_explore=100000,steps_exploit=100000,label=\"AR alpha=1/k\")\n",
        "sag3ar =Q_AR_learner(senv,0.9,explore=0.2,fixed_alpha=False,alpha_fun=lambda k:10/(9+k))\n",
        "# plot_rl(sag3ar,steps_explore=100000,steps_exploit=100000,label=\"AR alpha=10/(9+k)\")\n"
      ],
      "metadata": {
        "id": "bgAE4xnViXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.4 Model-based Reinforcement Learner"
      ],
      "metadata": {
        "id": "TJGIXQY8ii8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from rlQLearner import RL_agent\n",
        "from display import Displayable\n",
        "from utilities import argmaxe, flip\n",
        "class Model_based_reinforcement_learner(RL_agent):\n",
        "    \"\"\"A Model-based reinforcement learner\n",
        "    \"\"\"\n",
        "    def __init__(self, env, discount, explore=0.1, qinit=0,\n",
        "        updates_per_step=10, label=\"MBR_learner\"):\n",
        "        \"\"\"env is the environment to interact with.\n",
        "        discount is the discount factor\n",
        "        explore is the proportion of time the agent will explore\n",
        "        qinit is the initial value of the Q's\n",
        "        updates_per_step is the number of AVI updates per action\n",
        "        label is the label for plotting\n",
        "        \"\"\"\n",
        "        RL_agent.__init__(self)\n",
        "        self.env = env\n",
        "        self.actions = env.actions\n",
        "        self.discount = discount\n",
        "        self.explore = explore\n",
        "        self.qinit = qinit\n",
        "        self.updates_per_step = updates_per_step\n",
        "        self.label = label\n",
        "        self.restart()\n",
        "      def restart(self):\n",
        "          \"\"\"make the agent relearn, and reset the accumulated rewards\n",
        "          \"\"\"\n",
        "          self.acc_rewards = 0\n",
        "          self.state = self.env.state\n",
        "          self.q = {} # {(st,action):q_value} map\n",
        "          self.r = {} # {(st,action):reward} map\n",
        "          self.t = {} # {(st,action,st_next):count} map\n",
        "          self.visits = {} # {(st,action):count} map\n",
        "          self.res_states = {} # {(st,action):set_of_states} map\n",
        "          self.visits_list =  [] # list of (st,action)\n",
        "          self.previous_action = None\n",
        "      def do(self,num_steps=100):\n",
        "          \"\"\"do num_steps of interaction with the environment\n",
        "          for each action, do updates_per_step iterations of asynchronous\n",
        "          value iteration\n",
        "          \"\"\"\n",
        "          for step in range(num_steps):\n",
        "              pst = self.state # previous state\n",
        "              action = self.select_action(pst)\n",
        "              self.state,reward = self.env.do(action)\n",
        "              self.acc_rewards += reward\n",
        "              self.t[(pst,action,self.state)] = self.t.get((pst,\n",
        "              action,self.state),0)+1\n",
        "              if (pst,action) in self.visits:\n",
        "                  self.visits[(pst,action)] += 1\n",
        "                  self.r[(pst,action)] +=\n",
        "                  (reward-self.r[(pst,action)])/self.visits[(pst,action)]\n",
        "                  self.res_states[(pst,action)].add(self.state)\n",
        "              else:\n",
        "                  self.visits[(pst,action)] = 1\n",
        "                  self.r[(pst,action)] = reward\n",
        "                  self.res_states[(pst,action)] = {self.state}\n",
        "                  self.visits_list.append((pst,action))\n",
        "              st,act = pst,action #initial state-action pair for AVI\n",
        "              for update in range(self.updates_per_step):\n",
        "                    self.q[(st,act)] = self.r[(st,act)]+self.discount*( sum(self.t[st,act,rst]/self.visits[st,act]* max(self.q.get((rst,nact),self.qinit) for nact in self.actions)\n",
        "                          for rst in self.res_states[(st,act)]))\n",
        "                          st,act = random.choice(self.visits_list)\n",
        "      def select_action(self, state):\n",
        "          \"\"\"returns an action to carry out for the current agent\n",
        "          given the state, and the q-function\n",
        "          \"\"\"\n",
        "          if flip(self.explore):\n",
        "              return random.choice(self.actions)\n",
        "          else:\n",
        "              return argmaxe((next_act, self.q.get((state, next_act),self.qinit)) for next_act in self.actions)\n",
        "\n",
        "from rlQTest import senv # simple game environment\n",
        " mbl1 = Model_based_reinforcement_learner(senv,0.9,updates_per_step=10)\n",
        "#plot_rl(mbl1,steps_explore=100000,steps_exploit=100000,label=\"model-based(10)\")\n",
        " mbl2 = Model_based_reinforcement_learner(senv,0.9,updates_per_step=1)\n",
        "# plot_rl(mbl2,steps_explore=100000,steps_exploit=100000,label=\"model-based(1)\")\n"
      ],
      "metadata": {
        "id": "XZ8ogQArnrhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.5 Reinforcement Learning with Features\n"
      ],
      "metadata": {
        "id": "rvEUV3BRnrlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.5.1 Representing Features\n",
        "A feature is a function from state and action. To construct the features for a\n",
        "domain, we construct a function that takes a state and an action and returns the\n",
        "list of all feature values for that state and action. This feature set is redesigned\n",
        "for each problem."
      ],
      "metadata": {
        "id": "9fBVZq6xnroT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###rlSimpleGameFeatures.py"
      ],
      "metadata": {
        "id": "BEY3AyM7nrqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rlSimpleEnv import Simple_game_env\n",
        "from rlProblem import RL_env\n",
        "def get_features(state,action):\n",
        "    \"\"\"returns the list of feature values for the state-action pair\n",
        "    \"\"\"\n",
        "    assert action in Simple_game_env.actions\n",
        "    (x,y,d,p) = state\n",
        "    # f1: would go to a monster\n",
        "    f1 = monster_ahead(x,y,action)\n",
        "    # f2: would crash into wall\n",
        "    f2 = wall_ahead(x,y,action)\n",
        "    # f3: action is towards a prize\n",
        "    f3 = towards_prize(x,y,action,p)\n",
        "    # f4: damaged and action is toward repair station\n",
        "    f4 = towards_repair(x,y,action) if d else 0\n",
        "    # f5: damaged and towards monster\n",
        "    f5 = 1 if d and f1 else 0\n",
        "    # f6: damaged\n",
        "    f6 = 1 if d else 0\n",
        "    # f7: not damaged  \n",
        "    f7 = 1-f6\n",
        "    # f8: damaged and prize ahead\n",
        "    f8 = 1 if d and f3 else 0\n",
        "    # f9: not damaged and prize ahead\n",
        "    f9 = 1 if not d and f3 else 0\n",
        "    features = [1,f1,f2,f3,f4,f5,f6,f7,f8,f9]\n",
        "    # the next 20 features are for 5 prize locations\n",
        "    # and 4 distances from outside in all directions\n",
        "    for pr in Simple_game_env.prize_locs+[None]:\n",
        "    if p==pr:\n",
        "      features += [x, 4-x, y, 4-y]\n",
        "    else:\n",
        "      features += [0, 0, 0, 0]\n",
        "    # fp04 feature for y when prize is at 0,4\n",
        "    # this knows about the wall to the right of the prize\n",
        "    if p==(0,4):\n",
        "      if x==0:\n",
        "        fp04 = y\n",
        "      elif y<3:\n",
        "        fp04 = y\n",
        "      else:\n",
        "          fp04 = 4-y\n",
        "    else:\n",
        "      fp04 = 0\n",
        "      features.append(fp04)\n",
        "  return features\n",
        "\n",
        "def monster_ahead(x,y,action):\n",
        "    \"\"\"returns 1 if the location expected to get to by doing\n",
        "    action from (x,y) can contain a monster.\n",
        "    \"\"\"\n",
        "    if action == \"right\" and (x+1,y) in Simple_game_env.monster_locs:\n",
        "      return 1\n",
        "    elif action == \"left\" and (x-1,y) in Simple_game_env.monster_locs:\n",
        "      return 1\n",
        "    elif action == \"up\" and (x,y+1) in Simple_game_env.monster_locs:\n",
        "      return 1\n",
        "    elif action == \"down\" and (x,y-1) in Simple_game_env.monster_locs:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        " def wall_ahead(x,y,action):\n",
        "    \"\"\"returns 1 if there is a wall in the direction of action from (x,y).\n",
        "    This is complicated by the internal walls.\n",
        "    \"\"\"\n",
        "    if action == \"right\" and (x==Simple_game_env.xdim-1 or (x,y) in   Simple_game_env.vwalls):\n",
        "        return 1\n",
        "    elif action == \"left\" and (x==0 or (x-1,y) in Simple_game_env.vwalls):\n",
        "\n",
        "        return 1\n",
        "    elif action == \"up\" and y==Simple_game_env.ydim-1:\n",
        "        return 1\n",
        "    elif action == \"down\" and y==0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        " def towards_prize(x,y,action,p):\n",
        "    \"\"\"action goes in the direction of the prize from (x,y)\"\"\"\n",
        "    if p is None:\n",
        "      return 0\n",
        "    elif p==(0,4): # take into account the wall near the top-left prize\n",
        "      if action == \"left\" and (x>1 or x==1 and y<3):\n",
        "        return 1\n",
        "      elif action == \"down\" and (x>0 and y>2):\n",
        "        return 1\n",
        "      elif action == \"up\" and (x==0 or y<2):\n",
        "        return 1\n",
        "      else:\n",
        "        return 0\n",
        "    else:\n",
        "      px,py = p\n",
        "      if p==(4,4) and x==0:\n",
        "        if (action==\"right\" and y<3) or (action==\"down\" and y>2) or  (action==\"up\" and y<2):\n",
        "          return 1\n",
        "        else:\n",
        "          return 0\n",
        "        if (action == \"up\" and y<py) or (action == \"down\" and py<y):\n",
        "          return 1\n",
        "        elif (action == \"left\" and px<x) or (action == \"right\" and x<px):\n",
        "          return 1\n",
        "      else:\n",
        "        return 0\n",
        "def towards_repair(x,y,action):\n",
        "    \"\"\"returns 1 if action is towards the repair station.\n",
        "    \"\"\"\n",
        "    if action == \"up\" and (x>0 and y<4 or x==0 and y<2):\n",
        "      return 1\n",
        "    elif action == \"left\" and x>1:\n",
        "      return 1\n",
        "    elif action == \"right\" and x==0 and y<3:\n",
        "      return 1\n",
        "    elif action == \"down\" and x==0 and y>2:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n"
      ],
      "metadata": {
        "id": "ipxsg6pdnrtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simp_features(state,action):\n",
        "  \"\"\"returns a list of feature values for the state-action pair\n",
        "  \"\"\"\n",
        "  assert action in Simple_game_env.actions\n",
        "  (x,y,d,p) = state\n",
        "  # f1: would go to a monster\n",
        "  f1 = monster_ahead(x,y,action)\n",
        "  # f2: would crash into wall\n",
        "  f2 = wall_ahead(x,y,action)\n",
        "  # f3: action is towards a prize\n",
        "  f3 = towards_prize(x,y,action,p)\n",
        "  return [1,f1,f2,f3]\n"
      ],
      "metadata": {
        "id": "_Pw6VjZHnrvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.5.2 Feature-based RL learner"
      ],
      "metadata": {
        "id": "60WP3rrrnryR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This learns a linear function approximation of the Q-values. It requires the\n",
        "function get features that given a state and an action returns a list of values for\n",
        "all of the features. Each environment requires this function to be provided."
      ],
      "metadata": {
        "id": "P1PUnglSnr2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from rlQLearner import RL_agent\n",
        "from display import Displayable\n",
        "from utilities import argmaxe, flip\n",
        "class SARSA_LFA_learner(RL_agent):\n",
        "      \"\"\"A SARSA_LFA learning agent has\n",
        "      belief-state consisting of\n",
        "      state is the previous state\n",
        "      q is a {(state,action):value} dict\n",
        "      visits is a {(state,action):n} dict. n is how many times action was done in state\n",
        "      acc_rewards is the accumulated reward\n",
        "      it observes (s, r) for some world-state s and real reward r\n",
        "      \"\"\"\n",
        "    def __init__(self, env, get_features, discount, explore=0.2, step_size=0.01,winit=0, label=\"SARSA_LFA\"):\n",
        "          \"\"\"env is the feature environment to interact with\n",
        "          get_features is a function get_features(state,action) that returns the list of feature values\n",
        "          discount is the discount factor\n",
        "          explore is the proportion of time the agent will explore\n",
        "          step_size is gradient descent step size\n",
        "          winit is the initial value of the weights\n",
        "          label is the label for plotting\n",
        "          \"\"\"\n",
        "        RL_agent.__init__(self)\n",
        "        self.env = env\n",
        "        self.get_features = get_features\n",
        "        self.actions = env.actions\n",
        "        self.discount = discount\n",
        "        self.explore = explore\n",
        "        self.step_size = step_size\n",
        "        self.winit = winit\n",
        "        self.label = label\n",
        "        self.restart()\n",
        "    def restart(self):\n",
        "        \"\"\"make the agent relearn, and reset the accumulated rewards\n",
        "        \"\"\"\n",
        "        self.acc_rewards = 0\n",
        "        self.state = self.env.state\n",
        "        self.features = self.get_features(self.state, list(self.env.actions)[0])\n",
        "        self.weights = [self.winit for f in self.features]\n",
        "        self.action = self.select_action(self.state)\n",
        "\n",
        "    def do(self,num_steps=100):\n",
        "        \"\"\"do num_steps of interaction with the environment\"\"\"\n",
        "        self.display(2,\"s\\ta\\tr\\ts'\\tQ\\tdelta\")\n",
        "        for i in range(num_steps):\n",
        "            next_state,reward = self.env.do(self.action)\n",
        "            self.acc_rewards += reward\n",
        "            next_action = self.select_action(next_state)\n",
        "            feature_values = self.get_features(self.state,self.action)\n",
        "            oldQ = dot_product(self.weights, feature_values)\n",
        "            nextQ = dot_product(self.weights, self.get_features(next_state,next_action))\n",
        "            delta = reward + self.discount * nextQ - oldQ\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] += self.step_size * delta * feature_values[i]\n",
        "            self.display(2,self.state, self.action, reward, next_state,\n",
        "            dot_product(self.weights, feature_values), delta, sep='\\t')\n",
        "            self.state = next_state\n",
        "            self.action = next_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"returns an action to carry out for the current agent\n",
        "        given the state, and the q-function.\n",
        "        This implements an epsilon-greedy approach\n",
        "        where self.explore is the probability of exploring.\"\"\"\n",
        "\n",
        "        if flip(self.explore):\n",
        "            return random.choice(self.actions)\n",
        "        else:\n",
        "            return argmaxe((next_act, dot_product(self.weights, self.get_features(state,next_act))) for next_act in self.actions)\n",
        "\n",
        "    def show_actions(self,state=None):\n",
        "      \"\"\"prints the value for each action in a state.\n",
        "      This may be useful for debugging.\n",
        "      \"\"\"\n",
        "      if state is None:\n",
        "        state = self.state\n",
        "      for next_act in self.actions:\n",
        "        print(next_act,dot_product(self.weights, self.get_features(state,next_act)))\n",
        "\n",
        "    def dot_product(l1,l2):\n",
        "        return sum(e1*e2 for (e1,e2) in zip(l1,l2))"
      ],
      "metadata": {
        "id": "Jn_6T-Hwnr6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "zfukyr0Rh1sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test code:\n"
      ],
      "metadata": {
        "id": "7d_GVTTbh2pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rlQTest import senv # simple game environment\n",
        "from rlSimpleGameFeatures import get_features, simp_features\n",
        "from rlPlot import plot_rl\n",
        "fa1 = SARSA_LFA_learner(senv, get_features, 0.9, step_size=0.01)\n",
        "#fa1.max_display_level = 2\n",
        "#fa1.do(20)\n",
        "#plot_rl(fa1,steps_explore=10000,steps_exploit=10000,label=\"SARSA_LFA(0.01)\")\n",
        "fas1 = SARSA_LFA_learner(senv, simp_features, 0.9, step_size=0.01)\n",
        "#plot_rl(fas1,steps_explore=10000,steps_exploit=10000,label=\"SARSA_LFA(simp)\")"
      ],
      "metadata": {
        "id": "h4L3c5Mih2sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.5.3 Experience Replay\n"
      ],
      "metadata": {
        "id": "We7Xy2OWh2vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rlFeatures import SARSA_LFA_learner, dot_product\n",
        "from utilities import flip\n",
        "import random\n",
        "class SARSA_LFA_AR_learner(SARSA_LFA_learner):\n",
        "    def __init__(self, env, get_features, discount, explore=0.2,step_size=0.01,winit=0, label=\"SARSA_LFA-AR\", max_buffer_size=500,num_updates_per_action=5, burn_in=100 ):\n",
        "        SARSA_LFA_learner.__init__(self, env, get_features, discount,explore, step_size,winit, label)\n",
        "        self. max_buffer_size = max_buffer_size\n",
        "        self.action_buffer = [0]*max_buffer_size\n",
        "        self.number_added = 0\n",
        "        self.num_updates_per_action = num_updates_per_action\n",
        "        self.burn_in = burn_in\n",
        "        def add_to_buffer(self,experience):\n",
        "        if self.number_added < self.max_buffer_size:\n",
        "            self.action_buffer[self.number_added] = experience\n",
        "        else:\n",
        "          if flip(self.max_buffer_size/self.number_added):\n",
        "            position = random.randrange(self.max_buffer_size)\n",
        "            self.action_buffer[position] = experience\n",
        "        self.number_added += 1\n",
        "\n",
        "    def do(self,num_steps=100):\n",
        "      \"\"\"do num_steps of interaction with the environment\"\"\"\n",
        "      self.display(2,\"s\\ta\\tr\\ts'\\tQ\\tdelta\")\n",
        "      for i in range(num_steps): \n",
        "        next_state,reward = self.env.do(self.action)\n",
        "        self.add_to_buffer((self.state,self.action,reward,next_state)) #remember experience\n",
        "        self.acc_rewards += reward\n",
        "        next_action = self.select_action(next_state)\n",
        "        feature_values = self.get_features(self.state,self.action)\n",
        "        oldQ = dot_product(self.weights, feature_values)\n",
        "        nextQ = dot_product(self.weights, self.get_features(next_state,next_action))\n",
        "        delta = reward + self.discount * nextQ - oldQ\n",
        "      for i in range(len(self.weights)):\n",
        "      self.weights[i] += self.step_size * delta * feature_values[i]\n",
        "      self.display(2,self.state, self.action, reward, next_state,\n",
        "      dot_product(self.weights, feature_values), delta, sep='\\t')\n",
        "      self.state = next_state\n",
        "      self.action = next_action\n",
        "      if self.number_added > self.burn_in:\n",
        "      for i in range(self.num_updates_per_action):\n",
        "      (s,a,r,ns) =\n",
        "      self.action_buffer[random.randrange(min(self.number_added,\n",
        "      self.max_buffer_size))]\n",
        "      na = self.select_action(ns)\n",
        "      feature_values = self.get_features(s,a)\n",
        "      oldQ = dot_product(self.weights, feature_values)\n",
        "      nextQ = dot_product(self.weights, self.get_features(ns,na))\n",
        "      delta = reward + self.discount * nextQ - oldQ\n",
        "      for i in range(len(self.weights)):\n",
        "      self.weights[i] += self.step_size * delta *\n",
        "      feature_values[i]\n",
        "\n",
        "from rlQTest import senv # simple game environment\n",
        "from rlSimpleGameFeatures import get_features, simp_features\n",
        "from rlPlot import plot_rl\n",
        "\n",
        "fa1 = SARSA_LFA_AR_learner(senv, get_features, 0.9, step_size=0.01)\n",
        "#fa1.max_display_level = 2\n",
        "#fa1.do(20)\n",
        "#plot_rl(fa1,steps_explore=10000,steps_exploit=10000,label=\"SARSA_LFA_AR(0.01)\")\n",
        "fas1 = SARSA_LFA_AR_learner(senv, simp_features, 0.9, step_size=0.01)\n",
        "#plot_rl(fas1,steps_explore=10000,steps_exploit=10000,label=\"SARSA_LFA_AR(simp)\")\n"
      ],
      "metadata": {
        "id": "93JkEkp1h2yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.6 Multiagent Learning"
      ],
      "metadata": {
        "id": "fBs2JVrMh20w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###masLearn.py"
      ],
      "metadata": {
        "id": "k_F7Lqtqh23T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from display import Displayable\n",
        "import utilities # argmaxall for (element,value) pairs\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "class GameAgent(Displayable):\n",
        "    next_id=0\n",
        "    def __init__(self, actions):\n",
        "      \"\"\"\n",
        "      Actions is the set of actions the agent can do. It needs to be told that!\n",
        "      \"\"\"\n",
        "      self.actions = actions\n",
        "      self.id = GameAgent.next_id\n",
        "      GameAgent.next_id += 1\n",
        "      self.display(2,f\"Agent {self.id} has actions {actions}\")\n",
        "      self.dist = {act:1 for act in actions} # unnormalized distibution\n",
        "      self.total_score = 0\n",
        "    def init_action(self):\n",
        "      \"\"\" The initial action.\n",
        "      Act randomly initially\n",
        "      Could be overridden (but I'm not sure why you would).\n",
        "      \"\"\"\n",
        "      self.act = random.choice(self.actions)\n",
        "      return self.act\n",
        "    def select_action(self, reward):\n",
        "    \"\"\"\n",
        "    Select the action given the reward.\n",
        "    This implements \"Act randomly\" and should be overridden!\n",
        "    \"\"\"\n",
        "    self.total_score += reward\n",
        "    self.act = random.choice(self.actions)\n",
        "    return self.act\n"
      ],
      "metadata": {
        "id": "oSUzSAwVh26t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##masLearn.py"
      ],
      "metadata": {
        "id": "G3B08s4Ch1vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCountingAgent(GameAgent):\n",
        "  \"\"\"This agent just counts the number of times (it thinks) it has won and does the\n",
        "  actions it thinks is most likely to win.\n",
        "  \"\"\"   \n",
        "  def __init__(self, actions, prior_count=1):\n",
        "    \"\"\"\n",
        "    Actions is the set of actions the agent can do. It needs to be toldthat!\n",
        "    \"\"\"\n",
        "    GameAgent.__init__(self, actions)\n",
        "    self.prior_count = prior_count\n",
        "    self.dist = {a: prior_count for a in self.actions} # unnormalized distibution\n",
        "    self.averew = 0\n",
        "    self.num_steps = 0\n",
        "\n",
        "  def select_action(self, reward):\n",
        "    self.total_score += reward\n",
        "    self.num_steps += 1\n",
        "    self.display(2,f\"The reward for agent {self.id} was {reward}\")\n",
        "    self.averew = self.averew+(reward-self.averew)/self.num_steps\n",
        "    if reward>self.averew:\n",
        "        self.dist[self.act] += 1\n",
        "    else:\n",
        "        for otheract in self.actions:\n",
        "        if otheract != self.act:\n",
        "          self.dist[otheract] += 1/(len(self.actions))\n",
        "          self.display(2,f\"Distribution for agent {self.id} is {normalize(self.dist)}\")\n",
        "          self.act = select_from_dist(self.dist)\n",
        "          self.display(2,f\"Agent {self.id} did {self.act}\")\n",
        "        return self.act\n",
        "\n",
        "class SimpleQAgent(GameAgent):\n",
        "    \"\"\"This agent maintains the Q-function for each state.\n",
        "    (Or just the average reward as the future state is all the same).\n",
        "    Chooses the best action using\n",
        "    \"\"\"\n",
        "    def __init__(self, actions, q_init=100, alpha=0.1, prob_step_size=0.001, min_prob=0.01):\n",
        "        \"\"\"\n",
        "        Actions is the set of actions the agent can do. It needs to be told that!\n",
        "        q_init is the initial q-values\n",
        "        alpha is the step size for action estimate\n",
        "        prob_step_size is the step size for probability change\n",
        "        min_prob is the minimum a probability should become\n",
        "        \"\"\"\n",
        "        GameAgent.__init__(self, actions)\n",
        "        self.Q = {a:q_init for a in self.actions}\n",
        "        self.dist = normalize({a:0.7+random.random() for a in\n",
        "        self.actions}) # start with random dist but not too close to\n",
        "        zero\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.prob_step_size = prob_step_size\n",
        "        self.min_prob = min_prob\n",
        "        self.num_steps = 1 # (1 because it isonly used after initial step)\n",
        "\n",
        "    def select_action(self, reward):\n",
        "      self.total_score += reward\n",
        "      self.display(2,f\"The reward for agent {self.id} was {reward}\")\n",
        "      self.Q[self.act] += self.alpha*(reward-self.Q[self.act])\n",
        "      a_best = utilities.argmaxall(self.Q.items())\n",
        "      for a in self.actions:\n",
        "        if a in a_best:\n",
        "            self.dist[a] += self.prob_step_size\n",
        "        else:\n",
        "            self.dist[a] -= min(self.dist[a], self.prob_step_size)\n",
        "            self.dist[a] = max(self.dist[a],self.min_prob)\n",
        "            self.dist = normalize(self.dist)\n",
        "            self.display(2,f\"Distribution for agent {self.id} is {self.dist}\")\n",
        "            self.act = select_from_dist(self.dist)\n",
        "            self.display(2,f\"Agent {self.id} did {self.act}\")\n",
        "      return self.act\n",
        "\n",
        "    def normalize(dist):\n",
        "      \"\"\"unnorm dict is a {value:number} dictionary, where the numbers are all non-negative\n",
        "      returns dict where the numbers sum to one\n",
        "      \"\"\"\n",
        "      tot = sum(dist.values())\n",
        "      return {var:val/tot for (var,val) in dist.items()}\n",
        "\n",
        "    def select_from_dist(dist):\n",
        "        rand = random.random()\n",
        "        for (act,prob) in normalize(dist).items():\n",
        "          rand -= prob\n",
        "          if rand < 0:\n",
        "            return act\n",
        "\n"
      ],
      "metadata": {
        "id": "UQVBwECUh1x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimulateGame(Displayable):\n",
        "  def __init__(self, game, agents):\n",
        "      self.game = game\n",
        "      self.agents = agents # list of agents\n",
        "      self.action_history = []\n",
        "      self.reward_history = []\n",
        "      self.dist_history = []\n",
        "      self.actions = tuple(ag.init_action() for ag in self.agents)\n",
        "      self.num_steps = 0\n",
        "\n",
        "  def go(self, steps): \n",
        "      for i in range(steps):\n",
        "      self.num_steps += 1\n",
        "      self.rewards = self.game.play(self.actions)\n",
        "      self.reward_history.append(self.rewards)\n",
        "      self.actions =tuple(self.agents[i].select_action(self.rewards[i]) for i in range(self.game.num_agents))\n",
        "      self.action_history.append(self.actions)\n",
        "      self.dist_history.append([normalize(ag.dist) for ag in self.agents])\n",
        "      print(\"Scores:\", ' '.join(f\"Agent {ag.id} average reward={ag.total_score/self.num_steps}\" for ag in self.agents))\n",
        "      #return self.reward_history, self.action_history\n",
        "\n",
        "  def action_dist(self,which_actions=[1,1]):\n",
        "      \"\"\" which actions is [a0,a1]\n",
        "      returns the empirical disctribition of actions for agents,\n",
        "      where ai specifies the index of the actions for agent i\n",
        "      \"\"\"\n",
        "      return [sum(1 for a in sim.action_history if a[i]==gm.actions[i][which_actions[i]])/len(sim.action_history) for i in range(2)]\n",
        "\n",
        "  def plot_dynamics(self, x_action=0, y_action=0):\n",
        "    plt.ion() # make it interactive\n",
        "    agents = self.agents\n",
        "    x_act = self.game.actions[0][x_action]\n",
        "    y_act = self.game.actions[1][y_action]\n",
        "    plt.xlabel(f\"Action {self.agents[0].actions[x_action]} for Agent {agents[0].id}\")\n",
        "    plt.ylabel(f\"Action {self.agents[1].actions[y_action]} for Agent {agents[1].id}\")\n",
        "    plt.plot([self.dist_history[t][0][x_act] for t in range(len(self.dist_history))], [self.dist_history[t][1][y_act] for t in range(len(self.dist_history))])\n",
        "    #plt.legend()\n"
      ],
      "metadata": {
        "id": "KEl8UumLh11M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###masLearn.py"
      ],
      "metadata": {
        "id": "94MCHYRch13x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShoppingGame(Displayable):\n",
        "  def __init__(self):\n",
        "    self.num_agents = 2\n",
        "    self.actions = [['shopping', 'football']]*2\n",
        "\n",
        "  def play(self, actions):\n",
        "      return {('football', 'football'): (2,1),('football', 'shopping'): (0,0),('shopping', 'football'): (0,0), ('shopping', 'shopping'): (1,2)}[actions]\n",
        "\n",
        "\n",
        "class SoccerGame(Displayable):\n",
        "  def __init__(self):\n",
        "    self.num_agents = 2\n",
        "    self.actions = [['left', 'right']]*2\n",
        "\n",
        "  def play(self, actions):\n",
        "    return {('left', 'left'): (0.6, 0.4),\n",
        "    ('left', 'right'): (0.2, 0.8),\n",
        "    ('right', 'left'): (0.3, 0.7),\n",
        "    ('right', 'right'): (0.9,0.1)\n",
        "    }[actions]\n",
        "\n",
        "class GameShow(Displayable):\n",
        "  def __init__(self):\n",
        "    self.num_agents = 2\n",
        "    self.actions = [['take', 'give']]*2\n",
        "\n",
        "  def play(self, actions):\n",
        "    return {('take', 'take'): (100, 100),\n",
        "    ('take', 'give'): (1100, 0),\n",
        "    ('give', 'take'): (0, 1100),\n",
        "    ('give', 'give'): (1000,1000)\n",
        "    }[actions]\n",
        "\n",
        "class UniqueNEGameExample(Displayable):\n",
        "  def __init__(self):\n",
        "    self.num_agents = 2\n",
        "    self.actions = [['a1', 'b1', 'c1'],['d2', 'e2', 'f2']]\n",
        "    def play(self, actions):\n",
        "      return {('a1', 'd2'): (3, 5),\n",
        "      ('a1', 'e2'): (5, 1),\n",
        "      ('a1', 'f2'): (1, 2),\n",
        "      ('b1', 'd2'): (1, 1),\n",
        "      ('b1', 'e2'): (2, 9),\n",
        "      ('b1', 'f2'): (6, 4),\n",
        "      ('c1', 'd2'): (2, 6),\n",
        "      ('c1', 'e2'): (4, 7),\n",
        "      ('c1', 'f2'): (0, 8)\n",
        "      }[actions]\n",
        "# Choose one:\n",
        "# gm = ShoppingGame()\n",
        "# gm = SoccerGame()\n",
        "# gm = GameShow()\n",
        "# gm = UniqueNEGameExample()\n",
        "\n",
        "# Choose one:\n",
        "# sim=SimulateGame(gm,[SimpleQAgent(gm.actions[0]),\n",
        "SimpleQAgent(gm.actions[1])]); sim.go(10000)\n",
        "# sim= SimulateGame(gm,[SimpleCountingAgent(gm.actions[0]),\n",
        "SimpleCountingAgent(gm.actions[1])]); sim.go(10000)\n",
        "# sim=SimulateGame(gm,[SimpleCountingAgent(gm.actions[0]),\n",
        "SimpleQAgent(gm.actions[1])]); sim.go(10000)\n",
        "# sim.plot_dynamics()\n",
        "# empirical proportion that agents did their action at index 1:\n",
        "# sim.action_dist([1,1])\n",
        "# learned distribution for agent 0\n",
        "# sim.agents[0].dist\n"
      ],
      "metadata": {
        "id": "4oC4O_BojDvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter-7 Relational Learning"
      ],
      "metadata": {
        "id": "6sYHQaUUjDyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##relnCollFilt.py"
      ],
      "metadata": {
        "id": "9uld3YC3jD1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "12 import matplotlib.pyplot as plt\n",
        "13 import urllib.request\n",
        "14 from learnProblem import Learner\n",
        "15 from display import Displayable\n",
        "16\n",
        "17 class CF_learner(Learner):\n",
        "18 def __init__(self,\n",
        "19 rating_set, # a Rating_set object\n",
        "20 rating_subset = None, # subset of ratings to be used as\n",
        "training ratings\n",
        "21 test_subset = None, # subset of ratings to be used as test\n",
        "ratings\n",
        "22 step_size = 0.01, # gradient descent step size\n",
        "23 reglz = 1.0, # the weight for the regularization\n",
        "terms\n",
        "24 num_properties = 10, # number of hidden properties\n",
        "25 property_range = 0.02 # properties are initialized to be\n",
        "between\n",
        "26 # -property_range and property_range\n",
        "):\n",
        "28 self.rating_set = rating_set\n",
        "29 self.ratings = rating_subset or rating_set.training_ratings #\n",
        "whichever is not empty\n",
        "30 if test_subset is None:\n",
        "31 self.test_ratings = self.rating_set.test_ratings\n",
        "32 else:\n",
        "33 self.test_ratings = test_subset\n",
        "34 self.step_size = step_size\n",
        "35 self.reglz = reglz\n",
        "36 self.num_properties = num_properties\n",
        "37 self.num_ratings = len(self.ratings)\n",
        "38 self.ave_rating = (sum(r for (u,i,r,t) in self.ratings)\n",
        "39 /self.num_ratings)\n",
        "40 self.users = {u for (u,i,r,t) in self.ratings}\n",
        "41 self.items = {i for (u,i,r,t) in self.ratings}\n",
        "42 self.user_bias = {u:0 for u in self.users}\n",
        "43 self.item_bias = {i:0 for i in self.items}\n",
        "44 self.user_prop = {u:[random.uniform(-property_range,property_range)\n",
        "45 for p in range(num_properties)]\n",
        "46 for u in self.users}\n",
        "47 self.item_prop = {i:[random.uniform(-property_range,property_range)\n",
        "48 for p in range(num_properties)]\n",
        "49 for i in self.items}\n",
        "50 self.zeros = [0 for p in range(num_properties)]\n",
        "51 self.iter=0\n",
        "52\n",
        "53 def stats(self):\n",
        "54 self.display(1,\"ave sumsq error of mean for training=\",\n",
        "55 sum((self.ave_rating-rating)**2 for\n",
        "(user,item,rating,timestamp)\n",
        "56 in self.ratings)/len(self.ratings))\n",
        "57 self.display(1,\"ave sumsq error of mean for test=\",\n",
        "58 sum((self.ave_rating-rating)**2 for\n",
        "(user,item,rating,timestamp)\n",
        "59 in self.test_ratings)/len(self.test_ratings))\n",
        "60 self.display(1,\"error on training set\",\n",
        "61 self.evaluate(self.ratings))\n",
        "62 self.display(1,\"error on test set\",\n",
        "63 self.evaluate(self.test_ratings))\n"
      ],
      "metadata": {
        "id": "MFPai16UjD35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###relnCollFilt.py"
      ],
      "metadata": {
        "id": "hf56MIpmjD6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(self,user,item):\n",
        "66 \"\"\"Returns prediction for this user on this item.\n",
        "67 The use of .get() is to handle users or items not in the training\n",
        "set.\n",
        "68 \"\"\"\n",
        "69 return (self.ave_rating\n",
        "70 + self.user_bias.get(user,0) #self.user_bias[user]\n",
        "+ self.item_bias.get(item,0) #self.item_bias[item]\n",
        "72 +\n",
        "sum([self.user_prop.get(user,self.zeros)[p]*self.item_prop.get(item,self.zeros)73 for p in range(self.num_properties)]))\n",
        "74\n",
        "75 def learn(self, num_iter = 50):\n",
        "76 \"\"\" do num_iter iterations of gradient descent.\"\"\"\n",
        "77 for i in range(num_iter):\n",
        "78 self.iter += 1\n",
        "79 abs_error=0\n",
        "80 sumsq_error=0\n",
        "81 for (user,item,rating,timestamp) in\n",
        "random.sample(self.ratings,len(self.ratings)):\n",
        "82 error = self.prediction(user,item) - rating\n",
        "83 abs_error += abs(error)\n",
        "84 sumsq_error += error * error\n",
        "85 self.user_bias[user] -= self.step_size*error\n",
        "86 self.item_bias[item] -= self.step_size*error\n",
        "87 for p in range(self.num_properties):\n",
        "88 self.user_prop[user][p] -=\n",
        "self.step_size*error*self.item_prop[item][p]\n",
        "89 self.item_prop[item][p] -=\n",
        "self.step_size*error*self.user_prop[user][p]\n",
        "90 for user in self.users:\n",
        "91 self.user_bias[user] -= self.step_size*self.reglz*\n",
        "self.user_bias[user]\n",
        "92 for p in range(self.num_properties):\n",
        "93 self.user_prop[user][p] -=\n",
        "self.step_size*self.reglz*self.user_prop[user][p]\n",
        "94 for item in self.items:\n",
        "95 self.item_bias[item] -=\n",
        "self.step_size*self.reglz*self.item_bias[item]\n",
        "96 for p in range(self.num_properties):\n",
        "97 self.item_prop[item][p] -=\n",
        "self.step_size*self.reglz*self.item_prop[item][p]\n",
        "98 self.display(1,\"Iteration\",self.iter,\n",
        "99 \"(Ave Abs,AveSumSq) training\n",
        "=\",self.evaluate(self.ratings),\n",
        "100 \"test =\",self.evaluate(self.test_ratings))\n"
      ],
      "metadata": {
        "id": "NzbuGg1JjD9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###relnCollFilt.py —"
      ],
      "metadata": {
        "id": "YjZzldaujEBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(self,ratings):\n",
        "103 \"\"\"returns (avergage_absolute_error, average_sum_squares_error) for\n",
        "ratings\n",
        "104 \"\"\"\n",
        "105 abs_error = 0\n",
        "106 sumsq_error = 0\n",
        "107 if not ratings: return (0,0)\n",
        "108 for (user,item,rating,timestamp) in ratings:\n",
        "error = self.prediction(user,item) - rating\n",
        "110 abs_error += abs(error)\n",
        "111 sumsq_error += error * error\n",
        "112 return abs_error/len(ratings), sumsq_error/len(ratings)"
      ],
      "metadata": {
        "id": "Z9tAAyFcLZq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14.1.1 Alternative Formulation\n"
      ],
      "metadata": {
        "id": "69j5OxaFLZtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alternative formulation is to regularize after each update."
      ],
      "metadata": {
        "id": "XUbWn4KxLZxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14.1.2 Plotting"
      ],
      "metadata": {
        "id": "gPXDz0YkLqLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###relnCollFilt.py"
      ],
      "metadata": {
        "id": "dXGbOx_sLqPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(self, examples=\"test\"):\n",
        "    \"\"\"\n",
        "    examples is either \"test\" or \"training\" or the actual examples\n",
        "    \"\"\"\n",
        "    if examples == \"test\":\n",
        "        examples = self.test_ratings\n",
        "    elif examples == \"training\":\n",
        "      examples = self.ratings\n",
        "      plt.ion()\n",
        "      plt.xlabel(\"prediction\")\n",
        "      plt.ylabel(\"cumulative proportion\")\n",
        "      self.actuals = [[] for r in range(0,6)]\n",
        "      for (user,item,rating,timestamp) in examples:\n",
        "          self.actuals[rating].append(self.prediction(user,item))\n",
        "      for rating in range(1,6):\n",
        "      self.actuals[rating].sort()\n",
        "      numrat=len(self.actuals[rating])\n",
        "      yvals = [i/numrat for i in range(numrat)]\n",
        "    plt.plot(self.actuals[rating], yvals, label=\"rating=\"+str(rating))\n",
        "    plt.legend()\n",
        "    plt.draw()\n",
        "\n",
        "  def plot_property(self,\n",
        "  p, # property\n",
        "  plot_all=False, # true if all points should be plotted\n",
        "  num_points=200 # number of random points plotted if not all 140\n",
        "  ):\n",
        "  \"\"\"plot some of the user-movie ratings, if plot_all is true\n",
        "  num_points is the number of points selected at random plotted.\n",
        "  the plot has the users on the x-axis sorted by their value on property p and\n",
        "  with the items on the y-axis sorted by their value on property p and\n",
        "  the ratings plotted at the corresponding x-y position.\n",
        "  \"\"\"\n",
        "  plt.ion()\n",
        "  plt.xlabel(\"users\")\n",
        "  plt.ylabel(\"items\")\n",
        "  user_vals = [self.user_prop[u][p] for u in self.users]\n",
        "  item_vals = [self.item_prop[i][p] for i in self.items]\n",
        "  plt.axis([min(user_vals)-0.02, max(user_vals)+0.05, min(item_vals)-0.02, max(item_vals)+0.05])\n",
        "  if plot_all:\n",
        "    for (u,i,r,t) in self.ratings:\n",
        "      plt.text(self.user_prop[u][p], self.item_prop[i][p], str(r))\n",
        "  else:\n",
        "    for i in range(num_points):\n",
        "      (u,i,r,t) = random.choice(self.ratings)\n",
        "      plt.text(self.user_prop[u][p],self.item_prop[i][p],str(r))\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "1i59cV1NLqSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14.1.3 Creating Rating Sets"
      ],
      "metadata": {
        "id": "3WZDumaCLqU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Rating_set(Displayable):\n",
        "  def __init__(self, date_split=892000000,local_file=False,url=\"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\", file_name=\"u.data\"):\n",
        "    self.display(1,\"reading...\")\n",
        "    if local_file:\n",
        "        lines = open(file_name,'r')\n",
        "    else:\n",
        "        lines = (line.decode('utf-8') for line in\n",
        "        urllib.request.urlopen(url))\n",
        "    all_ratings = (tuple(int(e) for e in line.strip().split('\\t'))for line in lines)\n",
        "    self.training_ratings = []\n",
        "    self.training_stats = {1:0, 2:0, 3:0, 4:0 ,5:0}\n",
        "    self.test_ratings = []\n",
        "    self.test_stats = {1:0, 2:0, 3:0, 4:0 ,5:0}\n",
        "    for rate in all_ratings:\n",
        "      if rate[3] < date_split: # rate[3] is timestamp\n",
        "        self.training_ratings.append(rate)\n",
        "        self.training_stats[rate[2]] += 1\n",
        "      else:\n",
        "        self.test_ratings.append(rate)\n",
        "        self.test_stats[rate[2]] += 1\n",
        "        self.display(1,\"...read:\", len(self.training_ratings),\"training ratings and\", len(self.test_ratings),\"test ratings\")\n",
        "        tr_users = {user for (user,item,rating,timestamp) in self.training_ratings}\n",
        "        test_users = {user for (user,item,rating,timestamp) in self.test_ratings}\n",
        "        self.display(1,\"users:\",len(tr_users),\"training,\",len(test_users),\"test,\", len(tr_users & test_users),\"in common\")\n",
        "        tr_items = {item for (user,item,rating,timestamp) in self.training_ratings}\n",
        "        test_items = {item for (user,item,rating,timestamp) in self.test_ratings}\n",
        "        self.display(1,\"items:\",len(tr_items),\"training,\",len(test_items),\"test,\", len(tr_items & test_items),\"in common\")\n",
        "        self.display(1,\"Rating statistics for training set\",self.training_stats)\n",
        "        self.display(1,\"Rating statistics for test set: \",self.test_stats)\n"
      ],
      "metadata": {
        "id": "ExNUga7vLqXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes it is useful to plot a property for all (user, item,rating) triples.\n",
        "There are too many such triples in the data set. The method create top subset\n",
        "creates a much smaller dataset where this makes sense. It picks the most rated\n",
        "items, then picks the users who have the most ratings on these items. It is\n",
        "designed for depicting the meaning of properties, and may not be useful for\n",
        "other purposes."
      ],
      "metadata": {
        "id": "7IdJK5o2LqZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###relnCollFilt.py"
      ],
      "metadata": {
        "id": "dECUeOXLMF60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_top_subset(self, num_items = 30, num_users = 30):\n",
        "    \"\"\"Returns a subset of the ratings by picking the most rated items,\n",
        "    and then the users that have most ratings on these, and then all of the\n",
        "    ratings that involve these users and items.\n",
        "    \"\"\"\n",
        "    items = {item for (user,item,rating,timestamp) in self.training_ratings}\n",
        "    item_counts = {i:0 for i in items}\n",
        "    for (user,item,rating,timestamp) in self.training_ratings:\n",
        "      item_counts[item] += 1\n",
        "\n",
        "      items_sorted = sorted((item_counts[i],i) for i in items)\n",
        "      top_items = items_sorted[-num_items:]\n",
        "      set_top_items = set(item for (count, item) in top_items)\n",
        "      users = {user for (user,item,rating,timestamp) in self.training_ratings}\n",
        "      user_counts = {u:0 for u in users}\n",
        "      for (user,item,rating,timestamp) in self.training_ratings:\n",
        "        if item in set_top_items:\n",
        "        user_counts[user] += 1\n",
        "        users_sorted = sorted((user_counts[u],u) for u in users)\n",
        "        top_users = users_sorted[-num_users:]\n",
        "        set_top_users = set(user for (count, user) in top_users)\n",
        "        used_ratings = [ (user,item,rating,timestamp) for (user,item,rating,timestamp) in self.training_ratings if user in set_top_users and item in set_top_items]\n",
        "      return used_ratings\n",
        "\n",
        "movielens = Rating_set()\n",
        "learner1 = CF_learner(movielens, num_properties = 1)\n",
        "#learner1.learn(50)\n",
        "# learner1.plot_predictions(examples = \"training\")\n",
        "# learner1.plot_predictions(examples = \"test\")\n",
        "#learner1.plot_property(0)\n",
        "#movielens_subset = movielens.create_top_subset(num_items = 20, num_users = 20)\n",
        "#learner_s = CF_learner(movielens, rating_subset=movielens_subset,\n",
        "test_subset=[], num_properties=1)\n",
        "#learner_s.learn(1000)\n",
        "#learner_s.plot_property(0,plot_all=True)\n"
      ],
      "metadata": {
        "id": "8NNOpk7DMF9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PO_iao8uMGAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}